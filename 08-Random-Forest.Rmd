
# Titiannic Prediction with Random Forest

>
> Can't see forest for the trees.
>
>                                     -- English Proverb
>

As we can see from the previous chapter, decision tree does not preform well in our prediction. particularly, our re-engineered dataset performs worsen than the raw dataset. One reason could be the model itself. This chapter we will try different models to see if we can improve model's accuracy fby using **Random Forest** model. 

Random Forest model is one of the powerful ensembling machine learning algorithm which works by creating multiple decision trees and combining the output generated by each of the decision trees through a voting mechanism to produce the final output based on the majority of decision trees' votes.  The Figure \@ref(fig:forest) is an example of what a random forest classifier in general looks like:

```{r forest,  out.width = "50%", fig.align ="center", echo =FALSE, fig.cap="Example of the Random Forest."}
knitr::include_graphics(here::here("images", "Random_forest_diagram_complete.png"))
```

In random forest, the decision tree classifier does not select all the data samples and attributes in each of the trees. Instead, it randomly selects data samples and attributes in each of the tree that it creates and then combines the output at the end. It removes the bias that a decision tree model might introduce in the model. In random forest, multiple decision tree are created and based on the output of each model, a vote is carried out to find the result with the highest frequency. A test dataset is evaluated based on these outputs to get the final predicted results. Because of the evaluation process, the random forest model will have an estimated prediction accuracy once constructed. This models' estimated accuracy can be used to compare between random forest models.  

## Steps to Build a Random Forest

1. Randomly select $k$ attributes from total $m$ attributes where $k < m$, the default value of $k$ is generally $\sqrt{m}$.
2. Among the $k$ attributes, calculate the node $d$ using the **best split point**
3. Split the node into a number of nodes using the **best split method**. See Section \@ref(best_split), by default R random Forest uses Gini impurity values
4. Repeat the previous steps build an individual decision tree
5. Build a forest by repeating all steps for $n$ number times to create $n$ number of trees

After the random forest trees and classifiers are created, predictions can be made using the following steps:

1. Run the test data through the rules of each decision tree to predict the outcome and then 
2. Store that predicted target outcome
3. Calculate the votes for each of the predicted targets
4. Output the most highly voted predicted target as the final prediction 

Similar with the decision tree model, random forest also has many implementations already built. You do not need to write code to do the actual model construction. In R, you can use a package called  'randomForest'. There are a number of terminologies that are used in random forest algorithms need to be understood, such as:

1. **Variance**. When there is a change in the training data algorithm, this is the measure of that change. The most commonly used parameters to reflect changes are *ntree* and *mtry*. 

2. **Bagging**. This is a variance-reducing method that trains the model based on random sub-samples of training data. 

3. **Out-of-bag (oob)** error estimate - The random forest classifier is trained using bootstrap aggregation, where each new tree is fit from a bootstrap sample of the training dataset. The out-of-bag (OOB) error is the average error for each calculation using predictions from the trees that do not contain their respective bootstrap sample. This enables the random forest classifier to be adjusted and validated during training. 

## Titanic prediciton with a Random Forest

Letâ€™s now look at how we can implement the random forest algorithm for our Titanic prediction. 
R provides `'randomForest'` package. You can check the details of the package for full usage. We will start with direct function call with its default settings and we may change settings later. We will also use the original attributes first and then use re-engineered attributes to see if we can improve on the model.

### Random Forest with the default settings on dataset {-}
```{r echo = FALSE, warning=FALSE, message=FALSE}
# Install the random forest library, if you have not
# install.packages('randomForest')
# load library

library(randomForest)
library(plyr)
library(caret)
# load data if you have not

RE_data <- read.csv("RE_data.csv", header = TRUE)
train <- RE_data[1:891, ]
test <- RE_data[892:1309, ]
```

The process of using `randomForest` package to build a RF model is same with the decision tree package "rpart".  Note also if a dependent (response) variable is a factor, classification is assumed, otherwise regression is assumed. So to uses randomForest, we need to convert dependent variable into factor. 

```{r}
# convert variables into factor
train$Survived <- as.factor(train$Survived)
# convert other attributes which really are categorical data but in for m of numbers
train$Pclass <- as.factor(train$Pclass)
train$Group_size <- as.factor(train$Group_size)
#confirm types
sapply(train, class)
```
Let us use the same three most related attributes: Pclass, Sex and Fare_pp in the decision tree model4. We use all default parameters of the *randomForest*.

```{r}
# Build the random forest model only uses pclass, sex and Fare_pp
set.seed(1234) #for reproduction 
RF_model1 <- randomForest(Survived ~ Sex + Pclass + Fare_pp, data=train, importance=TRUE)
```

Let us check model's prediction accuracy.
```{r }
RF_model1
```

We can see that the model uses default parameters: *ntree* = 500 and *mtry* = 1. The model's estimated accuracy is **79%**. It is 1 - 0.21 (OOB error).

Let us make a prediction on train dataset and check the accuracy. 

```{r}
# Make your prediction using the validate dataset
#set.seed(1234)
RF_prediction1 <- predict(RF_model1, train)
#check up
confusionMatrix(RF_prediction1, train$Survived)
# Misclassification error
paste('Error =', round(mean(train$Survived != RF_prediction1), 4)) 
```
We can see that prediction on train dataset has achieved **81.4%** accuracy. 
It has made 135 wrong prediction and 518 correct prediction on death. The error rate is 20.67% (accuracy is 79.33%). The prediction on survived is 31 wrong prediction out of 207 correct predictions. The error rate is about 13.03% and the accuracy is 86.97%. 

The model has an accuracy of 79% after training, but our evaluation on the train dataset achieves 81.4%. It has increased a bit. Compare with the decision tree model4, which the the same attributes were used and the prediction accuracy on the train data was 81.48%, the accuracy is almost identical. Let us make a predciton on test dataset and submit to Kaggle to obtain an accuracy score. 

```{r}
# produce a submit with Kaggle required format that is only two attributes: PassengerId and Survived
test$Pclass <- as.factor(test$Pclass)
test$Group_size <- as.factor(test$Group_size)

#make prediction
RF_prediction <- predict(RF_model1, test)
submit <- data.frame(PassengerId = test$PassengerId, Survived = RF_prediction)
# Write it into a file "RF_Result.CSV"
write.csv(submit, file = "RF_Result1.CSV", row.names = FALSE)
```
We can see our random forest model has scored **0.76555** by the Kaggle competition. It is interesting to know that the random forest model has a slight higher accuracy on the test dataset compare with the decision tree model with the same predictors. The accuracy was  **0.75837**.

let us record these accuracies, 
```{r}
RF_model1_accuracy <- c(79, 81.4, 76.555)
```

### Random Forest with more variables {-}

Now let us see if We can obtain a better model if we use more variables. Note that the `randomForest` cannot handle attribute which is not a factor and has over 53 levels. 
```{r}
set.seed(2222)
RF_model2 <- randomForest(Survived ~ Pclass + Title + Sex + Age_group + Group_size + Ticket_class  + Fare_pp + Deck + HasCabinNum + Embarked, data = train, importance=TRUE)

```
We can assess the new model,

```{r}
RF_model2
```

Notice that the default parameter *mtry* = 3 and ntree = 500. It means the number of variable tried at each split is now 3 and number of trees can be built is 500. The model's estimated OOB error rate is 16%. It has a huge increase in comparison with the first model which was 21%. So the overall accuracy of the model has reached 84%.

Let us make a prediction on train Data to verify the model's training accuracy.
```{r}
RF_prediction2 <- predict(RF_model2, train)
#check up
confusionMatrix(RF_prediction2, train$Survived)
# Misclassification error
paste('Error =', round(mean(train$Survived != RF_prediction2), 4)) 

```
We can see the accuracy on train dataset ahs reached 93.5%. The result shows that the prediction on survive has 43 wrong predictions out of 534 correct predictions, the error rate is 7.5%, and the accuracy is 92.5%. The prediction on death has 299 correct predictions and 15 wrong predictions, the error rate is 4.8%, the accuracy is 95.2%. The overall accuracy reaches **93.5%**. It is again higher than the model training accuracy **84%**.

It has also increased a bit comparing with the accuracy on the estimated accuracy **79%** and the accuracy on train dataset **81.4%** of the random forest RF_model1.

Let us make another submit to Kaggle to see if the prediction on unseen data has been improved. 

```{r}
# produce a submit with Kaggle 
test$Pclass <- as.factor(test$Pclass)
test$Group_size <- as.factor(test$Group_size)

#make prediction
RF_prediction <- predict(RF_model2, test)
submit <- data.frame(PassengerId = test$PassengerId, Survived = RF_prediction)
# Write it into a file "RF_Result.CSV"
write.csv(submit, file = "RF_Result2.CSV", row.names = FALSE)
```
Unfortunately, the feedback shows the prediction only scored 0.76794! It has improved on the RF_model1, but it is still not as good as the decision tree model3! The gap between the model estimated accuracy and the varified accuracy has increased a lot! The model's estimated accuracy and accuracy on train dataset were 84% and 93.5% respectively! 

let us record these various accuracy.

```{r}
RF_model2_accuracy <- c(84, 93.5, 76.794)
```

There are two possible reasons for this dramatic accuracy fall. One reason is the model was constructed by using inappropriate predictors. This includes the wrong number of the predictors and the test conditions formed by the predictor are not appropriate; a model used too many predictors can pick up outliers and noise data, so the prediction accuracy will be decreased when used for unseen data. This can be discovered by the cross validation. Another possible reason is the model's parameter are not set to the most appropriate values. there are methods can be used to fine tune model's parameters. Both issues and the methods for resolving issues will be discussed in the next Chapter. 

## Summary

In this section we have demonstrated the use of random forest prediction models for the Titanic problem. We have tried two different models by using different numbers of the predictors. Their over all accuracy has illustrated in the figure \@.





Despite the efforts in features' engineering, the random forest models performed great at the train dataset but fall dramatically with the test dataset. It demonstrated the practical problem in data science project that is overfitting. A model can perform well with the train dataset but not with the unseen data. overfitting can be discovered and eliminated with Cross validation.  


## Excercise 8

<!-- 1. Explain the difference between train a random forest model using train dataset and trainData where trainData is a 70% of the train dataset. The train dataset is split with 70:30 ratio to produce trainData and validData. Which model will have a higher accuracy when predict on the validData? -->

<!-- 2. Explain why the variable importance has different order between *MeanDecreasAccuracy* and *MeanCecreaseJINI*. -->

<!-- 3. Explain why the importance of attributes produced by random forest is differnt with coorelation analysis in Chapter 6. -->

4. Find out what is "OOB estimate of error rate"? How to reduce its value?

5. In a random forest model, its Confusion matrix shows misclassified samples and their error rate. Explain the concept of the "Positive error" and "Negative error" how to balance them? 

6. Try different sampling methods by using different fold and repeat numbers in the Cross Validation to see the affect of the tune parameters and model's accuracy.

7. Explore train method in caret with different models and methods
   





