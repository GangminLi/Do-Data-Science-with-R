# Model Cross Validation

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,warning = FALSE)
```
```{r prediction1， out.width = "60%", fig.align ="center", echo =FALSE}
knitr::include_graphics(here::here("images", "knobs.jpg"))

```
>
>"...with four parameters I can fit an elephant, and with five I can make him wiggle his trunk."
>
>                                        -- John von Neumann
>
>


In the previous two chapters we have demonstrated how to build prediction models using both Decision Tree and Random Forest, the two popular prediction models. The models we built have different prediction accuracy. The big problem with the models is their reduced prediction accuracy with the test dataset. A even bigger problem is that the reduction of the prediction accuracy with each model is different and unpredictable. Together they created a great difficulty to choose which model should be used for the real applications. 

We are luck because we have Kaggle competition that provides us with a test dataset and the feedback on our model's performance. In real applications, as the titanic competition simulated, the test dataset has no response variable's (survival status) value. We will have no means to compare to evaluate model's accuracy. 

Although we may use the methods as we have used in Chapter 8, where we use our model to predict on the train dataset and made a comparison with the original value to estimate the model's prediction accuracy. The similar method (OOB) is also used in the random forest models (in Chapter 9) to estimate the model's accuracy. We know that our estimated accuracy is not reliable. 

There is a systematic method in data science to evaluate a prediction model's performance. It is called "Cross Validation (CV)". This chapter we will demonstrate how to use CV to evaluate a model's performance.
  
## Model's Underfitting and Overfitting

We have experienced the problems with  both of our decision tree models and random forest models. The models have a higher estimated accuracy (from the model construction) and a much lower accuracy when predicted on the test dataset. This Would only mean two things ether the prediction model is overfitting or it is underfitting.

let us quickly look at a very graphic example of underfitting and overfitting. 

```{r modelfit, fig.cap ="Model's fit by train and test data", out.width = "100%", fig.align ="center", echo =FALSE}
knitr::include_graphics(here::here("images", "underfit.png"))
```

We can see \@ref(fig:modelfit) that the first model is a straight line (a low variance model: $y$ = $m$ * $x$ + $c$) fails to capture the underlying parabolic curve in the data, this is underfitting. At the other extreme the high degree polynomial (a low bias model) captures too much of the noise at the same time as the underlying parabola, and is overfitting. Although it is following the data points provided (ie. the training dataset), this curve is not transferable to new data (ie. the test dataset).

Among the models we have produced, the decision tree model1 with only attribute *Sex* as its predictor is an example of underfitting model. It has 78.68% estimated accuracy on the training dataset but only has 76.56% accuracy on the test dataset. On the contrary, all our random forest Models have an issue of overfitting. 

## General Cross Validation Methods

There are two general CV methods can be used to valid a prediction model:

1. Single model CV
2. Multiple models comparison

### Single model Cross Validation
The goal of single model CV is to test the model's ability to predict new data that was not seen and not used in model construction. So, the problem can be spotted like overfitting or selection bias, in addition it can also give an insight on how the model will generalize to an independent dataset or an unknown dataset.

One round of CV involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set). To reduce variability, in most methods multiple rounds of CV are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model's predictive performance.

There are two major cross validation methods: exhaustive CV and non-exhaustive CV. 

+ **Exhaustive CV** learn and test on all possible ways to divide the original sample into a training and a validation set. **Leave-p-out CV (LpO CV)** is an exhaustive cross validation method. It involves using $p$ data samples as the validation dataset and the remaining data samples as the training dataset. This is repeated over and over until all possible ways to divide the original data sample into a training and a validation dataset $p$. 

+ **Non-exhaustive cross validation**, in the contrary, does not compute all the possible ways of splitting the original data sample but still has a certain coverage. **$k$-fold CV** is a typical non-exhaustive cross validation. 

```{r k-fold, fig.cap ="K-Folds Cross Validation", out.width = "100%", fig.align ="center", echo =FALSE}
knitr::include_graphics(here::here("images", "k-fold.png"))
```
In $k$-fold CV, the original data sample is randomly partitioned into $k$ equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation dataset for testing the model, and the remaining $k$ − 1 subsamples are used as training data. The CV process is then repeated $k$ times, with each of the $k$ subsamples used exactly once as the validation data. The $k$ results can then be averaged to produce a single estimation. The advantage of this method over repeated random sub-sampling is that all observations are used for both training and validation, and each observation is used for validation exactly once. 10-fold CV is commonly used in practice. 

### General Procedure of CV

General process of Cross Validation is as follows:

1. Split the entire data randomly into $K$ folds (value of $K$ shouldn’t be too small or too high, ideally we choose 5 to 10 depending on the data size). The higher value of $K$ leads to less biased model (but large variance might lead to overfit), where as the lower value of $K$ is similar to the train-test split approach we saw before.
2. Then fit the model using the $K - 1$ folds and validate the model using the remaining $K$th fold. Note down the scores/errors.

3. Repeat this process until every $K$ fold serve as the test set. Then take the average of your recorded scores. That will be the performance metric for the model.

We will use examples to demonstrate this procedure. 

### Cross Validation on Decision Tree Models

We have produced four decision tree models in Chapter 8. Let us do Cross validation on model2 and model3 since they have identical predictors with the random forest RF_model1 and RF_model2 which we will do Cross validation later.
```{r}
library(caret)
library(rpart)
library(rpart.plot)

#read Re-engineered dataset
RE_data <- read.csv("RE_data.csv", header = TRUE)

#Factorize response variable
RE_data$Survived <- factor(RE_data$Survived)
RE_data$Survived <- factor(RE_data$Survived)
#Separate Train and test data.
train <- RE_data[1:891, ]
test <- RE_data[892:1309, ]

#setup model's train and valid dataset
set.seed(1000)
samp <- sample(nrow(train), 0.8 * nrow(train))
trainData <- train[samp, ]
validData <- train[-samp, ]

```
```{r}
# set random for reproduction
set.seed(3214)
# specify parameters for cross validation
control <- trainControl(method = "repeatedcv", 
                        number = 10, # number of folds
                        repeats = 5, # repeat times
                        search = "grid")
```
Our cross validation settings are: 10 folds, and repeat 5 times, with "Grid" search the optimal parameter. The detailed meaning of each settings refers to http://topepo.github.io/caret/data-splitting.html.

Let us to Cross validation for Tree model2,

```{r}
set.seed(1010)
#create model from cross validation data
Tree_model2_cv <- train(Survived ~ Sex + Pclass + HasCabinNum + Deck + Fare_pp,
                      data = trainData, 
                      method = "rpart", 
                      trControl = control)
```
Display details of the cross validation, 

```{r tree_model2_CV, out.width='32.8%', fig.show='hold', fig.cap='Decision Tree CV model2.'}
#Visualize cross validation tree
rpart.plot(Tree_model2_cv$finalModel, extra=4)
print.train(Tree_model2_cv)
plot.train(Tree_model2_cv)
model_accuracy <- Tree_model2_cv$results$Accuracy[1]
# accuracy is 81.48.
```
let us record the model's accuracy on *trainData*, *validData*, and *test* dataset. Remember *trainData* and *validData* are randomly partitioned from the train dataset.                                                   
```{r}
### Access accuracy on different datasets
#predict on train
predict_train <-predict(Tree_model2_cv, trainData)
conMat <- confusionMatrix(predict_train, trainData$Survived)
conMat$table
#conMat$overall
predict_train_accuracy <- conMat$overall["Accuracy"]
predict_train_accuracy
#predict on valid
predict_valid <-predict(Tree_model2_cv, validData)
conMat <- confusionMatrix(predict_valid, validData$Survived)
conMat$table
#conMat$overall
predict_valid_accuracy <- conMat$overall["Accuracy"]
predict_valid_accuracy

#predict on test
predict_test <-predict(Tree_model2_cv, test)
submit <- data.frame(PassengerId = test$PassengerId, Survived = as.factor(predict_test))
write.csv(submit, file = "Tree_model2_CV.CSV", row.names = FALSE)

## test accuracy 75837
# accumulate model's accuracy
Tree_model2_CV_accuracy <- c(model_accuracy, predict_train_accuracy, predict_valid_accuracy, 0.75837)
```
We can see the tree differences from Figure \@ref(fig:tree_model2_CV) and Figure \@ref(fig:tree2). We can also see that despite the model tried the best parameters, the prediction accuracy on the test dataset is dropped from 0.76555 (default decision tree) to 0.75837. It shows that the model construction has reached the best since the change of the tree structure does not increase the accuracy. The drop of the accuracy may caused by the reduction of the size of the training dataset. It reflects the second possible cause of the overfitting, that is the size of the training sample. Recall that decision tree model2 was trained on the train dataset and now it is trained on the trainData. the later is a random subset of the train dataset and only has 80 percent of the data samples. That is to say, the smaller of the training dataset the more chance of the inaccurate prediction accuracy on the test dataset (overfitting or underfitting).

Let us do cross validation on tree model3, 

```{r}
set.seed(1234)
tree_model3_cv <- train(Survived ~ Sex + Fare_pp + Pclass + Title + Age_group + Group_size + Ticket_class  + Embarked,

                       data = trainData, 
                       method = "rpart", 
                       trControl = control)
```
Visualize model,
```{r tree_model3_CV, out.width='32.8%', fig.show='hold', fig.cap='Decision Tree CV model3.'}
#Visualize cross validation tree

rpart.plot(tree_model3_cv$finalModel, extra=4)

print.train(tree_model3_cv)
plot.train(tree_model3_cv)
model_accuracy <- tree_model3_cv$results$Accuracy[1]
# accuracy is 0.82.
```
Record model's accuracy,

```{r}
### Access accuracy on different datasets
#predict on train
predict_train <-predict(tree_model3_cv, trainData)
conMat <- confusionMatrix(predict_train, trainData$Survived)
conMat$table
#conMat$overall
predict_train_accuracy <- conMat$overall["Accuracy"]
predict_train_accuracy
#predict on valid
predict_valid <-predict(tree_model3_cv, validData)
conMat <- confusionMatrix(predict_valid, validData$Survived)
conMat$table
#conMat$overall
predict_valid_accuracy <- conMat$overall["Accuracy"]
predict_valid_accuracy

#predict on test
predict_test <-predict(tree_model3_cv, test)
submit <- data.frame(PassengerId = test$PassengerId, Survived = as.factor(predict_test))
write.csv(submit, file = "Tree_model3_CV.CSV", row.names = FALSE)

## test accuracy 0.77751
# accumulate model's accuracy
Tree_model3_CV_accuracy <- c(model_accuracy, predict_train_accuracy, predict_valid_accuracy, 0.77751)
Tree_model3_CV_accuracy
```
The results shows a consistent prediction accuracy. The accuracy on the test dataset has been increased from 0.77033 (Tree model3) to 0.7775. The point perhaps is that the increase of predictors does improve the accuracy (so far). 

Based on the two cross validation we did on the two decision tree models: model2 and model3, we can conclude that the decision tree model default setting are nearly reaches the best setting. This is because after we used 10 folds and repeat 5 times cross validation and Grid search for best parameters, we did not improve much of the models' accuracy.   

Now, Let us try the same cross validation with the two Random forest models constructed in the Chapter 9.

```{r }
# set seed for reproduction
set.seed(2307)
RF_model1_cv <- train(Survived ~ Sex + Pclass + HasCabinNum +      Deck + Fare_pp,  
                       data = trainData, 
                       method = "rf", 
                       trControl = control)

print(RF_model1_cv)
print(RF_model1_cv$results)
model_accuracy <- RF_model1_cv$results$Accuracy[2]
model_accuracy
```
We can see that the best model parameters are *mtry = 7* and *ntree = 500*, The trained model's best accuracy is 83.87%.

Let us verify on validate dataset and make prediction on test dataset. 
```{r}
### Access accuracy on different datasets
#predict on train
predict_train <-predict(RF_model1_cv, trainData)
conMat <- confusionMatrix(predict_train, trainData$Survived)
conMat$table
#conMat$overall
predict_train_accuracy <- conMat$overall["Accuracy"]
predict_train_accuracy
#predict on valid
predict_valid <-predict(RF_model1_cv, validData)
conMat <- confusionMatrix(predict_valid, validData$Survived)
conMat$table
#conMat$overall
predict_valid_accuracy <- conMat$overall["Accuracy"]
predict_valid_accuracy

#predict on test
predict_test <-predict(RF_model1_cv, test)
submit <- data.frame(PassengerId = test$PassengerId, Survived = as.factor(predict_test))
write.csv(submit, file = "RF_model1_CV.CSV", row.names = FALSE)

## test accuracy 74641
# accumulate model's accuracy

RF_model1_cv_accuracy <- c(model_accuracy, predict_train_accuracy, predict_valid_accuracy, 0.74641)
RF_model1_cv_accuracy
```
The trainData set (randomly selected 80 percent of train dataset ), the random forest parameter (mtry = 7, ntree = 500) and the cross validation settings (fold = 10 and repeats= 5) combined together a model. Its prediction accuracy is pretty bad with 74.6% accuracy on the test dataset. The same predictors using default random forest settings(mtry = 1, ntree = 500) and trained on the train dataset has a prediction accuracy of 0.76555.

Let us try on random forest model2,
```{r}
# set seed for reproduction
set.seed(2300)

RF_model2_cv <- train(Survived ~ Pclass + Title + Sex + Age_group + Group_size + Ticket_class  + Fare_pp + Deck + HasCabinNum + Embarked, 
                       data = trainData, 
                       method = "rf", 
                       trControl = control)

print(RF_model2_cv)
print(RF_model2_cv$results)
model_accuracy <- RF_model2_cv$results$Accuracy[2]
model_accuracy
```
Let us calculate model's accuracy,
```{r}
### Access accuracy on different datasets
#predict on train
predict_train <-predict(RF_model2_cv, trainData)
conMat <- confusionMatrix(predict_train, trainData$Survived)
conMat$table
#conMat$overall
predict_train_accuracy <- conMat$overall["Accuracy"]
predict_train_accuracy
#predict on valid
predict_valid <-predict(RF_model2_cv, validData)
conMat <- confusionMatrix(predict_valid, validData$Survived)
conMat$table
#conMat$overall
predict_valid_accuracy <- conMat$overall["Accuracy"]
predict_valid_accuracy

#predict on test
predict_test <-predict(RF_model2_cv, test)
submit <- data.frame(PassengerId = test$PassengerId, Survived = as.factor(predict_test))
write.csv(submit, file = "RF_model2_CV.CSV", row.names = FALSE)

## test accuracy 0.75119
# accumulate model's accuracy

RF_model2_cv_accuracy <- c(model_accuracy, predict_train_accuracy, predict_valid_accuracy, 0.75119)
RF_model2_cv_accuracy
```
We have used 10 folds and repeating 5 times cross validation with 80% of the train dataset to build and validate 4 models we have produced, two from decision tree and two from random forest. The accuracy with different datasets have been collected. let us put them into one table and plot them so we can make a comparison. 

```{r tree_rf_com}
library(tidyr)
Model <- c("Tree_M2","Tree_M3","RF_model1","RF_model2")
Tree_model2_CV_accuracy
Tree_model3_CV_accuracy
RF_model1_cv_accuracy
RF_model2_cv_accuracy

Pre <- c("Sex, Pclass, HasCabinNum, Deck, Fare_pp", "Sex, Fare_pp, Pclass, Title, Age_group, Group_size, Ticket_class, Embarked", "Sex, Pclass, HasCabinNum, Deck, Fare_pp", "Sex, Fare_pp, Pclass, Title, Age_group, Group_size, Ticket_class, Embarked")

Learn <- c(Tree_model2_CV_accuracy[1]*100, Tree_model3_CV_accuracy[1]*100, RF_model1_cv_accuracy[1]*100, RF_model2_cv_accuracy[1]*100)
Learn
Train <- c(Tree_model2_CV_accuracy[2]*100, Tree_model3_CV_accuracy[2]*100, RF_model1_cv_accuracy[2]*100, RF_model2_cv_accuracy[2]*100)
Train
Valid <- c(Tree_model2_CV_accuracy[3]*100, Tree_model3_CV_accuracy[3]*100, RF_model1_cv_accuracy[3]*100, RF_model2_cv_accuracy[3]*100)
Valid
Test <- c(Tree_model2_CV_accuracy[4]*100, Tree_model3_CV_accuracy[4]*100, RF_model1_cv_accuracy[4]*100, RF_model2_cv_accuracy[4]*100)
Test

df1 <- data.frame(Model, Pre, Learn, Train, Valid, Test)
df2 <- data.frame(Model, Learn, Train, Valid, Test)
knitr::kable(df1, longtable = TRUE, booktabs = TRUE, digits = 2, col.names =c("Models", "Predictors", "Accuracy on Learn", "Accuracy on Train", "Accuracy on Valid",  "Accuracy on Test"), 
  caption = 'The Comparision among 4 CV models'
)
```

```{r CVmodelcompare, fig.cap = "Cross valid models' accuracy on model learning, Traindata dataset. Validdata and Test dataset."}
df.long <- gather(df2, Dataset, Accuracy, -Model, factor_key =TRUE)
ggplot(data = df.long, aes(x = Model, y = Accuracy, fill = Dataset)) +
  geom_col(position = position_dodge()) 

```

From the Cross validation results we can conclude that:

1. Both decision tree and random forest models default settings are good settings. Despite dynamic search for best parameters, the change of the parameter setting do not affect the prediction accuracy much. So both default settings for the prediction model are acceptable.
2. Change of training dataset for model building from train dataset to its subset trianData, in 10 fold 5 repeat cross validation settings, does not change the order of models' performance in terms of decision tree and random forest. It however, when considering a single model, does suggest that the number of samples used for learn a model has an impact on model's prediction results. 
3. It is clearly shows that the random forest models have an overfitting. 
4. It does not provide a conclusive result that decision tree is better than random forest or vice verse. 

A general rule seems that, the more features you have, and the more samples you used in the training, the more likely your model will suffer from overfitting and vice verse. 

Therefore to choose a model for real prediction, we should choose the model that has the smallest accuracy decrease from the model's training to verification by the cross validation. 

## Multiple Models Comparison

Multiple model comparison is also called Cross Model Validation. The idea is to use multiple models constructed from the same training dataset and validated using the same verification dataset to find out the performance of the different models.

We somehow already used the technique to compare our decision tree models and random forest models. Cross model verification has a broader meaning that refers to the comparison between different models produced by the different algorithms or completely different approaches such as decision tree against random forest or decision tree against Support Victor Machine(SVM) etc.

To demonstrate cross model validation, let us produce a few more models with complete different algorithms with same predictors as much as possible. Let us use *Sex*, *Fare_pp*, *Pclass*, *Title*, *Age_group*, *Group_size*, *Ticket_class*, *Embarked* as predictors.

### Regression Model for Titanic 
```{r}
LR_Model <- glm(formula = Survived ~ Pclass + Title + Sex + Age_group + Group_size + Ticket_class  + Fare_pp + Embarked, family = binomial, data = trainData)

#summary(LR_Model_CV)
### Validate on trainData
Valid_trainData <- predict(LR_Model, newdata = trainData, type = "response") #prediction threshold
Valid_trainData <- ifelse(Valid_trainData > 0.5, 1, 0)  # set binary 
#produce confusion matrix
confusion_Mat<- confusionMatrix(as.factor(trainData$Survived),as.factor(Valid_trainData))
# output accuracy
Regression_Acc_Train <- round(confusion_Mat$overall["Accuracy"]*100,2)
paste('Model Train Accuracy =', Regression_Acc_Train)

### Validate on validData
validData_Survived_predicted <- predict(LR_Model, newdata = validData, type = "response")  
validData_Survived_predicted  <- ifelse(validData_Survived_predicted  > 0.5, 1, 0)  # set binary prediction threshold
conMat<- confusionMatrix(as.factor(validData$Survived),as.factor(validData_Survived_predicted))

Regression_Acc_Valid <-round(conMat$overall["Accuracy"]*100,2)
paste('Model Valid Accuracy =', Regression_Acc_Valid) 

### produce a prediction on test data
library(pROC)
auc(roc(trainData$Survived,Valid_trainData))  # calculate AUROC curve
#predict on test
test$Survived <- predict(LR_Model, newdata = test, type = "response")  
test$Survived <- ifelse(test$Survived > 0.5, 1, 0)  # set binary prediction threshold
submit <- data.frame(PassengerId = test$PassengerId, Survived = as.factor(test$Survived))

write.csv(submit, file = "LG_model1_CV.CSV", row.names = FALSE)
Regr_Acc <- c(Regression_Acc_Train, Regression_Acc_Valid, 0.76555)
Regr_Acc
```

### Support Vector Machine Model for Titanic 

Let us also consider a support vector machine (SVM) model [@Cortes-and-Vapnik](Cortes and Vapnik 1995 Cortes, Corinna, and Vladimir Vapnik. 1995. “Support-Vector Networks.” Machine Learning, 273–97.). We use the C-classification mode. Again, we fit a model with the same set of attributes as in the logistic regression model to fit the model, we use function `svm()` from the `e1071` package (Meyer et al. 2019).

We could try to tune the two parameters of the SVM model `gamma` & `cost`, find and select the best parameters (see exercise). 

We then use the best model to make predictions. The results of the model are collected for comparison.

```{R}
library(e1071)
#gamma=.05, cost=1.30
#SVM_model<-svm(Survived ~ Pclass + Title + Sex + Age_group + Group_size + Ticket_class + Fare_pp + Deck + HasCabinNum + Embarked, data=trainData, gamma=0.05, cost=1.3,type="C-classification")

SVM_model<- svm(Survived ~ Pclass + Title + Sex + Age_group + Group_size + Ticket_class + Fare_pp + Deck + HasCabinNum + Embarked, data=trainData, kernel = 'radial', type="C-classification")

#summary(SVM_model)
### Validate on trainData
Valid_trainData <- predict(SVM_model, trainData) 
#produce confusion matrix
confusion_Mat<- confusionMatrix(as.factor(trainData$Survived), as.factor(Valid_trainData))
# output accuracy
AVM_Acc_Train <- round(confusion_Mat$overall["Accuracy"]*100,4)
paste('Model Train Accuracy =', AVM_Acc_Train)

### Validate on validData
validData_Survived_predicted <- predict(SVM_model, validData)  
conMat<- confusionMatrix(as.factor(validData$Survived), as.factor(validData_Survived_predicted))

AVM_Acc_Valid <- round(conMat$overall["Accuracy"]*100,4)
paste('Model Valid Accuracy =', AVM_Acc_Valid) 

### make prediction on test
# SVM failed to produce a prediction on test because test has Survived col and it has value NA. A work around is assign it with a num like 1.
test$Survived <-1

Survived <- predict(SVM_model, test)
solution <- data.frame(PassengerId=test$PassengerId, Survived =Survived)
write.csv(solution, file = 'svm_predicton.csv', row.names = F)

SVM_Acc <- c(AVM_Acc_Train, AVM_Acc_Valid, 0.78947)
SVM_Acc
```

### Neural Network Models

Neural networks are a rapidly developing paradigm for information processing based loosely on how neurons in the brain processes information. A neural network consists of multiple layers of nodes, where each node performs a unit of computation and passes the result onto the next node. Multiple nodes can pass inputs to a single node and vice versa.

The neural network also contains a set of weights, which can be refined over time as the network learns from sample data. The weights are used to describe and refine the connection strengths between nodes. 

Neural Network with one hidden layer utilizing all features

```{r}
library(nnet)

xTrain = train[ , c("Survived", "Pclass","Title", "Sex","Age_group","Group_size", "Ticket_class", "Fare_pp", "Deck", "HasCabinNum", "Embarked")]

NN_model1 <- nnet(Survived ~ ., data = xTrain, size=10, maxit=500, trace=FALSE)

#How do we do on the training data?
nn_pred_train_class = predict(NN_model1, xTrain, type="class" )  # yields "0", "1"
nn_train_pred = as.numeric(nn_pred_train_class ) #transform to 0, 1
confusion_Mat<-confusionMatrix(as.factor(nn_train_pred), train$Survived)
# output accuracy
NN_Acc_Train <- round(confusion_Mat$overall["Accuracy"]*100,4)
paste('Model Train Accuracy =', NN_Acc_Train)

#How do we do on the valid data?
nn_pred_valid_class = predict(NN_model1, validData, type="class" )  # yields "0", "1"
nn_valid_pred = as.numeric(nn_pred_valid_class ) #transform to 0, 1
confusion_Mat<-confusionMatrix(as.factor(nn_valid_pred), validData$Survived)
# output accuracy
NN_Acc_Valid <- round(confusion_Mat$overall["Accuracy"]*100,4)
paste('Model valid Accuracy =', NN_Acc_Valid)

#make a prediction on test data
nn_pred_test_class = predict(NN_model1, test, type="class" )  # yields "0", "1"
nn_pred_test = as.numeric(nn_pred_test_class ) #transform to 0, 1
solution <- data.frame(PassengerId=test$PassengerId, Survived = nn_pred_test)
write.csv(solution, file = 'NN_predicton.csv', row.names = F)

###
# 0.8934,0.8547, 0.71052
NN_Acc <- c(NN_Acc_Train, NN_Acc_Valid, 0.71052)
NN_Acc
```
### Comparision among Different Models

Let us compare the different models we have produced and see which one has a better prediction accuracy on the test dataset. We will use our best prediction accuracy on the test dataset for decision tree and random forest models.

```{r Tabmodelcompare}
library(tidyr)
Model <- c("Regression","SVM","NN", "Decision tree", "Random Forest")
Train <- c(Regression_Acc_Train, AVM_Acc_Train, NN_Acc_Train, 82.72, 83.16)
Valid <- c(Regression_Acc_Valid, AVM_Acc_Valid, NN_Acc_Valid, 81.01, 92)
Test <- c(76.56, 78.95, 71.05, 77.75, 78.95)
df1 <- data.frame(Model, Train, Valid, Test)

knitr::kable(df1, longtable = TRUE, booktabs = TRUE, digits = 2, col.names =c("Models", "Accuracy on Train", "Accuracy on Valid","Accuracy on Test"), 
  caption = 'The Comparision among 3 Machine Learning Models'
)
```
```{r RFmodelcompare, fig.cap = "Accuracy comparision among differnt Machine Leanring models."}
df.long <- gather(df1, Dataset, Accuracy, -Model, factor_key =TRUE)
ggplot(data = df.long, aes(x = Model, y = Accuracy, fill = Dataset)) +
  geom_col(position = position_dodge()) 

```
From the above table and plot we can see that multiple models cross validation does not provide a conclusive answer about which model to us for real applications. Ideally, we would choose a model that has a higher train and validation accuracy, from the table \@ref:(tab:Tabmodelcompare), we should choose model *NN* since it has the highest train accuracy (91.13%) and second highest validation accuracy (87.71%), however, it has the lowest test accuracy (71.05%); Another possible logic would be choose the highest validation accuracy ignoring the train accuracy, in this case, we would choose *Random Forest* model since it has the highest validation accuracy (92.00%), and its test accuracy reaches the highest among the models (78.95%) indeed. However, the *SVM* model also has 78.95% test accuracy and its validation accuracy is the lowest. 

It revealed a unpleasant fact that there is no a conclusive model can be certain performs better when facing unseen data after cross validation. 

## Summary

Remember the initial motivation of introduce Cross Validation was to identify the overfitting of a prediction model and find the possible causes, so it can be avoid or eliminated. It means not use it for the real application. For the first purpose, single model Cross validation has been successful. After split a training dataset into K-fold partitions, the model that has an overfit problem can be clearly seen with the validation dataset. For example, Random forest model4 in table \@ref(tab:tree_rf_com), it has 83.99% estimated accuracy on the model's construction, train, and	96.63% of accuracy on the train dataset,	77.09% accuracy on the validation dataset, which the model does not see before, is is a big drop, it get worse when use the model on the test dataset, it only has	75.12% of the prediction accuracy. Cross validation on multiple models does not provide a definite conclusion. It does revealed a insight that any model's perfromace can be affected by the samples and the model's parameters, and combination of the two. In data science, to find the relationship between them is called **model fit**. That is what we are going to discuss in the next Chapter. 


## Excersis

1. Tune SVM models using `e1071` package. There are two parameters gamma & cost. Using tune.svm() to tune the model and find the best vlaues for gamma & cost. 

```
library(e1071)
#use grid-search to find the best gamma & cost. this can be done with different range settings
svm_test<-tune.svm(Survived ~ Pclass + Title + Sex + Age_group + Group_size + Ticket_class + Fare_pp + Deck + HasCabinNum + Embarked, data=trainData, gamma = seq(0, 0.3, .05), cost = seq(.5, 10, .5))

plot(svm_test)
```
```
##the accuracy with each gamma and cost 
d<-double(nrow(svm_test$performances))
for(i in 1:nrow(svm_test$performances)){
  b_svm<-svm(Survived~ Pclass + Title + Sex + Age_group + Group_size + Ticket_class + Fare_pp + Deck + HasCabinNum + Embarked, trainData, gamma=svm_test$performances[i,1],cost=svm_test$performances[i,2],type="C-classification")
  b<-table(trainData[,1],predict(b_svm,trainData[,-1]))
  d[i]<-sum(diag(b))/sum(b)
}
e<-data.frame(gamma=svm_test$performances[1],cost=svm_test$performances[2],error=svm_test$performances[3],dispersion=svm_test$performances[4],accrancy=d)
e<-e[order(e$error,decreasing = F),]
head(e,10)  
```

2. Try different models with Neural Networks. It should has a better prediction accuracy then the one we have produced.


## Reference:

1, Rob J Hyndman and George Athanasopoulos, "Forecasting: Principles and Practice" (2nd ed) https://otexts.com/fpp2/selecting-predictors.html

2. (Cortes and Vapnik 1995) Cortes, Corinna, and Vladimir Vapnik. 1995. “Support-Vector Networks.” Machine Learning, 273–97.)
