--- 
title: "Do Data Science in 10 Hours"
author: "Gangmin Li"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: rstudio/bookdown-demo
description: "This is my first book writtn using the bookdown package to write a book. The output format for this book is bookdown::gitbook."
---

#  {-}


```{r fig.align = 'center', out.width = "60%", echo=FALSE}

knitr::include_graphics(here::here("images", "hotstove.jpg"))

```


``` 

"Dont't touch it, It's hot!",

...


You now know it is hot. don't you?

How?

Because, you'v touched it!

```



<!--chapter:end:index.Rmd-->

# Preface {-}

## What is this book? Why to read it?
This book is originated from my Data Science course. It was in the lab sessions, where students practice different steps in the workflow of Data Science projects. Students enjoyed the detailed practices using different methods and different algorithms to solve analytical problems. To my surprise, we only find out later, that the student failed to grasp the real meaning of doing data science, which is not to provide an absolutely working solution to a problem, rather, an experimental interpretation of data at hand. In other words, you are just telling other propel what the data is telling you. You are not using data to solve problems that data has not provide any solution to you! They appeared also lost the track of doing a data science project. stuck at circles between two steps and failed to move foreword. 

I did a short tutorial for my students. the tutorial was emphasizing on the process and workflow of doing a data science project. That short tutorial was extremely successful and welcomed by all students, particularly the students who is not from computer Science, software engineering, Statistics, applied math, rather, from information science and management science. 

So, I suppose this book is practical for students who has no background of computing and programming knowledge but interested in doing data science project or move to data science in the future. 

It is introduction level book for novelty students who want learn data science in short period time perhaps a few days or at their winter or summer holidays.  

## Structure of the Book

## What Can This Course Offer You?

This book offers a short practical course.  

This course will bring you:

1. Understand the data analysis procedure
2. Familiar with R language 
3. Using RStudio to do your prototype of data project
4. Data preprocess - manipulate data for further analysis
3. Manage basic methods like Descriptive, Exploratory and Predictive data analysis
5. Have basic skills of visualize data results
6. Basic interpretation of you analyzing results and communicate with others
7. Report your results 
8. Enter the data science community


## Schedule

is specifically designed for students who has no background of much Algorithms, programming and even computing. The basic requirements are desire to learn, attitude of humble and diligence of working. All that mean you need to get your hand dirty, spent time and do more practices. At the end, it cannot guaranty you become a data scientist but it will help you find the way to wards doing data science and be confident to start doing data science projects. It is certainty that if you persist on this road, you will no doubt becomes a future data scientist. 
Setup	Download files required for the lesson
00:00	1. Analyzing Patient Data	How do I read data into R?
How do I assign variables?
What is a data frame?
How do I access subsets of a data frame?
How do I calculate simple statistics like mean and median?
Where can I get help?
How can I plot my data?
00:45	2. Creating Functions	How do I make a function?
How can I test my functions?
How should I document my code?
01:15	3. Analyzing Multiple Data Sets	How can I do the same thing to multiple data sets?
How do I write a for loop?
01:45	4. Making Choices	How do I make choices using if and else statements?
How do I compare values?
How do I save my plots to a PDF file?
02:15	5. Command-Line Programs	How do I write a command-line script?
How do I read in arguments from the command-line?
02:45	6. Best Practices for Writing R Code	How can I write R code that other people can understand and use?
02:55	7. Dynamic Reports with knitr	How can I put my text, code, and results all in one document?
How do I use knitr?
How do I write in Markdown?
03:15	8. Making Packages in R	How do I collect my code together so I can reuse it and share it?
How do I make my own packages?
03:45	9. Introduction to RStudio	How do I use the RStudio graphical user interface?
04:00	10. Addressing Data	What are the different methods for accessing parts of a data frame?
04:20	11. Reading and Writing CSV Files	How do I read data from a CSV file into R?
How do I write data to a CSV file?
04:50	12. Understanding Factors	How is categorical data represented in R?
How do I work with factors?
05:10	13. Data Types and Structures	What are the different data types in R?
What are the different data structures in R?
How do I access data within the various data structures?
05:55	14. The Call Stack	What is the call stack, and how does R know what order to do things in?
How does scope work in R?
06:10	15. Loops in R	How can I do the same thing multiple times more efficiently in R?
What is vectorization?
Should I use a loop or an apply statement?
06:40	Finish

## Notes
Our goal is not to teach you R, but to teach you the basic process of doing a data science project that many other  programming language like Java and Python can do. We use R in our lessons because:

+ we have to use something for examples;
+ it’s free, well-documented, and runs almost everywhere;
+ it has a large (and growing) user base among scientists; and
+ it has a large library of external packages available for performing diverse tasks.

But the two most important things are to use whatever language your colleagues are using, so you can share your work with them easily, and to use that language well. apparently. R is the most used language in Data Science

## Convention

info

```{block2, Info, type='rmdinfo'}
R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under the terms of the
GNU General Public License versions 2 or 3.
For more information about these matters see
http://www.gnu.org/licenses/.
```
Instruction

```{block2, Inst, type='rmdinstruction'}
R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under the terms of the
GNU General Public License versions 2 or 3.
For more information about these matters see
http://www.gnu.org/licenses/.
```

Todo

```{block2, ToDo, type='rmdaction'}
R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under the terms of the
GNU General Public License versions 2 or 3.
For more information about these matters see
http://www.gnu.org/licenses/.
```

hints

```{block2, hint, type='rmdnote'}
R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under the terms of the
GNU General Public License versions 2 or 3.
For more information about these matters see
http://www.gnu.org/licenses/.
```

## Acknowledgements {-}

1. A beginner’s guide to Kaggle’s Titanic problem
Sumit Mukhija (https://towardsdatascience.com/a-beginners-guide-to-kaggle-s-titanic-problem-3193cb56f6ca)

2. 
https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy

Machine Learning for Dummies by John Mueller and Luca Massaron - Easy to understand for a beginner book, but detailed to actually learn the fundamentals of the topic


This is a _sample_ book written in **Markdown**. You can use anything that Pandoc's Markdown supports, e.g., a math equation $a^2 + b^2 = c^2$.

The **bookdown** package can be installed from CRAN or Github:


Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading `#`.

To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): <https://yihui.name/tinytex/>.

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

```{block, type='rmdcaution'}
Some text for this block.
```

<!--chapter:end:00-preface.Rmd-->

# Introduction {#intro}

```{r fig.align = 'center', out.width = "50%", echo=FALSE}

knitr::include_graphics(here::here("images", "dataScientist.jpeg"))

```


```{block2, type='rmdnote'} 




"What profession did Harvard call the Sexiest Job of the 21st Century?"^[Data Scientist: The Sexiest Job of the 21st Century. (https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century).  A version of this article appeared in the October 2012 issue of Harvard Business Review.(https://hbr.org/archive-toc/BR1210) ]

...

That’s right… 

The data scientist.

```



Ah yes, the ever mysterious data scientist. So what exactly is the data scientist’s secret sauce, and what does this “sexy” person actually do at work every day? How so they do it?



## What is Data Science?

Data science is a multidisciplinary filed. It blends of data mining, data analysis, statistics, algorithm development, machine learning and advanced computing and software technology together in order to solve analytically complex problems. Its ultimate goal is to reveal insight of data and get the data value for business.

```{r include=TRUE, fig.cap='Concept of Data Science', out.width='80%', fig.asp=.75, fig.align='center', echo = FALSE}
knitr::include_graphics("img/whatisdatascience.jpg")
```


### Data science as Discovery of Data Insight
This aspect of data science is all about uncovering hidden patterns from data. Diving in at a granular level to mine and understand complex patterns, trends, and relations. It's about surfacing hidden insight that can help and enable companies to make smarter business decisions and take appropriate actions to gain competitive advantages in the market. For example:

* Amazon build recommendation system to provide users suggestion on purchase based on the user's shopping history.
* Netflix data mines movie viewing patterns to understand what drives user interest, and uses that to make decisions on which Netflix original series to produce.
* Target identifies what are major customer segments within it's base and the unique shopping behaviors within those segments, which helps to guide messaging to different market audiences.
* Proctor & Gamble utilizes time series models to more clearly understand future demand, which help plan for production levels more optimally.
How do data scientists mine out insights? It starts with data exploration. When given a challenging question, data scientists become detectives. They investigate leads and try to understand pattern or characteristics within the data. This requires a big dose of analytical creativity.

How do data scientists mine data insights? there is a procedure to follow. It generally starts with data description it is called Described data analysis (DDA) to get first sight on the data sets available. DDS will help data scientist to grasp the quantity and quality of the data. so they can decide how to deal with the data. it then generally followed by data cleaning, manipulation, transform and attributes engineering etc, together called preprocess. Data preprocess is also generally combined with exploratory data analysis (EDA). When given a challenging question, data scientists normally become detectives. They investigate all the information available and follow any possible leads and try to understand pattern or characteristics within the data. This not only requires huge amount tools and techniques but also demand analytical creativity .

Then as needed, data scientists may apply quantitative technique in order to get a level deeper – e.g. statistical methods, projections, inferential models, segmentation analysis, time series forecasting, synthetic control experiments, etc. The intent is to scientifically piece together a forensic view of what the data is really saying.

This data-driven insight is central to providing strategic guidance. In this sense, data scientists act as consultants, information provider help business stakeholders on how to act on findings.

### Data science as Development of Data Product

A "data product" is a technical asset that: 

1. utilizes data as input, and 
2. processes that data to return algorithmically-generated results. 

A typical example is users' scoring system. It takes users profile or/and behavior data as input and with a complex scoring engine, it produces a credit score of the users for business decision making. 
Another example of a data product is a recommendation engine, which ingests user data, and makes personalized recommendations based on that data. 
Here are some examples of data products:

* Amazon's recommendation engines suggest items for you to buy, determined by their algorithms. 
* Netflix recommends movies to you. Spotify recommends music to you.
* Gmail's spam filter is data product – an algorithm behind the scenes processes incoming mail and determines if a message is junk or not.
* Computer vision used for self-driving cars is also data product – machine learning algorithms are able to recognize traffic lights, other cars on the road, pedestrians, etc.

This is different from the "data insights" section above, where the outcome to that is to perhaps provide advice to an executive to make a smarter business decision. In contrast, a data product is technical functionality that encapsulates an algorithm, and is designed to integrate directly into core applications. Respective examples of applications that incorporate data product behind the scenes: Amazon's homepage, Gmail's inbox, and autonomous driving software.

Data scientists play a central role in developing data product. This involves building out algorithms, as well as testing, refinement, and technical deployment into production systems. In this sense, data scientists serve as technical developers, building assets that can be leveraged at wide scale.

## What is Data Scientist?
Data scientists are a new breed of analytical data expert who have the technical skills to solve complex problems – and the curiosity to explore what problems need to be solved. They are part mathematician, part computer scientist and part business trend-spotter. They straddle in both the business and IT worlds with mathematical and programming weaponry.  

### The Requisite Skill Set 
Data scientist needs a blend of skills in three major areas:


1. Mathematics
2. Computing and Software Engineering
3. Business 

```{r fig.cap='Quality of Data Scientists', out.width='60%', fig.asp=.75, fig.align='center', echo=FALSE}
knitr::include_graphics("img/datascientist.jpg")
```

#### Mathematics Narrator

At the heart of mining data insight and building data product is the ability to view the data through a quantitative lens. There are textures, dimensions, and correlations in data that can be expressed mathematically. Finding solutions utilizing data becomes a brain teaser of heuristics and quantitative technique. Solutions to many business problems involve building analytic models grounded in the hard math, where being able to understand the underlying mechanics of those models is key to success in building them.

Also, a misconception is that data science all about **statistics**. While statistics is important, it is not the only type of math utilized. First, there are two branches of statistics – classical statistics and Bayesian statistics. When most people refer to stats they are generally referring to classical statistics, but knowledge of both types is helpful. Furthermore, many inferential techniques and machine learning algorithms lean on knowledge of **linear algebra**. For example, a popular method to discover hidden characteristics in a data set is SVD, which is grounded in matrix math and has much less to do with classical stats. Overall, it is helpful for data scientists to have breadth and depth in their knowledge of mathematics.

#### Computing and Software Engineer Skills

Data is now collected, stored and processed with computer. With the increasing of data quantity, as termed as, we are enter the big data era. The conventional way of processing data facing unprecedented challenge. The personal computer may be not adequate to handle big data. Distributed storage, clouds computing and computer clusters become commonly-used platforms for data access and controls. Basic computing environment configuration and settings are common skills need to handle data. 

The data processing tools and languages like R or Python, and a database querying language like SQL are the common used languages in data process and data analyzing. It is also important to have a strong software engineering knowledge so it can be comfortable to  handle a large amount of data logging, and to develop data-driven products.

Data scientists need utilizing new technology in order to wrangle enormous data sets and work with complex algorithms, and to code or prototype quick solutions, as well as interact and integrate with complex data systems. Core languages associated with data science include SQL, Python, R, and SAS. On the periphery are Java, Scala, Julia, and others. But it is not just knowing language fundamentals. A data scientist is a technical ninja, able to creatively navigate their way through technical challenges in order to make their code work.

Along these lines, a data science is a solid algorithmic thinker, having the ability to break down messy problems and recompose them in ways that are solvable. This is critical because data scientists operate within a lot of algorithmic complexity. They need to have a strong mental comprehension of high-dimensional data and tricky data control flows. Full clarity on how all the pieces come together to form a cohesive solution.

#### Strong Business Acumen

It is important for a data scientist to be a tactical business consultant, an operation narrator and story teller. Working so closely with data, data scientists are positioned to learn from data in ways no one else can. They can understand the language the data speak and listen the story the data tells. That creates the responsibility to translate observations, discovery to shared knowledge, and contribute to strategy on how to solve core business problems. This means a core competency of data science is using data to cogently tell a story. No data present a cohesive narrative of problem and solution, using data insights as supporting pillars, that lead to guidance.

Having this business acumen is just as important as having acumen for technology and math and algorithms. There needs to be clear alignment between data science projects and business goals. Ultimately, the value doesn't come from data, math, and tech itself. It comes from leveraging all of the above to build valuable capabilities and have strong business influence.

### How to Become a Data Scientist?

Many people start to Position themselves for a career in data science. Not only for good job opportunities, but also for excitement of work in the technology field with freedom for experimentation and creativity. To get to this position you need solid foundations.

A conventional way of becoming a data scientist is Choosing a university that offers a data science degree. Or register yourself for courses that in data science and analytics fields. If you cannot do these, the option left to you is to learn by yourself. 

The knowledge and skills you should have are:

+ **Statistics and machine learning**. A good understanding of statistics is vital as a data scientist. You should be familiar with statistical tests, distributions, maximum likelihood estimators, etc. Statistics knowledge will also help you understand when different techniques are (or aren’t) a valid approach. Machine learning (ML) is a good weapon when you involve a big data project. Algorithms is the core of machine learning, although many implementations with R or Python libraries do exist and convenient to use, It is still needed a thorough understand how the algorithms works and when when it is appropriate to use different ones. 
+ **Coding languages such as R or Python**. It is essential, a data scientist is competent with a number of computing and data querying languages like R, Python and SQL. 
+ **Databases such as MySQL and Postgres**. Data is generally stored in a Database. it is important to have necessary skills for data access and control from a DBMS systems. The most commonly used DBMS systems are MySql (https://www.mysql.com/)  and Postgres (https://www.postgresql.org/) in addition to ACCESS and EXCEL. 
+ **Visualization and reporting technologies**. Visualizing and communicating data is incredibly important, especially with companies that are making data-driven decisions, or companies where data scientists are viewed as people who help others make data-driven decisions. When it comes to communicating, this means describing your findings, or the way techniques work to audiences, both technical and non-technical. Visualization can be immensely helpful. Therefore familiar with data visualization tools like matplotlib, ggplot, or d3.js. Tableau and dashboarding have become a popular data visualization tools. It is important to not just be familiar with the tools necessary to visualize data, but also the principles behind visually encoding data and communicating information.
+ **Big data platforms like Hadoop**.(https://hadoop.apache.org/) and **Spark** (https://spark.apache.org/). Although a lot of Data Science project can be tried, or at least prototyped on PC or workstations, it is reality that most large data analyzing is done on advanced computing platforms like distributed infrastructure or computer clusters. these advanced platform mostly deploy Hadoop ecosystems. 

If you don’t want to learn these skills on your own, take an online course or enroll in a bootcamp. Like what you do now. It not only provides you opportunity to gain knowledge quickly but also provides you chance of networking with other people who has the similar situation like you do. Connect with other people can lead you into an online community. They all will help you gain fine gran and insider knowledge of solving problems. 

## Process of Doing Data Science {#process}

Understand what data science is about is just a start of becoming a data scientist. Once the goal is set. The next task is to select a correct path and work hard to to reach your destination. The path is important which can be shorter or longer, or direct and smooth,  or curvy and bumpy. It is vital to follow a short and smooth path. This path is the data science project process. Figure \@ref(fig:process) is the 6 steps process, which is inspired by the CRISP (Cross Industry Standard Process for Data Mining) [@Chapman2000], [@Shearer2000] and KDD (knowledge discovery in data bases) process [@Li2018].  


```{r process, include=TRUE, fig.cap='Process of doing Data Science', out.width='100%', fig.asp=.75, fig.align='center', echo = FALSE}
knitr::include_graphics("img/process.png")
```


### Step 1: Understand the Problem - Define Objectives {#step1 -}
Any data analysis must begin with business issues. With business issues a number of questions should be asked. These questions have to be the right questions and measurable, clear and concise. Define analysis question is regarded as define a Data Requirements engineering and to get a a data project specification. It starts from a business issue and asking relevant questions, only after you fully understand the problem and the issues you may be able to turn practical problem into analytical questions. 

For example, start with a business issue: A contractor is experiencing rising costs and is no longer able to submit competitive contract proposals. One of many questions to solve this business problem might include: Can the company reduce its staff without compromising quality? Or, can company find alternative suppler on the production chain?

Once you have questions, you can start to thinking about data required for analysis. The data required for analysis is based on questions. The data necessary as inputs to the analysis can then be identified (e.g., Staff skills and performance). Specific variables regarding a staff member (e.g., Age and Income) may be specified and obtained. Data type can be defined as numerical or categorical.

After you defined you analytical questions. It is important to Set Clear evaluation of your project to measurement how success of your project. 

This generally breaks down into two sub-steps: A) Decide what to measure, and B) Decide how to measure it.

 **A) What To Measure?**
 
Using the contractor example, consider what kind of data you’d need to answer your key question. In this case, you would need to know the number and cost of current staff and the percentage of time they spend on necessary business functions. This is what is called in business as KPI - Key performance indicators. In answering this question, you likely need to answer many sub-questions (e.g., Are staff currently under-utilized? If so, what process improvements would help?). Finally, in your decision on what to measure, be sure to include any reasonable objections any stakeholders might have (e.g., If staff are reduced, how would the company respond to surges in demand?).

**B) How To Measure? **

Thinking about how you measure the success fo your data science project, the deep end is to measure some key performance indicators. They are the data you have chosen to use in the previous step. So measure your data is just as important, especially before the data collection phase, because your measuring process either backs up or discredits your project later on. Key questions to ask for this step include:

+ What is your time frame? (e.g., annual versus quarterly costs)
+ What is your unit of measure? (e.g., USD versus Euro)
+ What factors should be included? (e.g., just annual salary versus annual salary plus cost of staff benefits)

### Step 2: Undertand Data {#step2 -}
The second step is understand data. It includes **Data collection** and **Data Validation**. With problem understood and analytical questions defined and your validation criteria and measurements set, It is time to collect data. 


#### Data Collection {-}
Before collect data,  the data source has to be determined based on the relevance. Variety of data source may be assessed and accessed to get relevant data. These data source may include an existing databases, or organization's file system, or a third party service or even open web sources. They could provide redundant, or complementary, sometimes conflict data. it has to be cautious to select right data source from the very beginning. sometimes you need gather data via observation or interviews, then develop an interview template ahead of time to ensure consistency. it is a good idea to Keep your collected data organized in a log with collection dates and add any source notes as you go (including any data normalization performed). This practice validates your data and any conclusions down the road.

Data Collection is the actual process of gathering data on targeted variables identified as data requirements. The emphasis is on ensuring correct and accurate data collection, which means correct procedure was taken and appropriate measurements were adopted. the maximum efforts were spent to ensure the data quality. Remember that data Collection provides both a baseline to measure and a target to improve for a successful data science project. 

#### Data Validation {-}
Data validation id the process to Assess data quality. It is to ensure the collected data have reached quality requirements identified in the step 1, that is, they are useful and correct. The usefulness is the most important aspect. Regardless how accurate your data collections can be, if it is not useful, anything follows are just waste. It is hard to define the usefulness. depends on the problem at hand and the requirements for the find al delivery. The usefulness can various from a strong correlation between the raw data and the expected outcomes, to direct prediction power from the raw data to the concequence variables. Gernrally data validation can include:

+ Data type validation
+ Range and constraint validation
+ Code and cross-reference validation
+ Structured validation
+ Consistency validation
+ Relevancy validation 


### Step 3: Data Preprocess {#preprocess -}

Data preprocess is step that takes data processing method and technique to transforms raw data into a formatted and understandable form and ready for analyzing. Real world data is often incomplete, inconsistent, and is likely to contain many errors. Data preprocess is a proven method of resolving such issues. Tasks of data preprocess may include:

+ **Data cleaning**. The process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database. It normally includes identifying incomplete, incorrect, inaccurate or irrelevant data and then replacing, modifying, or deleting the dirty or coarse data. After cleansing, a data set should be consistent with other data sets. 

+ **Data editing**. The process involves changing and adjusting of collected data. The purpose is to ensure the quality of the collected data. Data editing should be done by fully understand the data collected and the data requirement specification. Editing data without them can be disastrous. 

+ **Data reduction**. The process and methods used to reduce data quantity to fit for analyzing. Raw data set collected or selected for analysis can be huge, then it could drastically slow down the analysis process. Reducing the size of the data set without jeopardizing the data analysis results is often desired. It includes records' number reduction and data attributes reduction. Methods used for reduce data records size includes **Sampling** and **Modelings** (e.g., regression or log-linear models or histograms, clusters, etc). Methods used for attributes reduction include **Feature selection** and **Dimension reduction**. Feature selection means removal of irrelevant and redundant features. such operation should not lose information the data set has. Data analysis algorithms work better if the dimensionality, which is the number of attributes in a data object is low. Data compression techniques (e.g., wavelet transforms and principal components analysis), attribute subset selection (e.g., removing irrelevant attributes as discussed in previous paragraph), and attribute construction (e.g., where a small set of more useful attributes is derived from the large numbers of attributes in the original data set) are useful techniques.    

+ **Data transformation** sometimes referred to as **data munging** or **data wrangling**. It is the process of transforming and mapping data from one data form into another format with the intent of making it more appropriate and valuable for downstream analytics. It is often that data analysis method requires data to be analyzed have certain format or possesses certain attributes. For example, classification algorithms require that the data be in the form of categorical (nominal) attributes; algorithms that find association patterns require that the data be in the form of binary attributes. Thus, it is often necessary to transform a continuous attribute into a categorical attribute, which is called **Discretization**, and both continuous and discrete attributes may need to be transformed into one or more binary attributes, whci iscalled **Banalization**. Other methods include **SCaling** and **normalization**.Scaling changes the bounds of the data, and can be useful, for example, when you are working with data in different units. Normalization scales data sets to a smaller range such as [0.0, 1.0].   

+ **Data re-engineering**
Re-engineering data is necessary when raw data come from many different data sources and in different format. Data re-engineering similar with data transformation can be done in both record level and in attributes level. Record level re-engineering is also called data **Integration**, which integrates variety of data into one file or place and in one format for analysis. for predictive analysis with a model, data re-enginering is also including split a given data set into two subsets called "Training" and "Test" Set.



### Step 4: Analyze Data{#analyse -}

After your collected data being preprocessed and suitable for analysis. Now you can drill down and attempt to answer your question from [Step 1](#step1) with the actions called Data Analyzing. It is the core activity in data science project process by writing, executing, and refining computer programs that utilize some analytical methods and algorithms to obtain insights from data sets. The methods in data analysis can be categorized into three major groups: **Descriptive data analysis (DDA)**, **Exploratory data analysis (EDA)** and **Predictive data analysis (PDA)**. DDA and EDA uses quantitative and statistical methods on data sets and data attributes measurements and their value distributions while DDA focus on numeric summary but EDA emphasis on graphical (plot) means. PDA on other hand may involve modeling and machine learning. Data analyzing is generally starting from Descriptive analysis, and goes further with Exploratory analysis. The most advanced methods are predictive analysis and machine learning. The later is built based on the results form the former methods. Some times mixed methods work better. 

#### **Descriptive data analysis** {-}

It is the simplest type of analysis. It describes and summarizes a data set quantitatively. Descriptive analysis\index{Descriptive analysis} generally starts with an univariate analysis, meaning describing a single variable (can also be called attribute, column or field) of the data. The appropriate depends on the level of measurement. For nominal variables, a frequency table and a listing of the modes are sufficient. For ordinal variables the median can be calculated as a measure of central tendency and the range (and variations of it) as a measure of dispersion. For interval level variables, the arithmetic mean (average) and standard deviation are added to the toolbox and, for ratio level variables, we could add the geometric mean and harmonic mean as measures of central tendency and the coefficient of variation as a measure of dispersion. However, there are many other possible statistics which covers areas such as location (“middle” of the data), dispersion (range or spread of data) and shape of the distribution. Moving up to two variables, descriptive analysis can involve measures of association such as computing a correlation coefficient or covariance. Descriptive analysis’ goal is to describe the key features of the sample numerically. It should shed light on the key numbers that summarize distributions within the data, may describe or show the relationships among variables with metrics that describe association, or by tables that cross tabulation counts. Descriptive analysis is typically the first step on the data analysis ladder, which only tries to get a sense of the data. 

#### **Explorative data analysis** {-}

Descriptive analysis\index{Descriptive analysis} is very important. However, numerical summaries can only get you so far. One problem is that it can only converting a large number of values down to a few summary numbers. Unsurprisingly, different samples with different distributions, shapes, and properties can result in the same summary statistics. This will cause problems. When you are looking a simple single summary statistic, the mean of a single variable, there can be a lot of possible “solutions” or samples. The typical example is Anscombe’s quartet [@Anscombe1973], it comprises four datasets that have nearly identical simple statistical properties, yet appear very different when graphed. Most kinds of statistical calculations rest on assumptions about the behavior of the data. Those assumptions may be false, and then the calculations may be misleading. We ought always to try and check whether the assumptions are reasonably correct; and if they are wrong we ought to be able to perceive in what ways they are wrong. Graphs are very valuable for these purposes.

EDA allows us to challenge or confirm our assumptions about the data. It is a good tool to be used in [data prerpocess](#preprocess). We often have pretty good expectations of what unclean data might look like, such as outliers, missing data, and other anomalies, perhaps more so than our expectations of what clean data might look like. With the more we understood data, we could develop our intuition of what factors and possible relations at are play. EDA, with its broad suite of ways to view the data points and relationships, provides us a range of lenses with which to study story that data is telling us. That in turn, helps us to come up with new hypotheses of what might be happening. Further,  if we understood which variables we can control, which levers we have to work within a system to drive the metrics such as business revenue or customer conversion in the desired direction. EDA can also highlight gaps in our knowledge and which experiments might make sense to run to fill in those gaps.

The basic tools of EDA are plots, graphs and summary statistics. Generally speaking, it’s a method of systematically going through the data, plotting distributions of all variables (using box plots), plotting time series of data, transforming variables, looking at all pairwise relationships between variables using scatterplot matrices, and generating summary statistics for all of them or identifying outliers. 

#### **Predictive data analysis ** {#predictive -}

Predictive analysis\index{Predictive analysis} builds upon **inferential analysis**, which is to learn about relationships among variables from an existing training data set and develop a model that can predict values of attributes for new, incomplete, or future data points. Inferential analysis is a type of analysis that from a dataset sample in hand infer some information, which might be parameters, distributions, or relationships about the broader population from which the sample came. We typically infer metrics about the population from a sample because data collection is too expensive, impractical, or even impossible to obtain all data. The typical process of inferential analysis includes testing hypothesis and deriving estimates. 
There are a whole slew of approaches and tools in predictive analysis. **Regression** is the broadest family of tools. Within that, however, are a number of variants (lasso, ridge, robust etc.) to deal with different characteristics of the data. Of particular interest and power is **Logistic Regression** that can be used to predict classes. For instance, spam/not spam used to be mostly predicted with a **Naïve Bayes predictor** but nowadays logistic regression is more common. Other techniques and what come under the term **Machine Learning** include neural networks, tree-based approaches such as classification and regression trees, random forests, support vector machines (SVM), and k-nearest neighbours. 

### Step 5: Results Interpretation and Evaluation {-}

After analyzing your data and get some answers about your original questions, it is possible that you need conduct further research and more analysis. Let us suppose that you are happy with the analysis results you have. It is finally time to interpret your results. As you interpret your analysis, keep in mind that you cannot ever prove a hypothesis true: rather, you can only fail to reject the hypothesis. Meaning that no matter how much data you collect, chance could always interfere with your results. Interpreting the results of analysis, you should thinking of how close the results address the original problems by asking yourself these key questions:

+ Does the data answer your original question? How?
+ Does the data help you defend against any objections? How?
+ Are there any limitation on your conclusions, any angles you haven’t considered?

If your interpretation of the data holds up under all of these questions and considerations, then you likely have come to a productive conclusion. However, there could be a chance that you may find you might need to revise your original question or collect more data and you may need to roll the ball from the starting line. Again. Either way, this initial analysis of trends, correlations, variations and outliers are not completely wasted. They help you focus your data analysis on better answering your question and any objections others might have. That is the next step report and communication. 

### Step 6: Data Report and Communication) {-}

Whereas the analysis phase involves programming and run programs on different computer platforms, the reporting involves narrative the results of analysis, thinking how close the results address the original problems and communicating about the outputs of analyses with interesting parties in many cases in visual formats.
During this step, data analysis tools and software are helpful but visual tools are intuitive and worth a lot of words. Visio, tableau (https://www.tableau.com/), Minitab (https://www.minitab.com/) and Stata are all good software packages for advanced statistical data analysis. There are also plenty of open source data visualization tools available.

It is important to note that the above 6 steps process is not a linear process. Any discovery of useful relationships and valuable patterns are enabled by a set of iterative activities. Iteration can occur in a single step or in a few steps in any point in the process.    


## Tools used in Doing a Data Science Project

Data Scientists use traditional statistical methodologies that form the core backbone of Machine Learning algorithms. They also use Deep Learning algorithms to generate robust predictions. Data Scientists use the following tools and programming languages:

### R {-}

R (https://www.r-project.org/) is a scripting language that is specifically tailored for statistical computing and data. It is widely used for data analysis, statistical modeling, time-series forecasting, clustering etc. R is mostly used for statistical operations. It also possesses the features of an object-oriented programming language. R is an interpreter based language and is widely popular across multiple industries particularly for doing data Science projects.

### Python {-}

Like R, Python (https://www.python.org/) is an interpreter based high-level programming language. Python is a versatile language. It is mostly used for Data Science and Software Development. Python has gained popularity due to its ease of use and code readability. As a result, Python is widely used for Data Analysis, Natural Language Processing, and Computer Vision. Python comes with various graphical and statistical packages like Matplotlib, Numpy, SciPy and more advanced packages for Deep Learning such as TensorFlow, PyTorch, Keras etc. For the purpose of data mining, wrangling, visualizations and developing predictive models, we utilize Python. This makes Python a very flexible programming language.

### SQL {-}

SQL stands for Structured Query Language. Data Scientists use SQL for managing and querying data stored in databases. Being able to extract data from databases is the first step towards analyzing the data. Relational Databases are a collection of data organized in tables. We use SQL for extracting, managing and manipulating the data. For example, A Data Scientist working in the banking industry uses SQL for extracting information of customers. While Relational Databases use SQL, **NoSQL** is a popular choice for non-relational or distributed databases. Recently NoSQL has been gaining popularity due to its flexible scalability, dynamic design, and open source nature. MongoDB, Redis, and Cassandra are some of the popular NoSQL databases.

### Hadoop {-}

Big data is another trending term that deals with management and storage of huge amount of data. Data is either structured or unstructured. A Data Scientist must have a familiarity with complex data and must know tools that regulate the storage of massive datasets. One such tool is Hadoop (https://hadoop.apache.org/). While being open-source software, Hadoop utilizes a distributed storage system using a model called **MapReduce**. There are several other packages in Hadoop together formed a Apache ecosystem, such as Apache Pig, Hive, HBase etc. Due to its ability to process colossal data quickly, its scalable architecture and low-cost deployment, Hadoop has grown to become the most popular software for Big Data.

### Tableau {-}

Tableau (https://www.tableau.com/) is a Data Visualization software specializing in graphical analysis of data. It allows its users to create interactive visualizations and dashboards. This makes Tableau an ideal choice for showing various trends and insights of the data in the form of interactable charts such as Treemaps, Histograms, Box plots etc. An important feature of Tableau is its ability to connect with spreadsheets, relational databases, and cloud platforms. This allows Tableau to process data directly, making it easier for the users.

### Weka {-}

For Data Scientists looking forward to getting familiar with Machine Learning in action, Weka (https://www.cs.waikato.ac.nz/ml/weka/) is, can be,  an ideal option. Weka is generally used for Data Mining but also consists of various tools required for Machine Learning operations. It is completely open-source software that uses GUI Interface making it easier for users to interact with, without requiring any line of code.

## Applications of Data Science

Data Science has created a strong foothold in several industries such as Government and education, Healthcare and medicine, banking and commerce, manufacturing and transportation etc. It has immense applications and has variety of uses. Some of the applications of Data Science are listed below:

### Data Science in Healthcare {-}
Data Science has been playing a pivotal role in the Healthcare Industry. With the help of classification algorithms, doctors are able to detect cancer and tumors at an early stage using Image Recognition software. Genetic Industries use Data Science for analyzing and classifying patterns of genomic sequences. Various virtual assistants are also helping patients to resolve their physical and mental ailments.

### Data Science in E-commerce {-}
Amazon uses a recommendation system that recommends users various products based on their historical purchase. Data Scientists have developed recommendation systems predict user preferences using Machine Learning.

### Data Science in Manufacturing {-}
Industrial robots have made taken over mundane and repetitive roles required in the manufacturing unit. These industrial robots are autonomous in nature and use Data Science technologies such as Reinforcement Learning and Image Recognition.

### Data Science as Conversational Agents {-}
Amazon’s Alexa and Siri by Apple use Speech Recognition to understand users. Data Scientists develop this speech recognition system, that converts human speech into textual data. Also, it uses various Machine Learning algorithms to classify user queries and provide an appropriate response.

### Data Science in Transport {-}
Self Driving Cars use autonomous agents that utilize Reinforcement Learning and Detection algorithms. Self-Driving Cars are no longer fiction due to advancements in Data Science.


## Summary {-}
While Data Science is a vast subject, being an aggregate of several technologies and disciplines, it is possible to acquire these skills with the right approach. In the end, Data Science is a very robust field that best fits people who have a knack for experimentation and problem-solving. With a large number of applications, Data Science has become the most versatile career.

## Exercise 1 {-}
1. Explain Data Science in your own term. What is relation between data science and Data mining, Data Science with Data Analytics? 
2. What a Data Scientist do? What skills a data SCientist should have?
3. How you interpret the saying that "Data Scientist is a detective. An investigation into datasets may not results in a plausible conclusion". How do you explain digging value out of data in this situation?

Analytics and machine learning – how it ties to data science
There are a slew of terms closely related to data science that we hope to add some clarity around.

What is Analytics?
Analytics has risen quickly in popular business lingo over the past several years; the term is used loosely, but generally meant to describe critical thinking that is quantitative in nature. Technically, analytics is the "science of analysis" — put another way, the practice of analyzing information to make decisions.

Is "analytics" the same thing as data science? Depends on context. Sometimes it is synonymous with the definition of data science that we have described, and sometimes it represents something else. A data scientist using raw data to build a predictive algorithm falls into the scope of analytics. At the same time, a non-technical business user interpreting pre-built dashboard reports (e.g. GA) is also in the realm of analytics, but does not cross into the skill set needed in data science. Analytics has come to have fairly broad meaning. At the end of the day, as long as you understand beyond the buzzword level, the exact semantics don't matter much.

What is the difference between an analyst and a data scientist?
"Analyst" is somewhat of an ambiguous job title that can represent many different types of roles (data analyst, marketing analyst, operations analyst, financial analyst, etc). What does this mean in comparison to data scientist?

Data Scientist: Specialty role with abilities in math, technology, and business acumen. Data scientists work at the raw database level to derive insights and build data product.
Analyst: This can mean a lot of things. Common thread is that analysts look at data to try to gain insights. Analysts may interact with data at both the database level or the summarized report level.
Thus, "analyst" and "data scientist" is not exactly synonymous, but also not mutually exclusive. Here is our interpretation of how these job titles map to skills and scope of responsibilities:


What is Machine Learning?
Machine learning is a term closely associated with data science. It refers to a broad class of methods that revolve around data modeling to (1) algorithmically make predictions, and (2) algorithmically decipher patterns in data.

Machine learning for making predictions — Core concept is to use tagged data to train predictive models. Tagged data means observations where ground truth is already known. Training models means automatically characterizing tagged data in ways to predict tags for unknown data points. E.g. a credit card fraud detection model can be trained using a historical record of tagged fraud purchases. The resultant model estimates the likelihood that any new purchase is fraudulent. Common methods for training models range from basic regressions to complex neural nets. All follow the same paradigm known as supervised learning.
Machine learning for pattern discovery — Another modeling paradigm known as unsupervised learning tries to surface underlying patterns and associations in data when no existing ground truth is known (i.e. no observations are tagged). Within this broad category of methods, the most commonly used are clustering techniques, which algorithmically detect what are the natural groupings that exist in a data set. For example, clustering can be used to programmatically learn the natural customer segments in a company's user base. Other unsupervised methods for mining underlying characteristics include: principal component analysis, hidden markov models, topic models, and more.
Not all machine learning methods fit neatly into the above two categories. For example, collaborative filtering is a type of recommendations algorithm with elements related to both supervised and unsupervised learning. Contextual bandits are a twist on supervised learning where predictions get adaptively modified on-the-fly using live feedback.

This wide-ranging breadth of machine learning techniques comprise an important part of the data science toolbox. It is up to the data scientist to figure out which tool to use in different circumstances (as well as how to use the tool correctly) in order to solve analytically open-ended problems.

What is Data Munging?
Raw data can be unstructured and messy, with information coming from disparate data sources, mismatched or missing records, and a slew of other tricky issues. Data munging is a term to describe the data wrangling to bring together data into cohesive views, as well as the janitorial work of cleaning up data so that it is polished and ready for downstream usage. This requires good pattern-recognition sense and clever hacking skills to merge and transform masses of database-level information. If not properly done, dirty data can obfuscate the 'truth' hidden in the data set and completely mislead results. Thus, any data scientist must be skillful and nimble at data munging in order to have accurate, usable data before applying more sophisticated analytical tactics.

<!--chapter:end:01-intro.Rmd-->

---
title: "Setup and famililar with your working environment"
author: "Gangmin Li"
date: "9/25/2020"
output: html_document
---

# Get Your Tools Ready {#tools}

```{r fig.align = 'center', out.width = "80%", echo=FALSE}

knitr::include_graphics(here::here("images", "tools.jpg"))

```



工欲善其事，必先利其器

An artisan must first sharpen his tools if he is to do his work well.


-- 孔子《论语》
   Confucius . Analects


Since this book is "Do Data Science". It means learn data science by doing. First of all we need to get our weaponry or tools ready.

We already knew that there are a list of tools used by data scientists. Apart from the personal preference, the most used tool is R. This book will use R as the tools to do a complete data science project. However this is not a R language book, it will not teach you about R language and how to use it. It will simply demonstrate a data science project completion step by step, which is completed with R language. 

By doing, I mean that you can simply mimic what I have done and follow along by typing or copy past my code into your working space, observe the effects and the results of each line of code execution. Thinking of why I have to do this and what results can I expect along the line of data science project's process. monitoring the issue raised and the methods used to resolve the issues. It is a hope that at some points you can have your own thoughts, perhaps your own code, methods and experiments. Once that is achieved. the goals are reached.


## Brief introductiuon about R and RStudio
R is one of the most widely used programming languages for statistical modeling. It has become the lingua franca of Data Science. Being open-source, R enjoys community support of avid developers who work on releasing new packages, updating R and making it a steady and fast programming package for Data Science.   

### Features of R Programming
R Programming has the following features:

 + R is a comprehensive programming language that provides support for procedural programming involving functions as well as object-oriented programming with generic functions.
 + R can be extended easily. There are over 10,000 packages in the repository of R programming. With these packages, one can make use of extended functions to facilitate easier programming.
+ Being an interpreter based language, R produces a machine-independent code that is portable in nature. Furthermore, it facilitates easy debugging of the code.
+ R supports complex operations with vectors, arrays, data frames as well as other data objects that have varying sizes. 
+ R can be easily integrated with many other technologies and frameworks like Hadoop and Spark. It can also integrate with other programming languages like C, C++, Python, Java, FORTRAN, and JavaScript.
+ R provides robust facilities for data handling and storage.
As discussed in the above section, R has extensive community support that provides technical assistance, seminars and several boot camps to get you started with R.
+ R is cross-platform compatible. R packages can be installed and used on any OS in any software environment without any changes.

### R Scripts
R is the primary statistical programming language for performing modeling and graphical tasks. so it can run in command line as an interpreting languages. However, With its extensive support for performing increasingly complex  computations such as manipulations on matrix and dataframes, R is now mostly running in script for a variety of tasks that involve complex datasets with complex operations.

There is plenty of editing tools which perform interactions with the native R console. With any one of them you can edit and run R script. You can also simply import extra packages and use the provided functions to achieve results with minimal number lines of code. There are several editors and IDEs that facilitate GUI features for authoring and executing R scripts. Some of the useful editors that support the R programming language are: RGui (R Graphical User Interface) and RStudio, a integrated R script development environment. 

This book will NOT teach you how to code in R. Learning R and to code in R language is not so hard. It just requires a lot of trials and time-spending. You can always going online and searching on Google, Baidu or [stackoverflow](https://stackoverflow.com/). There are also plenty of examples and code. The chances are if you’re trying to figure out how to do something in R, other people have tried as well, so rather than banging your head against the wall, look online. There are also some books available to help you out on this front as well. I suggest looking other people’s code and run it to see the results. R manual is always handy and is available in [here](https://cran.r-project.org/manuals.html) .

If you want learn R systematically, there are many sources online providing good tutorials. You can try to learn more R language from R tutorials. Tutorialspoint (http://www.tutorialspoint.com/r/index.htm), codecademy (https://www.codecademy.com/). If you prefer an online interactive environment to learn R, this free R tutorial by DataCamp (https://www.datacamp.com/courses/free-introduction-to-r) is a great way to get started.

### R Graphical User Interface (RGui)

RGui is a standard GUI (Graphic User Interface) platform comes with a R release. By default it provides two windows: R Console (on the left) and R Editor (on the right). See: Figure \@ref(fig:rgui) 

```{r rgui, fig.cap='Screen capture of RGui: where Console i son the left and Editor is on teh right', out.width='95%', fig.asp=.75, fig.align='center', echo=FALSE}
knitr::include_graphics("img/RGui.jpg")
```

**R Console** is an essential part of the RGui. In this window, we input various instructions, commands and scripts for different operations. The results of any operation or instruction execution are displayed at the console window including warning and error messages. Console window utilizes several other useful tools embedded to facilitate and ease of various of operations. The console window appears whenever you access the RGui. 

**R Editor** is an simple build-in text editor. Where you can create new R script, edit, test and debug the script and save it into a file. To lunch R Editor, in the main panel of RGui, go to the "File" menu and select the "New Script" option. This will lunch R Editor and allow you create a new script in R. R Editor has a function of "Run line or selection". It means you can debug your code by line or selection. It is very convenient tool for debugging. 

### RStudio

RStudio \index{RStudio}(https://rstudio.com/products/rstudio/) is an integrated and comprehensive Integrated Development Environment (IDE) for R. It facilitates extensive code editing, debugging and development. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management. Figure \@ref(fig:rstudio) is a screen shot of the RStudio. 


```{r rstudio, fig.cap='Screen capture of RStudio with integrated R code developemtn environment', out.width='90%', fig.asp=.75, fig.align='center', echo=FALSE}

knitr::include_graphics("img/RStudio.png")

```

Here are some distinctive features provided by the RStudio:

+ **An IDE that was built just for R**.  With Syntax highlighting, code completion, and smart indentation. It can execute R code directly from the source editor. it can quickly jump to function definitions
+ **Bring your workflow together**. Integrated R help and documentation with easily manage multiple working directories using projects and Workspace browser and data viewer
+ **Powerful authoring & Debugging**. Interactive debugger to diagnose and fix errors quickly and extensive package development tools can authoring with Sweave and R Markdown

RStudio is available in open source and commercial editions and runs on the desktop (Windows, Mac, and Linux) or in a browser connected to RStudio Server or RStudio Server Pro.

We will use RStudio for the whole book. The detailed RStudio IDE is explained in Section \@ref(rs).

## Downlaod and Install R and RStudio

It is simple to download and install both R and RStudio.

### R Download and Installation

To download R, please either directly from here (http://cran.us.r-project.org/bin/windows/base) or your preferred CRAN mirror (https://cran.r-project.org/mirrors.html). If you have questions about R like how to download and install the software, or what the license terms are, please read the answers to frequently asked questions (http://cran.r-project.org/faqs.html).  

Once you have chosen a site and click the download, you will will see Figure \@ref(fig:rd),

```{r rd, fig.cap='Screen capture of R dowanload papge from CRAN', out.width='80%', fig.asp=.75, fig.align='center', echo=FALSE}

knitr::include_graphics("img/R.png")

```

Pickup your platform and download the latest version (4.0.2), follow instruction to install it (Assume you choose Windows). In Windows, double click downloaded executable file, you will see [this](fig:rinstall) (as shown in Figure \@ref(fig:rinstall)),

```{r rinstall, fig.cap='Screen capture of R install in Windows', out.width='60%', fig.asp=.75, fig.align='center', echo=FALSE}

knitr::include_graphics("img/Rinstall.png")

```

Click 'Run', and answer the security message with 'Yes'. Choose your language (English),

```{r rinstlang, fig.cap='Screen capture of R install in Windows', out.width='50%', fig.asp=.75, fig.align='center', echo=FALSE }

knitr::include_graphics("img/Rinstlang.png")

```


Click 'Ok'. And follow the instructions on screen by click 'Next', until the whole process is complete, click 'Finish'. You now  have a version (choose 64bit) R installed.  The installation program will create the directory "C:\\Program Files\\R\\\<your version>", according to the version of R that you have installed.
The actual R program will be "C:\\Program Files\\R\\\<your version>\bin\\Rgui.exe". A windows "shortcut" should have been created on the desktop and/or in the start menu. You can launch it any time you want by click on it. 

### RStudio Download and Installation

To download RStudio, to go Rstudio products Web page (https://rstudio.com/products/rstudio/). Choose "RStudio Desktop" between "RStudio Serve" and "RStudio Desktop". See, Figure \@ref(fig:rstudio1),



```{r rstudio1, fig.cap='Screen capture of RStudio selection', out.width='50%', fig.asp=.75, fig.align='center', echo=FALSE }

knitr::include_graphics("img/Rstudio1.png")

```
After choosing the desktop version it will take you to a page (http://www.RStudio.org/download/desktop). Where  several possible downloads are displayed, different one for each operating systems. However, the webpage was designed that it can automatically recommends the download that is most appropriate for your computer. Click on the appropriate link, and the RStudio installer file will start downloading.

Once it is finished downloading, open the installer file and answer all on screen questions or click "next" in the usual way to install RStudio. 

After it is finished installing, you can launch RStudio from windows start button.. 

As we explained in the previous section, Rstudio is a comprehensive and integrated development environment. It can be overwhelming for people who contact it in the first time. Next section we will introduce its interface in great details. 

### Familiar with RStudio interface {#rs}

Open RStudio and you will see a rather sophisticated interface. Apart from the usual top level manual like "File Edit ...", there are four panes. I labeled 1 to 4 on the following image (Figure \@ref(fig:RStudio)), these panels are called **pane**\index{pane} in RStudio.

```{r RStudio, fig.align = 'center', out.width = "95%", fig.cap = "RStudio interface", echo=FALSE }
knitr::include_graphics(here::here("img", "Rstudio.jpg"))
```


RStudio does allow you to move panes around in the options menu, and also select tabs you want. Before you can missing around and lost yourself on the way. Let us stick on this default layout ion the moment. It is waht you see when you first lunch it, so we’ll act as though it’s standard. 

#### Pane 1: Script Pane - View Files and Data {-}
Script pane appears by default in the top left of the RStudio interface. it is where you enter your script and code, you can edit and debug your code or your script. 

This pane also display files When you click on a data file in the Workspace pane (top right, number 2 on the above image), or open a file from the Files pane (right bottom, number 3 on the above image), the results will appear in Pane 1. Each file opens in its own tab, and you can navigate between tabs by clicking on them (or using keyboard shortcuts).

#### Pane 2: WorkSpace Pane - Environment and History {-}
Workspace pane appears by default in the top right of the RStudio interface. It has four tabs by defult: **Environment**, **Histroy**, **Connection** and **Tutorial**. among these 4, the Environment is the default and it is selected. It shows a list of all the objects you have loaded into your workspace. For example, all datasets you have loaded will appear here, along with any other objects you have created (special text formats, vectors, etc). see this image (Figure \@ref(fig:RStudioEven)):


```{r RStudioEven, fig.align = 'center', out.width = "80%", fig.cap = "RStudio Environemnt Tab in WorkSpace Pane", echo=FALSE }
knitr::include_graphics(here::here("img", "environment.jpg"))
```


If you click on the **History** tab, you will see the complete history of code you have typed, over all sessions, as in this image (Figure \@ref(fig:RStudiohist)):

```{r RStudiohist, fig.align = 'center', out.width = "80%", fig.cap = "RStudio Histroy Tab in WorkSpace pane", echo=FALSE }
knitr::include_graphics(here::here("img", "history.jpg"))
```

The history is searchable, so you can use the search box at the upper right of the pane to search through your code history. If you find a line of code you want to re-run, just select it and click the “To Console” button as shown below (\@ref(fig:RStoconsole)): 

```{r RStoconsole, fig.align = 'center', out.width = "95%", fig.cap = "RStudio \"To Console\" button under History Tab in WorkSpace Pane", echo=FALSE }
knitr::include_graphics(here::here("img", "toconsole.jpg"))
```


You can also select any number of lines of scripts (by click with holding shift key) and click the "To Source" button, they will inset into source, See Figure \@ref(fig:tosource),


```{r tosource, fig.align = 'center', out.width = "95%", fig.cap = "RStudio \"To Source\" button under History Tab in WorkSpace Pane", echo=FALSE }
knitr::include_graphics(here::here("img", "tosource.jpg"))
```


#### Pane 3: Console Pane {-}

By default console pane appears at the bottom left. Console pane is the most important pane – the Console! This is where you enter your commands to be executed or your R code to do everything in the curriculum. The rest of the document will be largely concerned with working in the Console, with occasional references to other panes.
By default it also has 4 tabs: **Console**, **Terminal**, **R markdown** and **Jobs**. Apart from console, Other three, as their name suggested, they are the interface between you and other systems. Terminal is the interface between you and operating system, where you can have a direct interaction with OS, in our case it is the Windows. R markdown^[R Markdown is an authoring framework for data science. Using a single R Markdown file, data Scientists can save, execute R code and generate high quality reports that can be shared with other people.] is interface between you and the markdown compiler, if authoring a markdown file, every time you compile (knitr\index{Knitr}^[knitr is an engine for dynamic report generation with R. It is a package in the programming language R that enables integration of R code into LaTeX, LyX, HTML, Markdown, AsciiDoc, and reStructuredText documents.]) the code, system will report status in that window. Jobs is th interface between you and your job execution system. it is generally running on a remote server. 
Basically Console pane is the communication interface between you and systems. The information appears here are generally important if any proble,m occurs.  

#### Pane 4: Multifunction Pane {-}

The multifunction pane appears by default at the bottom right. It has many tabs. By default it opens the **Files** tab. My version it has **File**, **Plots**, **Package**, **Help** and **Viewer** tabs.   

##### **Files tab** {-}

This tab works like your file explorer. It shows you all the files you have in your RStudio account (your document in windows). The buttons underneath the tab allow you to do operations on the files like create a new folder, delete files. rename files and many more functions. which you normally do on file system. 

##### **Plots** {-}

When you run code in the Console pane that creates a plot, the plots tab will be automatically selected and the result of the plot generated will be displayed. See Figure \@ref(fig:plot))

```{r plot, fig.align = 'center', out.width = "95%", fig.cap = "RStudio **Plot** Tab under History Tab in Multifunction Pane", echo=FALSE }
knitr::include_graphics(here::here("img", "plot.jpg"))
```

Any time you want to view plots, you can select this tab manually, but it will be selected when you run plot code. Notice the arrow buttons at the top left of the pane; these allow you to scroll through all the plots you have created in a session.

##### **Packages** {-}

This tab allows you to see the list of all the packages (add-ons to the R code) you have access to, and which are loaded in already. You can also check packages in your system (installed) and the version of them.  

##### **Help** {-}

This tab will be automatically selected whenever you run help code in the Console, by type in console `? function`or type in script `help(function)`. It is very useful for beginner to get quick reference on any function or command you are not sure of. here is an example of asking help with `plot` function: 

```
help(plot)
```

You can access it at any time by clicking on the tab "Help" to see what the "Help" tab can offer. See Figure \@ref(fig:help),

```{r help, fig.align = 'center', out.width = "95%", fig.cap = "RStudio **Help** Tab under in Multifunction Pane", echo=FALSE }
knitr::include_graphics(here::here("img", "help.jpg"))
```


If you want to use the "Help" without using the help command, you can also use the search bar at the upper right of the tab to search within the R documentation.

##### **Viewer** {-}

Viewer tab in the multifunction pane is designed for view or display R markdown \index{R Markdown} results. If you are authoring a R notebook^[R Notebooks are an implementation of Literate Programming that allows for direct interaction with R while producing a reproducible document with publication-quality output.] or any Markdown file, your Knit\index{knit} results can be viewed by select "Preview in Viewer Pane". once this selection is made, you will see the notebook or your Markdown\index{Markdown} document will be displayed in the Viewer window and you will notice that the Viewer tab is automatically selected and the viewer window is also maximized. See Figure \@ref(fig:SSview) 

```{r SSview, fig.align = 'center', out.width = "95%", fig.cap = "RStudio **Viewer** Tab under in Multifunction Pane", echo=FALSE}
knitr::include_graphics(here::here("img", "RSview.png"))
```

RStudio allows a user to close or minimize certain panes or windows and focused on one or two panes. It also allows users to customize tabs in each pane. Check top level menu "View" for details. Figure \@ref(fig:RSview) illustrate the function.

```{r RSview, fig.align = 'center', out.width = "95%", fig.cap = "RStudio Pane change under the top level memu View", echo=FALSE }
knitr::include_graphics(here::here("img", "RSPaneview.png"))
```


RStudio provides large numbner of help functions, which can be explored under **Help** top level menu. One help is the keyBoard Shortcuts help. I find it is very useful. Figure \@ref(fig:rssc) shows the shortcuts. 

```{r rssc, fig.align = 'center', out.width = "95%", fig.cap = "RStudio KeyBoard Shortcuts", echo=FALSE }
knitr::include_graphics(here::here("img", "RSsc.png"))
```

RStudio is a complicated, comprehensive IED for R, R Markdown, R Notebook and many other R language developments and other languages like Java Python developments too. Its powerful functions can only be revealed and made useful after you have used it for a while. The more use it, the more likely you will find it is so easy to use. I will leave this for you to explore.   

## Bootsup your RStudio

Once you boots up your RStudio, you are ready to kick off your R coding. However, the first thing you may want to do is to set up your working directory. This will change the default location for all file input and output that you will do in the current session.

RStudio makes this easy, simply click "Session -> Set Working Directory -> Choose Directory…". See figure Figure \@ref(fig:setwk) below,

```{r setwk, fig.align = 'center', out.width = "95%", fig.cap = "Set workling Directory by Session", echo=FALSE }
knitr::include_graphics(here::here("img", "setwk.png"))
```

Then you need to navigate to where you want your project to be sit. For example, in my case I used "D:/Teach2020/short course/Data analysis - prediction with Rstudio/IntroToDataScience-master", it is silly to be so long, you can certainly set up for a shorter one. Anyway, the point is choose your won directory and remember it. If you tried it, you should notice that once you have chosen a directory, A command appeared in the Console pane and this is the command R executes when you set your working directory from the session menu. To achieve the same result you normally would have typed this manually in the console. See Figure \@ref(fig:setwkc) below,

```{r setwkc, fig.align = 'center', out.width = "95%", fig.cap = "Set a working directory with R command in Console", echo=FALSE }
knitr::include_graphics(here::here("img", "setwkc.png"))
```

Type command or instructions on command line at Console is what the general data scientists do when they try to analysis some data or prove some ideas. You can complete this tutorial at the command line in Console pane. I would suggest you, instead, creating a script to save all your hard work. This way you can easily reproduce the results or make changes without retyping everything.

To do so, you need to create a new file by click the "File ->New file", and select “R Script”. See Figure \@ref(fig:newrfile),

```{r newrfile, fig.align = 'center', out.width = "95%", fig.cap = "Creat a new file in RStudio", echo=FALSE }
knitr::include_graphics(here::here("img", "RStudionew.png"))
```

If you do so, you should notice that a new tab appeared on the script pane with a name of "Untitled1" and the script editor is now opened for you with the cursor flashes on line number 1. See Figure \@ref(fig:newrcfile),

```{r newrcfile, fig.align = 'center', out.width = "95%", fig.cap = "Creat a new file in RStudio and ready to enter code", echo=FALSE }
knitr::include_graphics(here::here("img", "RStudionewfile.png"))
```


Now in side the script editor, you can type you code! let us try this first, type 

```
# This is my first R code
```

and hit "Return", see next image (Figure \@ref(fig:newrcode)),

```{r newrcode, fig.align = 'center', out.width = "95%", fig.cap = "New R Script file with one line", echo=FALSE }
knitr::include_graphics(here::here("img", "new.png"))
```


Notice that the tab "Untitled1" has changed to red color and with a "*" as superscript. It means that the current file has been changed and not saved.

Go ahead and copy the `setwd` command from the console and paste it into your script. 

Now save the script to your working directory, give it a name my first R, or any name you prefer.

Now you have your first R code!

## Instructions

This book is intend to work in two ways: one way is to be used as a manual, you can follow along to accomplish a complete Data Science project; Another way is to be used as a company to my online video recordings. If you can get the video that is great. But if you cannot, it is also fine, The only drawback is you have read the whole contends line by line. 

I will use the following stickers to indicate the text is an explanation or an instruction or actions need you to do. So you know what you have to read word by word and what you can skip. 

### Code {-}

Code appears with code sticker. Like this, 

```r
# Load raw data
train <- read.csv("train.csv", header = TRUE)
test <- read.csv("test.csv", header = TRUE)
```

They are the one you have to read word by word and type (copy-past) into your script and run them. It is also a good idea to record the results or plot (graph results) into a file. So you can always come back to check them. 

### Tips {-}

Tips, like this one, 

```{block2, tip2, type='rmdtip'}
Within the console, you can use the up and down arrows to find recent commands, and hitting tab will auto-complete commands and object names if possible.
```

are general advice. You can skip them if you already know. They can save your time but not affect your learning.

### Actions {-}

Any actions, by default, are assumed you will act upon. It appears in action sticker,

```{block2, act, type='rmdact'}
Change data type
Go back to look into Kaggle to explain pclass: proxy for social class: richer or poor. It should be factor, it does not make sense to stay in int, we are not add or calculate with them

`data.combined$pclass <- as.factor(data.combined$pclass)`

```


particularly, they are in a sequential order. If you did not take previous actions you cannot do the current. It is possible you have processed some datasets and it is used later on. So you must carry out actions one by one, and not jump to the later ones without accomplish the earlier ones.  

### Exercise {-}

Exercises at the end of each chapter, are provided for you to periodically explore alternatives of a solution or to enhance some key techniques. It is always good if you can do the exercises. 

The default protocol is that I have some codes written and you will down load them and open in your RStudio. Then you need to type (or copy and past line by line into your file. you can run them and understand their functions and the reason to function like that. After you understand them you can change them or write some new code. While you are doing that, you simply comment out my code rather than delete them just in case you need to come back to look at them again. Once you can write your own code, it shows you have learned.  

Before you go, let's try it,

```{block2, act2-1, type='rmdact'}
Open a new project called "MyDataSciece",
Set up working directory as "~/MyDataScienceWithR",
Create a first R program called ``DSPR1``,
`setwk(~/MyDataScienceWithR)`  

```

Okay. Save your file and move to next Tutorial.

## Exercise 2 {-}

1. Learn R basics from R tutorials. 

+ Tutorialspoint (http://www.tutorialspoint.com/r/index.htm), 
+ codecademy (https://www.codecademy.com/). 
+ DataCamp (https://www.datacamp.com/courses/free-introduction-to-r) 

2. Write simple R code with RStudio IDE.

+ Try create a new R script and save it in a file
+ Open it from your file system and edit it 
+ Run it line by line
+ Run it in one go

3. Explore RStudio help functions.

+ Try to type "?plot" in Console
+ Try run help(plot) in editor
+ Explore plot from RStudio help 
+ Search "R plot" from Google or Bing

4. Explore project and working directory from RStudio.
5. Create a R project called "MyDataScienceProject". 



<!--chapter:end:02-tools.Rmd-->



# Understand Problem {#prob}


```{r fig.align = 'center', out.width = "80%", echo=FALSE, fig.cap ="The sink of Titanic. Credits: Canoe1967/wikipeida.org" }

knitr::include_graphics(here::here("images", "Titanic.jpg"))

```

>

The sinking of the RMS Titanic occurred on the night of 14 April 1912 in the North Atlantic Ocean, four days into the ship's maiden voyage from Southampton, UK to New York City, USA. The largest passenger liner in service at the time, Titanic had an estimated 2,224 people on board when she struck an iceberg at around 23:40 (ship's time) on Sunday, 14 April 1912. Her sinking two hours and forty minutes later at 02:20 (05:18 GMT) on Monday, 15 April resulted in the deaths of more than 1,500 people, which made it one of the deadliest peacetime maritime disasters in history.

Later, in 1997 American file director James Cameron turned this disastrous and tragic event into an epic romance film. The film star's Leonardo DiCaprio and Kate Winslet outstanding performance in the file makes it a best selling movie in the year. 

Perhaps people are touched not only by the love story but also by the humanity norms in the life and death situation that is famous - Lady and children first. 

>

## Kaggle Competion

Kaggle (https://www.kaggle.com/), a subsidiary of Google LLC, is the world's largest data science community with powerful tools and resources to help you achieve your data science goals. Kaggle was founded in 2010 with the idea that data scientists need a place to come together and collaborate on projects, learning new techniques and share each others experience. This has transformed into a network with more than 1,000,000 registered users, and has created a safe place for data science learning, sharing, and competition.  

Using the human competitive spirit, Kaggle created a platform for organizations to host competitions which have fueled new methodology and techniques in data science, and given organizations new insights from the data they provided. 

```{r fig.align = 'center', out.width = "80%", echo=FALSE, fig.cap = "Kaggle Competition web site"}

knitr::include_graphics(here::here("images", "Kagglecomp.png"))

```

Generally, Each competition has a host, and each host has to prepare and provide data. When providing data, the host has the opportunity to give additional information such as a description, evaluation method, timeline, and prize for winning. Although this may not be an ideal real world data problem, which data scientist may face in the business. But it provides a good starting point for learners. In a real world, you may need to start from understand the business and find data sources by your self. Although competition host has provided data. You cannot assume the data provided are clean data and ready for analysis. Cleaning and preprocess data are part of the competition. Therefore, any solution can be tested to see how good a participant is with the whole process of data science project.

## Titianic at Kaggel

Titanic perhaps is the oldest and most participated competition on the Kaggle competition site. Even Kaggle used it as sample project to show how people can participant in a competition and submit your results. 

```{r fig.align = 'center', out.width = "80%", echo=FALSE, fig.cap= "Kaggle Competition on Titanic"}

knitr::include_graphics(here::here("images", "Titaniccompetition.png"))

```

We take Titanic as an example through this tutorial because of the following reasons:

1. The story is well known and east to understand and communicate any actions and the cause of the actions in the analyze process. 
2. The competition has largest participants, so any issues are most likely have been studied already. So explore the discussion and other sources can help to solve any problem you may have.
3. It is well studied, so there are plenty of alternative training materials available for your reference. 
4. Lastly, the problem itself is interesting one that has a characteristic of only has a better solution and no best solution. So people are still working in it and uses the latest technologies. 

## The Titanic problem

The objective of the Titanic problem defined on the Kaggle website as stated in the following:

"The sinking of the Titanic is one of the most infamous shipwrecks in history.

On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.

While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.

In this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (i.e. name, age, gender, socio-economic class, etc/)."

### The challenge {-}
The competition is simple: we want you to **use the Titanic passenger data** (name, age, price of ticket, etc) to try to **predict who will survive and who will die**.

The requirement is to predict passengers' **survive**. Like many other real data science problems, [Prediction](#predictive) is to build a model which takes input data and produce an output. A prediction model is a mathematical formula that takes input from historical facts reflecting past event and produce a output that to make predictions about future or otherwise unknown events. A simple way to understand model is to think a model in the following three ways:

1. The relationship between input and output can be expressed by some kinds of math formula. It is generally called definable model, the math formula can be as simple as a function of Polynomial expression or as complected as a regression model, or other statistics models.
2. Some models can not be explicitly expressed with a math formula, instead they are expressed in rules. those are rule-based models. 
3. Other models can not be expressed in a math formula nor in rules. The solution is build a neural networks\index {neural networks} to do prediction. An Neural Networks can be regard as a "black box", which takes input and produce output, the internal connections are transparent to users. Machine learning is more focused on models rooted in Neural networks. 

Any model fundamentally expresses relationships between inputs and outputs. So as part of understanding the problem, We could interpret that the Kaggle Titanic challenge is to find creditable relationships between input data and out put data (which is survive or not). Once the relationship is found, we can express using either a math formula, a set of rules or a Neural Network model.

### The data {-}

Kaggle competition usually provides competition data. There is a "Data" tab on any competition site. Click on the Data tab at the top of the competition page, you will find the raw data provided and most of time there are brief explanation of the data attributes^[We have used Data Science terminology in here. Data represent  objects in natural world. Object's properties are represented by attributes. That is a data record has a number of attributes representing a natural object with a number of properties. records is also called observations or samples in statistics, property is also called variables, parameters or dimensions] too.  

There are three files in the Titanic Challenge: 

(1) train.csv, 
(2) test.csv, and 
(3) gender_submission.csv.

The training set is supposedly used to build your models. For the training set, it provides the outcome (also known as the “ground truth”) for each passenger. Your model will be based on attributes like passengers’ gender and class. You can also use feature engineering to create new features.

The test set should be used to see how well your model performs on unseen data. For the test set, there is no ground truth for each passenger is provided. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.

The data sets has also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.

### Submission {-}

Submission at the Titanic competition is equivalent to the requirements on the final report of any data science project. that is one of the questions you need to understand in the beginning of the project. 

Titanic competition requires the results need be submitted in the file. The file structure is demonstrated in the "gender_submission.csv". It is also provided as an example that shows how you should structure your results, which means predictions. 

The example submission in "Gender_submission" predicts that all female passengers survived, and all male passengers died. It is clearly biased. Your hypotheses regarding survival will probably be different, which will lead to a different submission file. Properly it is a good idea now to rename the "Gender_submission.csv" file into "My_submission.csv" now. So you know that you have to submit "my_submission.csv" as the final report of your project and the submission indicate your completion of your project. 

```{block2, act-download, type='rmdaction'}
Do it yourself:

1. Download data file from Kaggel web site.(https://www.kaggle.com/c/titanic/data)
2. Unzip it into your working directory.
3. Rename "Gender_submission.csv" file into "My_submission.csv".

```

Make sure your submission should have:

1. "PassengerId" column containing the IDs of each passenger from test.csv.
2. "Survived" column (that you will create!) with a "1" for the rows where you think the passenger survived, and a "0" where you predict that the passenger died.



## Reflection

The purpose of this tutorial is to understand the process of data science project. The first step, as indicated by the 6-step process in section \@ref(process), is "understand the problem". Real world problem is far more complicated than this well defined problem. Mostly  business organizations don't know the the exact problem (that is part of reason why they want data analysis or business analysis) or they know the problem (in general) but the problem can not br expressed explicitly or in terms of data. 

I have met a situation that a business organization has created a data center and collected all their business operational data. The boss asked to analyze these data and find:
1. Is there are problems? 
2. If yes, how to overcome these problems?
3. If not, how to improve the business operations?

You see, here the problem is how to define the problem? how to convert business problem into data science problem.

For the example, the first problem in the above list needs to know what is the normal or expected performance? How to evaluate the performance? In terms of turn over or profit? In what time scale? It could be a short of profit in the moment but it not causes alarm because the recent investment for developing a new market. At a long run it will have a great ROI (Return on Investment). The second problem demands to identify the cause of the problem and the third to identify the KIP (Key performance Indicators). they are both to identify the relationships between predictor and dependent variables. But they can be completely different sets.  

Understand problem is actually more complicated in real world. Until you have completely understood it and turned it into a list of analytical problems you can move to next step.

With the Titanic problem, combining the story and the requirements on the Kaggle web site, I would consider these:


- On April 14 and 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. The overall  survival rate is 32%.
- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.
- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.
- The story tells us that when they were get on board the lifeboats, they applied a policy of "women and children first" and also "the ship crew are the last".
- Sometimes the family was boarding the life boat together and some of the family members were swimming together too. 

Those thoughts form some kind of assumptions in mind. They will guide later data explorations. 



## Exercises 3 {-}

1. Go to Kaggle Titanic web site, explore the challenge and data provided.
2. Based on what you know about Titanic story, who you think can survive? can you describe the people you believe can survive in terms of age, sex, cabin. 


<!--chapter:end:03-problem.Rmd-->

---
title: "Plotting"
output:
  pdf_document:
    fig_caption: yes
---


# Understand Data


```{r fig.align = 'center', out.width = "80%", echo=FALSE, fig.cap ="" }

knitr::include_graphics(here::here("images", "buildingmaterial.png"))

```

Understand your building raw materials can help you choose correct tools and make the most use of them to construct your ideal buildings. 

**Understand data** is the foundation for solving analytical problems. The two major purposes of understand data are: 

1. Access data quantity 
2. Access data quality 
3. Set up objects for **data preprocess**

In practice the two initial data assessments can be done together or separately. The purpose of them is to setup objective for **data preprocess** to accomplish. The methods used to understand data can be both Descriptive analysis and Exploratory analysis.

## Load data 

Here are my initial plan for understand Titanic data:

1. Get Titanic data load into RStudio
2. Assess Data quantity (number of files, size of each file in number of records, number of attributes in each record)
3. Attributes types assessment
4. Attributes value assessment (numbers and summary, description).

Now, get your RStudio ready. 

If you have not done the Exercises 2.5, which asked you to create a new R project named "MyDataScienceProject". You can do it now.

Open your RStudio, Click File-> New project->New Directory -> choose New R Project", then, enter "MyDataScienceProject" in the Directory name box and select your directory. Click "Create Project" at the right bottom as shown in Figure \@ref(fig:newproject) 


```{r newproject, fig.align = 'center', out.width = "95%", fig.cap = "Create a new project in RStudio ", echo=FALSE }
knitr::include_graphics(here::here("img", "NewProject.png"))
```

Load file "TitanicDataScience1.R" into RStudio. create a new R file and name it "MyTitanicDataScience1". 

The protocol is you copy lines indicated from this tutorial "chunk" by "chunk" into your R file and run them. 

Okay let us start,

In your RStudio (WorkSpace), copy lines from "TitanicDataScience1.R" into your file "MyTitanicDataScience1",

```{r}
# Load raw data
train <- read.csv("train.csv", header = TRUE)
test <- read.csv("test.csv", header = TRUE)
```
You will see this in your Console,

```markdown
> train <- read.csv("train.csv", header = TRUE)
Error in file(file, "rt") : cannot open the connection
In addition: Warning message:
In file(file, "rt") :
  cannot open file 'train.csv': No such file or directory
> test <- read.csv("test.csv", header = TRUE)
Error in file(file, "rt") : cannot open the connection
In addition: Warning message:
In file(file, "rt") :
  cannot open file 'test.csv': No such file or directory
> 
```
Don't panic. let us look into it.

The first thing I want you to learn is to use RStudio help. Remember how to use it?

Now type **`? read.cvs`** in your Console, look at the Multifunction pane, the tab **Help** is auto selected and help message for **`read.cvs`** is appeared. See Figure \@ref(fig:Rhelp)


```{r Rhelp, fig.cap='Screen capture of Error and Help', out.width='95%', fig.asp=.75, fig.align='center', echo=FALSE }

knitr::include_graphics("img/Rhelp.png")

```

Now, notice that the error message says, "`cannot open file 'test.csv': No such file or directory`". We don't have file `train.csv` and `test.csv` in our working directory.

Now, Download the `traon.csv` and `test.csv` from Kaggle (if you did not download already) and stored into our project working directory^[The data files were asked to be downloaded and unzipped in the previous chapter \@ref(act-download) . If you simply unzip it into the working directory, it will exists in "~/Titanic/" directory. In this case, you need to move them into your working directory.].


Please note that it is a common practice that data scientist download datasets from data sources and save to a local drive. Having a local version of the raw datasets is good idea. But a lot of times, it is unfeasible to do so either because the data is too big or there are some access restriction prevent you have a local version. So, you have to using service provider's API or data URI through HTTP protocol or other protocol like FTP etc. 

Once, you have download the datasets from Kaggle website and unzipped (or moved) them into your local working directory, run the same code again by select them all and click "Run" or type "Ctr + Enter" .

You will see the two new attributes have been created and displayed in the **WorkSpace pane**. See Figure \@ref(fig:importdata).

```{r importdata, fig.cap='Screen capture of import raw data', out.width='95%', fig.asp=.75, fig.align='center', echo=FALSE }

knitr::include_graphics("img/importdata.png")

```


```{block2, ToDo41, type='rmdaction'}
Try yourself:

Import data from **WorkSpace pane** by click "**Import Dataset**" button.
```

## Assess Data Quantity 

After we have load the raw data into our WorkSpace, we can start to explore and exam the raw data.

In R code, the best way to explore a dataset and get the first impression on its size (number of records and numbers of attributes) is using `str()` function. If you wan tot know more about it, as I mentioned earlier, using help by typing `? str()` in your Console. There is an equivalent R code is called `help <statement>`, you can try `help str()`.

Now let us run the following code,

```
# Exam train and test datasets
train <- read.csv("train.csv", header = TRUE)
test <- read.csv("test.csv", header = TRUE)
```
You will see this in your console, Figure \@ref(fig:Rstr).

```{r Rstr, fig.cap='Screen capture of str(tain) and str(test)', out.width='130%', fig.asp=.75, fig.align='center', echo=FALSE }

knitr::include_graphics("img/RstrResult.png")

```

Firstly, you will see the size of the two datasets: 

- train has **891 records** and each record has **12 attributes**. Okay, R uses statistics terminology, observation is record in data science term. properties of an observation are attributes of a record. Notice that train has a type of data.from. Data.frame is the most used data type in R. (Try `?data.frame` to explore)
- test has **418 records** and each record has **11 attributes**, which are less than train's in both number of records and attributes. 

Dataset `test` has less number of records makes sense because any model you need large data to train and less data to test ( it will become clear later). However, why `test` has one less attribute? compare with `train`, it is easy to find out that the missing attribute is *Survived*. Do you understand now? The dataset `test` is supposedly to be used for testing our model (we will have it later) for predicting passengers' have lived and dead. So, it should not have a value now. The entire problem is for us to come up with a value on the attribute.   

RStudio has a conveniently build-in function to explore data size. At the **WorkSpace pane**, you can see the under **Environment** tab, the two attributes we have created are listed there. In font of each attribute there is a 
![](img/arrow.png) sign. click it you can exam its size and structure. It is equivalent to run `str()` R instruction. You can also lick on the attribute name to explore the entire dataset. 

```{block2,  type='rmdaction'}
Try yourself:

At RStudio **WorkSpace pane**, 

Click varaible name `train` and `test` to explore the contents of datasets.

Click on the ![](img/arrow.png) sign in front of attribute to explore it sstructure. 
```

## General Data Attributes Assessment

After a brief assessment on the data quantity, we know that the both datasets are not too big in terms of both number records (891 and 418) and number of attributes (12 and 11). We also have an intuitive understanding about the attributes, some obvious names like *Name*, *Sex* and *Age*; and some not so obvious names like *SibSp* and *Parch*. 

Before we looking into individual attributes (single variate analysis) in our datasets, let us get some general sense of all attributes and make sure we understand each of them.

We knew that dataset `test` has 11 attributes and `train` has 12 attributes. The one attribute short is the *Survived*. The rest are the same. Let us look into those attributes, the following is from the Kaggel web site:

```{r  out.width='100%', fig.cap= "Data Dectionary from Kaggle website.", fig.asp=.75, fig.align='center', echo=FALSE }

knitr::include_graphics("img/DatafrKaggle.png")

```

```
attribute Notes
*Pclass*: A proxy for socio-economic status (SES)
1st = Upper
2nd = Middle
3rd = Lower

*Age*: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5

*Sibsp*: The dataset defines family relations in this way...
*Sibling* = brother, sister, stepbrother, stepsister
*Spouse* = husband, wife (mistresses and fiancés were ignored)

*Parch*: The dataset defines family relations in this way...
Parent = mother, father
Child = daughter, son, stepdaughter, stepson
Some children travelled only with a nanny, therefore parch=0 for them.
```
Just looking into these attributes' description, a few thoughts are occurred:

1. Attribute types

There are attributes should be categorical types. The values of those attributes can be any types but the importance is that they can classify the records into sets of similar groups and this grouping make sense to the problem to be solved. In Titanic datasets, attributes should have **categorical** type are: *Survived*, *Sex*, *Embarked*, and *Pclass*.

Other attribute perhaps should have numerical type. Thsi is because these attributes values change from record  to record. They can be  the values of discrete, continuous, or timeseries. One thing in common is that these values can be manipulated and applied with many math functions and plotting tools for visualization. In Titanic datasets, attributes should have **numerical** type are: *Age*, *Fare*, *SibSp*, *Parch*. 

2. Contribution to Survive

The final goal is to predict passenger's survived or not. It makes sense to assess the prediction power of each attribute. which is the contribution of an attribute to attrribute *Survived*. In other words, the potential relationships among these attributes and with the attribute *Survived* need to be assessed. Here are some thoughts:

- *Pclass* should somehow linked with *Fare* and *Cabin*. Generally, the higher the class is and the more expensive of the fare will be and the better cabin locations are. So those should have some sort of correlations among them. they together should have some affect on survive. You would think that the expensive ticket, means better cabin location and has privilege to escape first in the disaster.  

- What is the ticket number to do with survive? Is it just a random number? Or is associated with cabin? Or anything else like Port of embarkation? ticket number in some other systems could have more information rather than just an unique number. 

- Is the *Fair* in someways associated with journey length, which means the Port of embarkation and the port of disembarkation? Or cabin location and condition? 

You can have other thoughts too. To prove or disprove these assumptions and thoughts, we need to look into the actual datasets at least to see:

1. What are the data types for various attributes?

  - Which attributes are available in the dataset?
  - Which attributes are categorical?
  - Which attributes are numerical?
  - Which attributes are mixed data types?

2. Any errors in the attributes values?

  - Which features may contain errors or typos?
  - Which features contain blank, null or empty values?

These questions will be answered in the following two sections. 


## Actual Attributes Types Examination

Since we have our raw data in RStudio, We can exam attributes' types. From figure \@ref(fig:Rstr), we can see that all the attributes have three types, `int`, `Factor`, `num`.

Attributes have `int` types are: *PassengerId*, *Survived*, *SibSp*, *Parch*.

Attributes has  `Factor` types are: *Name*, *Sex*, *Ticket*, *Cabin* and *Embarked*.

Attributes has `num` types are: *Age* and *Fare*. 

We know that, the type `int` is for attribute that has an integer value; and `num` is for an numeric attribute, which has the values of real numbers.

Type `Factor` is R language's way to say category type. It is a attribute that can take on one of a limited, and usually fixed, number of possible values, such as blood type.

Attributes types affect the operations we can apply on that attributes. In other words inappropriate types can prevent us to do proper analysis on that attribute. For example, it doe's not make sense to calculate average on sex, so it is better to be with a type of Category, in R is a `Factor`. Similarly, *Survived* will have only two values 0 or 1, to represent death or live. It makes sense to be an `Factor` too. Being a `int` type, it will prevent us to apply many methods that only works for a `Factor` type attribute. 

Another example is *Name*, its original type is *Factor* to reflect on its uniqueness. However, Type "Factor" is not good for string processing. It has been prevented that to apply regular expression^[A regular expression is a sequence of characters that define a search pattern, which is used by string-searching algorithms to find a particular string or validate a input string.] on it. So, it is appropriate to change it into *chr* as a character.

There are other inappropriate or wrong attribute's types too such as *SibSp* and *Parch* are currently typed *int*. May be they should be considered as *Factor*. It is a common practice that data scientists apply different analyses on a attribute and change the attribute type to apply other different algorithms again. The goal is to dig the insight out of data.   

So, looking into data attributes types, compare with the original meaning of each attributes can help us to spot any inappropriate types or wrong types. 


```{block2,  type='rmdthinking'}
Thinking:

Is **Servived typed** `int` approriate?
  
What other attributes do you think are in a wrong type?
  
```

## Actual Data Attributes Value Examination {#attvalue}

To understand given datasets needs to carefully examine the values of each data attributes to:

- find any errors and missing values
- find value distribution 
- find potential relation with the attribute to be predicted (also called dependent variable)

Finding errors, typos and missing values can set up the goals for data prepsocess. 

Since the examine covers both datesets `train` and `test`, it make sense to combine the two datasets into one big dataset, so it can save us to run the same code twice on the different datasets.

Copy the following code into your script,

```{r}
# Add a "Survived" attribute to the test dataset to allow for combining with train dataset

test <- data.frame(test[1], Survived = rep("NA", nrow(test)), test[ ,2:ncol(test)])

# Combine data sets. Append test.survived to train
data <- rbind(train, test)

```
Now we have a dataset `data`, which combines both datasets `train` and `test` datasets. We assigned the value of attribute *Survived* in the original dataset `test` as "NA". You can check them in the **WorkSpace pane** by click variable data.  

```{block2,  type='rmdtry'}
Thinking:

1. Can we combine `train` and `test` without add *Survived* attribute to the test? Like,

```data <- rbind(train, test)```

2. Why add attibute *Survived* as the second attribute? Can we add it as the first one? Like,

```test <- data.frame(Survived = rep("NA", nrow(test)), test[,])```

```

It is good idea to have bird eye's view on our combined dataset.

```{r sofdata, fig.align = 'center', out.width = "95%" }
# check out data with a summary
summary(data)
```

This summary \ref{fig:sofdata} tell us a lot of information. Most obvious are:

1. *PassengerID* is useless in terms of predicting survived or not. in addition, it is not much help that provide a statistical summary on it. 
2. *Survived* and *Pclass* numbers are useful and interesting.
3. *Name* is mostly unique, which comes a surprise that only 2 names are repeated twice.
4. *Gender* distribution among passenger is unbalanced that male overweight female.
5. *Age* is interesting that minimum age 0.17 is alarming and there is 263 missing values.
6. *SibSp* tells us the largest relatives travel together is 8.
7. *ParCh* tells us the largest family travel together is 9.
8. There are a number of *ticket* has the same number. The most repeat number is CA. 2343, which has 11 duplicates. 
9. Ticket *Fare* shows the minimum is 0, which is interesting that someone take a free ride. The maximum is over 512, which is far too expensive when the mean value is only about 33. 
10. *Cabin* has a large number of missing values (identified by "").
11. *Embarked* only has three values which is not a good sign for prediction. it also has 2 missing value.

You can see now one function can provide so much information. Quantitative summary is a great tool for a data scientist. 

Now, Let us exam each attribute, 

### PassengerID. 

*PassengerId* is an identifier, So only its uniqueness and missing value are considered.
 
There are many ways you can use to find out. I simply check its total number and its unique number. If the both equal to the number of records in the dataset, it shows that there is no duplication and no missing values in the attribute. 

So we do,
 
```{r}
# Exam PassengerID

length(data$PassengerId)
length(unique(data$PassengerId))

```
The results shows the both number 1309, which is equal to the total number of records in the dataset. It proves the *PassengerID* has not missing value and duplication. 

### Survived

*Survived* is the attribute that its value will be produced by a model for the dataset `test`. It is called **Consequencer** in modeling contrast with other attributes. which are used to produce a prediction, are called **Predictor**.  So, our exam will be conducted only on dataset `train`. Again we can check the numbers whether they can add up or not. As we already mentioned that it makes sense to change the *Servived* from type `chr` into `Factor`. We do,

```{r} 
# Exam Survived
data$Survived <- as.factor(data$Survived)
table(data$Survived)
```
The results proved that the *Survived* value has the correct numbers: 

- 418 'NA' values are the *Survived*'s value in the  dataset `test`, and
- the 549 death and 342 survived, together maded up the total number of dataset `train `, which is 891. 

So we know the value of *Survived* in the dataset `train` are correct and has no missing values. It is interesting here to think about the survive rate. How to calculate? 

I will do this,
```{r}
# Calculate the survive rate in train data is 38% and the death rate is 61.61%
prop.table(table(as.factor(train$Survived)))
```
So we know the survive rate in the dataset `train` is about 61%. This is interesting because it reflects the overall survival rate.

### Pclass

*Pclass* is the feature which splits the passengers into three division namely class-1, class-2, class-3. As we understood it should be in type of `Factor` rather than `int`. We shall change its type first and then to see if there missing value or errors. It is also good to know the survival rate in each class. So. we can compare with the overall survival rate in the dataset `train`.

Copy the following code into your script.
```{r}
# Examine Pclass value,
# Look into Kaggle's explanation about Pclass: it is a proxy for social class i.e. rich or poor
# It should be factor and it does not make sense to stay in int.
data$Pclass <- as.factor(data$Pclass)
test$Pclass <- as.factor(test$Pclass)
train$Pclass <- as.factor(train$Pclass)

# Distribution across classes
table(data$Pclass)

```
If you want, you can check the total of the three classes which is 1309. It equals to the total number of records in the `data` (total number fo passenger). And there is no other numbers than 1,2 and 3. So we can conclude that there is no missing value and no errors in *Pcalss*.

It will be interesting to see the survival rate for each class,
```{r}
# Distribution across classes with survive
table(data$Pclass, data$Survived)
```
These numbers tell us many things: 

1. **The death distribution**. Among the three classes from class-1 to class-3 is: 80, 97 and 379. It confirms that the passenger in Class-3 has largest number of death (372);

2. **The survival distribution**. Among the three classes, class-1 has the highest number of survival (136);

3. **The passengers distribution**. Among the three classes, class-3 has the largest passenger numbers (372+119+218) in total, and overtaking other two classes together for both datasets `train` and `test` (372+119) > ((80+97) + (136+87)). 

4. The last column is the pasenger distribution among the three glasses for dataset `test`. This is because its *Survived* value is "NA" (not defined).

We can calculate distributions among the three classes in terms of percentage. 

1. The overall passenger's distribution among the three classes:
```{r}
# Calculate the distribution on Pclass
# Overall passenger distribution on classes.
prop.table(table(as.factor(data$Pclass)))
```
That is 24.67% passenger in Class-1, 21.16% passenger is class-2 and 54.16% of passenger in class-3.

2. The passenger's distribution among the three classes given by dataset `train`:
```{r}
# Train data passenger distribution on classes.
prop.table(table(as.factor(train$Pclass)))
```
The number tells us the distribution of passengers from  dataset `train` is: class-1, 24.24%; class-2, 20.65% and class-3 has 55.1%.

3. The passenger's distribution among the three classes given by dataset `test`:

```{r}
# Test data passenger distribution on classes.
prop.table(table(as.factor(test$Pclass)))
```
Lastly, the passenger distribution from dataset `test` are: 25.6% in class-1, 22.24% in class-2 and 52.15% percent in class-3.

We can see that the distribution of passengers, in terms of percentage, among the three classes are almost identical both in order and in proportion. That is the most passenger are in class-3, then class-1 and finally class-2.

Let us look into death and survive distribution among the three classes^[This code is not brilliant. It used many intermediate variables, you can check their structure and contents from **WorkSpace pane**. You can come up with a better code.],
```{r}
# Calculate death distribution across classes with Train data
SurviveOverClass <- table(train$Pclass, train$Survived)
# Convert SurviveOverClass into data frame
SoC.data.fram <- data.frame(SurviveOverClass) 
# Retrieve death distribution in classes
Death.distribution.on.class <- SoC.data.fram$Freq[SoC.data.fram$Var2==0]
prop.table(Death.distribution.on.class)
```
These numbers tell us the distribution of death among the three classes are: 14.57% death from class-1, 17.66% from class-2 and 67.75% death from class-3. 

Similarly, we can calculate survive distribution among the three classes, 

```{r}
# calculate survive distribution among the three classes
Survive.distribution.on.class <- SoC.data.fram$Freq[SoC.data.fram$Var2==1]
prop.table(Survive.distribution.on.class)
```
The results tell us that 39.76% of survived passenger are from class-1, and 25.43% from class-2, and 34.79% from class-3.

Let us thinking about this numbers. Class-3 has 55.1% of passenger distribution but has 34.79% passenger survival distribution. Clearly, the survive rate in class-3 is lower than other two classes. It is equivalent to say, **the survival chances of a passenger who is in class-1 are higher than who is a class-2 and class-3**.

```{block2,  type='rmdaction'}
Do it yourself:
  
Calculate the Survival rate among the three classes. What conclusion you have by compare them?
```

Numbers are good to provide summary and test some assumptions. Analyzing given data by means of statistical summary and other numbering methods is called **Descriptive analysis**. See section \@ref(analyse) .

Perhaps, it is a good time to introduce **Exploratory analysis**, on the contrast with the Descriptive analysis, it uses graphical tools to explore the inside of given datasets.

To do so, we need to import some useful graphical tools provided by R community. We can then use them to plot *Survived* as an factor on *Pclass* numbers.

```{r Survivalrate, fig.cap = "Total count and survive rate of passenger on Pcalss."}

# Load up ggplot2 package to use for visualizations
library(ggplot2)

ggplot(train, aes(x = Pclass, fill = factor(Survived))) +
  geom_bar(width = 0.3) +
  xlab("Pclass") +
  ylab("Total Count") +
  labs(fill = "Survived")
```

Graph is better, isn't it? It is very intuative. 

Let's briefly interpret this graph. The graph shown \@ref(fig:Survivalrate) tells us that the survive rate in Class-3 is the worst, and followed by class-2 and lastly, class-1. More people perished in the class-3 than any other two classes. It provides an important point that the chance of survive is associated with the **social glass**, if we can prove the Class-3 ticket is cheaper. 

To sum up the analysis with Pclass, We have used both **Descriptive analysis** and **Exploratory analysis**. The results suggested that **the *Pclass* has a strong relation with death rate**. That is passengers in Class-3 have a higher chance of death. The correlation with social class (richer or poor) is waiting to be proved if the class-3 ticket is cheaper than others.  

### Name
 
*Name* attribute by definition shows peoples' name. It should not have any impact on passengers' live and death. Never heard of someone was survived because one's name! 
 However we still need to asess its quality. Fir
 
Firstly, you may notice that the type of *Name* is a `Factor`, which is contradicted with the conventional understanding that name is a string or a list characters. Type `chr` would be more appropriate. Change its type to `chr` will help us to apply character functions to it and get it contents easily. Factor shows the uniqueness. it could help us to assess if there is missing value or duplicated values. 

Notice that attribute *Name* only has 1307 levels (can be observed from the `data` structure on the **WorkSpace pane**). In addition, the `data` summary, see Figure \@ref(fig:summaryofdata), not only confirmed this but also identified the two shorts because of the value "Connolly, Miss. Kate" and "Kelly, Mr. James" have been repeated twice each. 

Let us explore *Name* values in details.
Firstly, let us convert *Name* type into `chr`. Then we can check duplicated names using `which` function in R to get the duplicate names and store them into a vector `dup.names`. And echo them out. 

```{r}
# Convert Name type
data$Name <- as.character(data$Name)
# Find the two duplicate names
# First used which function to get the duplicate names and store them as a vector dup.names 
# check it up ?which.
dup.names <- data[which(duplicated(data$Name)), "Name"]

# Echo out 
dup.names
```
Our code confirmed that the two duplicated names are indeed ""Kelly, Mr. James" and "Connolly, Miss. Kate". It comes no surprise that the both names are pretty common in UK and USA.

One discovery though is that the names appeared has a title in it! **Mr.** is used in Kelly James and **Miss.** is used in Connolly Kate. This could be interesting. We can leave this for **Preprocess** to explore more. For the quality assessment it is mission accomplished. 
 
### Sex

*Sex* attribute value assessment is simple. Its type `Factor` helps a lot. Since it only has two values "male" and "female", we could easily check if there are missing values and any errors.   


```{r}
# Use summary to check numbers and distribution
summary(data$Sex)
# If you prefer you can use data.table functions
```
It is obvious that there is no error and missing values. The result confirms this: male 843 and female 466, together we have 1309 passengers, which is the total numbers of the passenger.  

It is also simple to explore the relationship between gender and the survival rate. We had an assumption that the male passengers have a high death rate. We have plot tools in our disposal, let's make use of it. Since only dataset `train` has the values on *Survived*, it makes sense that we only plot relation between gender and survival on dataset `train`.

```{r Survivalrateonsex, fig.cap = "Total count and survive rate of passenger on sex."}
# plot Survived over Sex on dataset train
ggplot(data[1:891,], aes(x = Sex, fill = Survived)) +
  geom_bar(width = 0.3) +
  xlab("Sex") +
  ylab("Total Count") +
  labs(fill = "Survived")

```

The graph shows that the male death rate is much higher than female passengers. 

```{block2,  type='rmdthinking'}
Thinking:

We have used `data[1:891,]` in our `ggplot` code. Why we do not use dataset `train` instead? What are the differnce if there is any?
  
```

### Age

To examine values of attribute *Age*, we do this,

```{r}
# Examine Age over data, train and test.
summary(data$Age)
summary(train$Age)
summary(test$Age)
```
These summary tell us that the minimum, median, mean, maximum and missing values (as NA). They are useful. but they failed telling us on the value distribution. 

```{r eval=FALSE, include=FALSE}
#it makes sense to change age type to Factor to see distribution
summary(as.factor(data$Age))
```

```{r ats, fig.cap="*Age* summary as categorical data.", fig.align = 'center', out.width = "80%", echo=FALSE} 

knitr::include_graphics(here::here("img", "ageresult.png"))

```

From Figure \@ref(fig:ats), we can see a few problems:

1. Age values have decimal point which is a sort of surprise and not sure if it is a mistake.

2. There are large number of missing values: 177 missing value in dataset `train` and 86 missing value in dataset `test`, in total of 263, which count as 263/1309 = 20%. large number of missing values sets up a task for **data preprocess** to deal with. In the same time, it make you think whether it is a valid predictor or not.

We can assess its impact on survive rate. So we need to look into dataset `train`.

```{r age, fig.align = 'center', fig.show="hold", out.width = "50%", fig.cap = "*Age* ditribution and its Survive rate of passenger"}
# plot distribution of age group
ggplot(data, aes(x = Age)) +
  geom_histogram(binwidth = 10, fill="steelblue") +
  xlab("Age") +
  ylab("Total Count")

# plot Survived on age group using train dataset
ggplot(data[1:891,], aes(x = Age, fill = Survived)) +
  geom_histogram(binwidth = 10) +
  xlab("Age") +
  ylab("Total Count")

```

The graph shows the relationship between *Age* and survival rate. It becomes apparent that age group between 15 and 25 has the worst survival rate. It is also interesting to know that there are some age has values less than 0! 

With this, we could conclude that.

The attribute *Age* has a serious quality problem: some age values are negative and large number 177 values are missing. If it is to be used as a predictor in our model for prediction, it needs a lot of work in the stage of preprocess. 

### SibSp 

Attribute *SibSp* represents passenger's siblings and sprouts who travels with the passenger. We will a have pretty good idea about its values. This will help us to spot any errors and missing values.

We do this,
```{r}
### Exam SibSp, Its original type is int
summary((data$SibSp))

# How many possible unique values?
length(unique(data$SibSp))

#is there an y missing values? check the total number
length(data$SibSp)

# Treat it as a factor, so we know the value distribution
data$SibSp <- as.factor(data$SibSp)
summary(data$SibSp)
```
The results have provided us with good evidence for access its values. Firstly, we know the minimum value is 0, and there are 891 records have 0 values. It means that there are 891 passenger who travel without siblings and sprouts; secondly, apart from the value 0, the 3 quarters of the passenger who has 1 company; and lastly the maximum number of the company a passenger had is 8. There are 9 of them. There are totally 7 kinds of company in terms of the numbers of company a passenger can have.

It has not error or missing value since the total number are correct. 

We can assess its prediction power by looking into the relationship between *SibSp* and *Suvivied*,

```{r SibSp, fig.align = 'center', fig.show="hold", out.width = "50%", fig.cap = "Plot *SibSp* distribution among the 7 values and its survive rate."}
# plot entire SibSp distribution among the 7 values
ggplot(data, aes(x = SibSp)) +
  geom_bar(width = 0.5) +
  xlab("SibSp") +
  ylab("Total Count")+
  coord_cartesian()

# Plot on the survive on SibSp
ggplot(data[1:891,], aes(x = SibSp, fill = Survived)) +
  geom_bar(width = 0.5) +
  xlab("SibSp") +
  ylab("Total Count")  +
  labs(fill = "Survived")
```
We run two plots: the first one is the value distribution on entire dataset to have an impression on its distribution shape; and the second one is checking the survival rate over its distribution groups by dataset `train`. It seems that passenger who have two companies tend to have a better survival rate. This could be an interesting pattern to explore. 

```{block2,  type='rmdaction'}
Do it yourself:
  
Calculate the Survival rate among the 7 possibilities in terms of have siblings or sprouds treval with them. What conclusion you have by compare them?
```

We can conclude that the value of `SibSp` have a pretty good quality and there is no apparent error and missing values. Its predication power needs further investigation but it is informative.

### Parch 

Attribute *Parch*, similar with *SibSp*, is representing the travel company or groups. *Parch* specifically represents parents or children. I don't know why Kaggle separate them but it seems reasonable to think they together represent one thing that is "travel with family". 

To access its value, we will do the same as we did on `SibSp`.

```{r}
### Exam Parch, Its original type is int
summary((data$Parch))

# How many possible unique values?
length(unique(data$Parch))

#is there an y missing values? check the total number
length(data$Parch)

# Treat it as a factor, so we know the value distribution
data$Parch <- as.factor(data$Parch)
summary(data$Parch)
```
The discovery is similar again with *SibSp*, that is:
1. The minimum value is 0, and there are 1002 records have 0 values. It means that there are 1002 passenger who travel without without parents or children (we still cannot see the passenger travel alone, he or she could travel with a sibling or a sprout, However, this rise an idea to look into passenger who travel alone, which means no sibling, sprout, parents and children.); 
2. The maximum number is 9. There are 2 of them.
3. Apart from the value 0, the largest company number is 1. There are 170.
4. There are totally 8 possibilities in terms of the numbers of company a passenger can have.
5. It has not error or missing value since the total number are correct. 

We can assess its prediction power too by looking into the relationship between *Parch* and *Survived*,

```{r Parch, fig.align = 'center', fig.show="hold", out.width = "50%", fig.cap = "Plot *Parch* distribution among the 8 values and its survive rate."}
# plot entire Parch distribution among the 7 values
ggplot(data, aes(x = Parch)) +
  geom_bar(width = 0.5) +
  xlab("Parch") +
  ylab("Total Count")+
  coord_cartesian()

# Plot on the survive on Parch
ggplot(data[1:891,], aes(x = Parch, fill = Survived)) +
  geom_bar(width = 0.5) +
  xlab("Parch") +
  ylab("Total Count")  +
  labs(fill = "Survived")
```
The plot shows us that it is definitely have impact on survival. But it i snot clear the prediction power in comparison with *SibSp*. I am not sure there are difference between "travel with parents or children" and "travel with siblings and sprout". In addition, value 0 in each attributes does not excludes other attributes. Travel without parents or children does not mean travel without siblings or sprout. If we try to see the impact on survived in terms travel alone or with a company, we need to re-engineer these attributes. It is a good point anyway and give another task for **data preprocess ** to do.

### Ticket
Intuitively, as mentioned before, ticket number like passenger names, should not be considered as bounded with the survival of a passenger. Unless the ticket number has other hidden information such as class or location on the boat. Bearing this in mind, let us assess its value.
```{r }
head(summary(data$Ticket),50)
# Take a look at the ticket value
str(data$Ticket)
which(is.na(data$Ticket))
```
The value of *Ticket* appears has no missing value and there are 929 different numbers and some with letters and some with special characters like "." and "/". There is no immediately apparent structure in the data.
 Let us plot them and also see if there is any pattern with survival.
 
```{r  tecket, fig.align = 'center', fig.show="hold", out.width = "50%", fig.cap = "Plot of *Ticket* distribution and survival rate."}
#plot it value 
ggplot(data[1:891,], aes(x = Ticket)) +
  geom_bar() +
  xlab("Ticket") +
  ylab("Total Count")
# Plot on the survive on Ticket
ggplot(data[1:891,], aes(x = Ticket, fill = Survived)) +
  geom_bar() +
  xlab("Ticket Number") +
  ylab("Total Count")  +
  labs(fill = "Survived")
```
The same tickets number has such a small number. It does not have any statistical meaning. It is possible to reengineer *ticket* number into groups like "number only" vs "with letter" or "with special characters", or simply group them with the length of the ticket or with the initials, etc. There is a lot of thing you can do to see if there is any patterns connected with the survival. 

Over all, *Ticket* has a good quality and has no missing value and errors (we dont count repeated ticket number is an error). However, there is no obvious relations with the survive rate.

### Fare
The value of *Fare* are expected associated with "passenger's wealth". You would naturally associate its value with cabin condition and perhaps location of he cabin. Let us assess its value quality.

```{r}
summary(data$Fare)
length(unique(data$Fare))
```
The initial assessment tells us that:
1. The value of `Fare` has one missing value.
2. They are 282 different prices among 1308 tickets.
3. The minimum value is 0 (Free ride?) and the maximum value is 512.329. 
4. The mean value is 33.295 and the median is only 14.454. 
5. There are two potential issues in here: 512.329 is extremely higher than others, it could be considered as an outlier or an error; another potential issue is the precision. Any currency cannot have a physical money which carry value three digits after the decimal point. so any value has three digits after decimal point could be an error.  

Let us examine the prediction power of attribute *Fare*.

```{r  Fareplot, fig.align = 'center', fig.show="hold", out.width = "50%", fig.cap = "Plot of *Fare* distribution and survival rate."}

ggplot(data, aes(x = Fare)) +
  geom_histogram(binwidth = 5) +
  ggtitle("Fare Distribution") +
  xlab("Fare") +
  ylab("Total Count") +
  ylim(0,200)
# Let's check to see if fare has predictive power
ggplot(data[1:891,], aes(x = Fare, fill = Survived)) +
  geom_histogram(binwidth = 5) +
  xlab("Fare") +
  ylab("Total Count") +
  ylim(0,50) + 
  labs(fill = "Survived")
```
It is not clear about *Fare* prediction power. One thing is clear that to be useful for predcition, Fare needs more engineer such as group it in different groups like <5, 5 to 10, 10 to 15, ..., etc.

### Cabin

*Cabin* has a large number of missing values as we noticed from the beginning of this section \@ref(attvalue). So its quality is expected to be bed. let us find out how many missing values is the dataset `train`, so we can assess its predictive power over survive. 

Firstly, by looking into the structure of the dataset, 

```{r}
# Examine cabin values
str(data$Cabin)
# Cabin really isn't a factor, make a string and the display first 100
data$Cabin <- as.character(data$Cabin)
data$Cabin[1:100]
```

we find out that *Cabin* is in a type of `Factor` and has 187 unique values with empty string "" and string start with letter like "A10".

By looking actual 100 values, we have a pretty good understand its contents. Notice that some string looks like multiple numbers, for instance "C23 C25 C27", it is odd in comparison with others. 

let us have a close look at the dataset `train` and assess its prediction power.
```{r}
# Find out number of the missing value in the train
train$Cabin <- as.character(train$Cabin)
# number of the missing value in the train
table(train[which(train$Cabin ==""), "Cabin"])
# percentage of the missing value in the train
table(train[which(train$Cabin ==""), "Cabin"])/length(train$Cabin)*100
```
The above code tells us that in the dataset `train`, there are 687 missing value and it count as 71 percent of total value. This is significant number. Generally it will write off the attribute for any meaning for use. However, like its relation with survive with the consideration of the missing value.

Since the small number of passenger in each cabin, we accumulate passenger with the first latter of the cabin number. That means we bin the passengers based on the first letter of their cabin number. Then we plot the survived number over the total number.

```{r Cabin, fig.align = 'center',  out.width = "90%", fig.cap = "Plot of the passenger number and  the survived number based on the first letter of their cabin number."}
# Take a look at just the first char as a factor and add to data as a new attribute
data$cabin.first.char<- as.factor(substr(data$Cabin, 1, 1))

# first cabin letter survival plot
ggplot(data[1:891,], aes(x = cabin.first.char, fill = Survived)) +
  geom_bar() +
  xlab("First Cabin Letter") +
  ylab("Total Count") +
  ylim(0,750) +
  labs(fill = "Survived")
```
To sumup, *Cabin* attribute has large number of missing value. The dataset `train` has 687 missing value and it counts as 71 percent of total value. Its prediction power is in serious doubt since it only has very small number for each cabin. To use it in any possible predictio model, it needs some re-engineering. 

### Embarkded

Attribute *Embarked* records where a passenger get on board. From the Kaggle data description we know that there are three possible values for Embark — Southampton (S), Cherbourg (C), and Queenstown (Q). Let's check the data quality.
```{r}
# Examine Embark values
summary(data$Embarked)
length(data$Embarked)
```
The results confirms that there two missing values and three ports. Southampton as its initial depart port has largest passengers get on board. Let's see its distribution and the survival rate.
```{r  Fare, fig.align = 'center', fig.show="hold", out.width = "50%", fig.cap = "Plot of *Embarked* distribution and survival rate."}
# Plot data distribution and the survival rate for analysis
ggplot(data, aes(x = Embarked)) +
  geom_bar(width=0.5) +
  xlab("Passenger embarked port") +
  ylab("Total Count") 


ggplot(data[1:891,], aes(x = Embarked, fill = Survived)) +
  geom_bar(width=0.5) +
   xlab("Embarked port") +
  ylab("Total Count") +
  labs(fill = "Survived")
```

The graph shows that about 70% of the people boarded from Southampton (914/1309 = 0.698). Just over 20% boarded from Cherbourg (270/1309 = 0.206) and the rest boarded from Queenstown about 10%.

```{r  embark, fig.align = 'center', fig.show="hold", out.width = "30%", fig.cap = "Plots of death distribution, survive distribution and death rate comparision over embarked port."}

# Calculate death distribution over Embarked port with Train data
# creat Embarked and Survived contingency table
SurviveOverEmbarkedTable <- table(train$Embarked, train$Survived)
# Death-0/survived-1 value distribution (percentage) based on embarked ports
# prop.table(mytable, 2) give us column (Survived) percentages
Deathandsurvivepercentage <- prop.table(SurviveOverEmbarkedTable, 2)
# Plot
M <- c("c-Cherbourg", "Q-Queenstown", "S-Southampton")
barplot(Deathandsurvivepercentage[2:4,1]*100, xlab =(""), ylim=c(0,100), ylab="Death distribution in percentage %",  names.arg = M, col="steelblue", main="Death distribution", border="black", beside=TRUE)
barplot(Deathandsurvivepercentage[2:4,2]*100, xlab =(""), ylim=c(0,100), ylab="Death distribution in percentage %",  names.arg = M, col="blue", main="Death distribution", border="black", beside=TRUE)

## Calculate survived RATE distribution based on embarked ports
# Death-0/survived-1 value distribution (percentage) based on embarked ports
# prop.table(mytable, 1) give us row (Port) percentages
# col-1 (Survived=0, perished) and col-2 (Survived =1, survived)
DeathandsurviveRateforeachport <- prop.table(SurviveOverEmbarkedTable, 1)
#plot
barplot(Deathandsurvivepercentage[2:4,1]*100, xlab =(""), ylim=c(0,100), ylab="Death rate in percentage %",  names.arg = M, col="red", main="Death rate comparison among mebarked ports", border="black", beside=TRUE)
```

The plot shows that both death and survive number distribution are similar. Southampton takes most death and survive portion because it has the largest number of passenger get on board, then Cherbourg, and last is Queenstown. However, in terms of death rate, which is the death/total from the passenger who get on board, southampton is the is the highest and then Queenstown, teh last is Cherbourg. That is to say, people who boarded from Cherbourg had a higher chance of survival than people who boarded from Southampton or Queenstown.

In summary, we have explored all attributes through descriptive analysis, which is mainly using numbers and through exploratory analysis, which is using plot. We have examined the quality of each attributes by finding missing values and duplication. We have spoted some outliars and odd values. 

We have also assessed relationship between attribute *Survived* and all other attributes. The prediction power of each attributes have been understand to some extend. More prediction power study such as combination of two or three attributes are needed. 

The findings of each attributes provide tasks and goals for data preprocess step to accomplish. 

## Data Recods Level Assessment

Although we have examined the raw datasets records' numbers in attributes level. We have a good knowledge about the record numbers in each given dataset. It is still necessary to check at the record level. It means if there some records have too many missing attributes' value, for example, although some records have ids and may be names but most of the useful attributes' value are missing.  These records are bed or invalid records, should be removed or solve the missing values. 

On other hand some records have most attributes values are identical. These could be considered as duplicates. depends on the problem to be solved, they could be problematic and need to be dealt with. 

In our Titanic problem, record level assessment is not an issue. Since we have almost all the records are different. This does not mean we should completely ignore this step and doing the checking.   

## Summary {-}

All the analyses actions provide demonstrations how to  access the raw data and understand their quantity and data quality. Notice that the understanding data is never a single one-off action. You never fully understood the given data. once the analytical process moving on, you may need to come back to apply some new decomposition on some attributes to explore more. 

Since our raw data is not too big in terms of both the number of records and the number of attributes. So it is relatively easy to assess their quality. In a real world project the raw data can be huge or can be too little. To perform an effective analysis you may need to reduce the data size or in other cases to increase the size. It means you need to do sampling on the given datasets and probably attributes selection too. Other cases you may need to create new attributes or combine a few attribute together. These are called attributes re-engineering. They are the part of important tasks in data preprocess , which is covered in the next chapter on **data preprocess**.  

```{block2,  type='rmdinfo'}
The entire R code in this chapter is avalable in the file "TitanicDataAnalysis_UnderstandData.R" and it can be find in the appendix. 
```

## Exercises 4 {-}

1. Identify the code in this tutorial which can be conceptually categorized as Descriptive analysis and which one can be Exploratory analysis?

2. Find out what is "rt" mean in R `train <- read.csv("train.csv", header = TRUE)` error message. Explore how to load files other than *csv* file.

3. Explore load data through RStudio build-in functions. Check "File -> Import Dataset", also check how to load data from a databases like MySql.

4. Calculate survival rate among the three Pclass.

5. Calculate the percentage of survival among different SibSp` and Parch` groups.

6. Plot distributions of Fare, Embarked of passengers who survived or did not survive.

7. Plot survival rate by Sex, Plot survival rate by Pclass, Plot survival rate by SibSp,Plot survival rate by Parch.

8. Plot survival rate (percentage of survived over total number) by over Embarked ports.




<!--chapter:end:04-understand-data.Rmd-->

# Data PreProcess

https://towardsdatascience.com/predicting-the-survival-of-titanic-passengers-30870ccc7e8

## Dealt with Miss values and Errors (Age, )

## Attributes selection (prediction power) 
explore relations for survivle

## Attribute reengineering ( title from name,  treval famail _ relatives, alone  from ParCh and SibSb)

## Assemble final datasets for modelling
need model, train and test.
 the goal is to get data ready for analysis
 
 what analysis?  
 
 prediction
 
 
 
1 dealt with error missing value
2. make attribute suiatble for modeling
3. features reenginering

name: extract title from name, 

Create family size and category for family size
https://www.kaggle.com/helgejo/an-interactive-data-science-tutorial

Extract ticket class from ticket number¶



The purposes of data preprocess is to make data suitable for analyzing. 

I this particular project, the purpose is to predict passengers survival. whatever a prediction model we may come up with, it should reflect the relations between other data attributes with the special one, which is "survived". So the data preprocess, whatever actions we are take, should focused on the attributes that has relations with the survive, or our preprocess should help to enhance the attribute's prediction power. An example is, "PassengerId", it has no relation with the survive, apart from to identify a passenger, its prediction power is 0. so there should be any efforts on this attributes apart from make sure its unique. 

Therefor it make sense to explore all attributes with surviels.



1. Survived
The first attribute reported if a traveler lived or died. A comparison revealed that more than 61% of the passengers had died.

code

```
table(as.factor(train$Survived))
prop.table(table(as.factor(train$Survived)))

```


 完成数据的基本探索后，在建立模型之前，我们还需要对数据进行清洗，并且对数据集中缺失的数据进行补全。

首先了解数据的缺失情况：

To begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview of variable datatypes (i.e. qualitative vs quantitative). Click here for the Source Data Dictionary.

The Survived variable is our outcome or dependent variable. It is a binary nominal datatype of 1 for survived and 0 for did not survive. All other variables are potential predictor or independent variables. It's important to note, more predictor variables do not make a better model, but the right variables.
The PassengerID and Ticket variables are assumed to be random unique identifiers, that have no impact on the outcome variable. Thus, they will be excluded from analysis.
The Pclass variable is an ordinal datatype for the ticket class, a proxy for socio-economic status (SES), representing 1 = upper class, 2 = middle class, and 3 = lower class.
The Name variable is a nominal datatype. It could be used in feature engineering to derive the gender from title, family size from surname, and SES from titles like doctor or master. Since these variables already exist, we'll make use of it to see if title, like master, makes a difference.
The Sex and Embarked variables are a nominal datatype. They will be converted to dummy variables for mathematical calculations.
The Age and Fare variable are continuous quantitative datatypes.
The SibSp represents number of related siblings/spouse aboard and Parch represents number of related parents/children aboard. Both are discrete quantitative datatypes. This can be used for feature engineering to create a family size and is alone variable.
The Cabin variable is a nominal datatype that can be used in feature engineering for approximate position on ship when the incident occurred and SES from deck levels. However, since there are many null values, it does not add value and thus is excluded from analysis.

<!--chapter:end:05-data-preprocess.Rmd-->

`r if (knitr:::is_html_output()) '
# Data Analysis 
'`

## models:Prediction

This was a case of classification problem and I tried predicting with two algorithms —
decision trees
Random Forest
Gaussian Naive Bayes
I was surprised at the results. The Gaussian Naive algorithm performed poorly and the Random Forest on the other hand was consistently predicting with an accuracy of more than 80%.

## Evaluation 

https://www.rpubs.com/rezapci/Data_Science_Machine_Learning_HarvardX

models and valiadition

<!--chapter:end:06-data-analysis.Rmd-->

`r if (knitr:::is_html_output()) '
# Result Interpretation
'`

<!--chapter:end:07-interpretation.Rmd-->

`r if (knitr:::is_html_output()) '
# Data Report
'`

<!--chapter:end:08-Data-Reprot.Rmd-->

---
title: "09-refer"
author: "Gangmin Li"
date: "9/26/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Apendix {-}
## TitanicDataAnalysis_UnderstandData.R {#UnderstandDatacode}

```
############################################################################
# Copyright 2020 Gangmin Li
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#  	http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# The base of this code was from Dave Langer "Intoduction to Data Science."
# Credit to him. You can find him on Github: https://github.com/EasyD
# I have also integrated other sources like Titanic Forum, Python code
# and some Chinese R code.
#
# Notably,
# https://www.kaggle.com/startupsci/titanic-data-science-solutions
#
# The whole purpose of this is to teach My students on Data Science.
#
# This R source code file corresponds to video 1 of the YouTube series
# "Introduction to Data Science with R" located at the following URL:
#     http://www.youtube.com/watch?v=32o0DnuRjfg
#
# The task is to build a model based on the train data
# then to predict test data who can survive in Kaggle Titanic competition
##########################################################################
# Tutorial One - Understand Data
#
# Understand data involves three steps:
# 1. Load data into memory - RStudio
# 2. Assess Data quantity
# 3. Assess data quality - set goals for next step in data preprocess
##########################################################################

### Load raw data

train <- read.csv("train.csv", header = TRUE)
test <- read.csv("test.csv", header = TRUE)

# RStudio help function
# what help with any R function just type ?
# for example ?read.csv in Console

# First explore of datasets
# the first R code you can use is str.
# use ?str to find more

help(str)

# use str to explore train and test

str(train)
str(test)

# Add a "Survived" variable to the test set to allow for combining data sets
test <- data.frame(Survived = rep("NA", nrow(test)), test[,])

# Now check test. you can see that it has 12 variable now.
# And Survived is the first variable. because we used test[,]
# if you want add Survived into the second position Do this,
# test <- data.frame(test[1], Survived = rep("NA", nrow(test)), test[,2:ncol(test)])

# Combine datasets together. actually append test to train
data <- rbind(train, test)

#### We have done combined datasest with only a few line of code
# notice that the type of Survived has been changed to Chr.
# This is because we used "NA" as its value
# A bit about R data types
# ?str structure of dataset
# chr, int
# Factor in R is 'category'. it likes a selection from a list.
# for example, Cabin Factor w/187 levels. It means there are 187 selections.
# Sex Factor w/ 2 "female", "male", 2,1 means, two options 2- female, 1- male.
#
# NA : not available (absent value, missing value)

str(data)

# Exam PassengerID, type INT, we can check total number and the number of unique values.
# If they are equal and both equal to the number of records. it means there are
# unique and has no missing value.
length(data$PassengerId)
length(unique(data$PassengerId))

### Exam Survived
data$Survived <- as.factor(data$Survived)
table(data$Survived)

# Calculate the survive rate in train data is 38% and the death rate is 61.61%
prop.table(table(as.factor(train$Survived)))

### Examine Pclass value,
# Look into Kaggle's explanation about Pclass: it is a proxy for social class i.e. rich or poor
# It should be factor and it does not make sense to stay in int.
data$Pclass <- as.factor(data$Pclass)
test$Pclass <- as.factor(test$Pclass)
train$Pclass <- as.factor(train$Pclass)

# Distribution across classes
table(data$Pclass)

# Distribution across classes with survive
table(data$Pclass, data$Survived)

# Calculate the distribution on Pclass
# Overall passenger distribution on classes.
prop.table(table(data$Pclass))

# Train data passenger distribution on classes.
prop.table(table(train$Pclass))

# Test data passenger distribution on classes.
prop.table(table(test$Pclass))

# Calculate death distribution across classes with Train data
SurviveOverClass <- table(train$Pclass, train$Survived)
# Convert SurviveOverClass into data frame
SoC.data.fram <- data.frame(SurviveOverClass)
# Retrieve death distribution in classes
Death.distribution.on.class <- SoC.data.fram$Freq[SoC.data.fram$Var2==0]
prop.table(Death.distribution.on.class)

# Calculate death rate in train data
# Distribution across classes with survive in Train data
SurviveOverClass <- table(train$Pclass, train$Survived)
# Convert SurviveOverClass into data frame
SoC.data.fram <- data.frame(SurviveOverClass)
# Retrieve death distribution in classes
Death.distribution.on.class <- SoC.data.fram$Freq[SoC.data.fram$Var2==0]
prop.table(Death.distribution.on.class)

# Try summary on data
summary.data.frame(data)

## Exploratory data analysis with graph
# Load up ggplot2 package to use for visualizations
# load it into memory
library(ggplot2)

# High class passenger has more chance of survive than passenger with lower class
# Hypothesis - Rich passengers can but expensive ticket. class=1 is more expensive
# survived at a higher rate
ggplot(train, aes(x = Pclass, fill = factor(Survived))) +
  geom_bar(width = 0.5) +
  xlab("Pclass") +
  ylab("Total Count") +
  labs(fill = "Survived")

# It sort of proved the survive rate with social class
# more people perished in the third class

### Exam Name attribute
# the original name typed as factor, which we really don't want (shows the uniqueness)
# convert Name type
data$Name <- as.character(data$Name)

# Confirm the name has 1037 unique values
length(unique(data$Name))

# Find the two duplicate names
# First used which function to get the duplicate names and store them as a vector dup.names
# check it up ?which.
dup.names <- data[which(duplicated(data$Name)), "Name"]

# Echo out
dup.names

### Exam Sex attribute
# Retrial male and females. then check their numbers.
summary(data$Sex)

#Plot Sex distribution on entire dataset and get general an impression
ggplot(data[1:891,], aes(x = Sex)) +
  geom_bar(fill="steelblue") +
  xlab("Sex") +
  ylab("Total Count")

# plot Survived over Sex on train. use data[1:891,]
ggplot(data[1:891,], aes(x = Sex, fill = Survived)) +
  geom_bar() +
  xlab("Sex") +
  ylab("Total Count") +
  labs(fill = "Survived")

### Examine Age
# Summary over data, train and test.
summary(data$Age)
summary(train$Age)
summary(test$Age)

#It makes sense to change Age type to Factor to see distribution
summary(as.factor(data$Age))

# Plot distribution of age group
ggplot(data, aes(x = Age)) +
  geom_histogram(binwidth = 10, fill="steelblue") +
  xlab("Age") +
  ylab("Total Count")

# Plot Survived on age group on train dataset
ggplot(data[1:891,], aes(x = Age, fill = Survived)) +
  geom_histogram(binwidth = 10) +
  xlab("Age") +
  ylab("Total Count")

### Exam SibSp, Its original type is int
summary((data$SibSp))

# How many possible unique values?
length(unique(data$SibSp))

# Treat it as a factor, so we know the value distribution
data$SibSp <- as.factor(data$SibSp)
summary(data$SibSp)

# Plot entire SibSp distribution among the 7 values
ggplot(data, aes(x = SibSp)) +
  geom_bar() +
  xlab("SibSp") +
  ylab("Total Count")+
  coord_cartesian()

# Plot on the Survived on SibSp
ggplot(data[1:891,], aes(x = SibSp, fill = Survived)) +
  geom_bar() +
  xlab("SibSp") +
  ylab("Total Count")  +
  labs(fill = "Survived")

### Exam Parch
summary(data$Parch)

# How many possible values?
length(unique(data$Parch))

# Treat is as a factor so we know the value distribution
data$Parch <- as.factor(data$Parch)
summary(data$Parch)

# Plot entire Parch distribution among the 8 posibilites
ggplot(data, aes(x = Parch)) +
  geom_bar() +
  xlab("Parch") +
  ylab("Total Count")

# Plot on the Survived on SibSp
ggplot(data[1:891,], aes(x = Parch, fill = Survived)) +
  geom_bar() +
  xlab("Parch") +
  ylab("Total Count")  +
  labs(fill = "Survived")

### Exam  ticket
summary(data$Ticket)
length(unique(data$Ticket))
str(data$Ticket)
which(is.na(data$Ticket))

# Plot it value
ggplot(data[1:891,], aes(x = Ticket)) +
  geom_bar() +
  xlab("Ticket") +
  ylab("Total Count")

# Plot on the survive on Ticket
ggplot(data[1:891,], aes(x = Ticket, fill = Survived)) +
  geom_bar() +
  xlab("Ticket") +
  ylab("Total Count")  +
  labs(fill = "Survived")

### Examine Fare
summary(data$Fare)
length(unique(data$Fare))

# Can't make fare a factor, treat as numeric & visualize with histogram
ggplot(data, aes(x = Fare)) +
  geom_histogram(binwidth = 5) +
  ggtitle("Fare Distribution") +
  xlab("Fare") +
  ylab("Total Count") +
  ylim(0,200)

# Let's check to see if fare has predictive power
ggplot(data[1:891,], aes(x = Fare, fill = Survived)) +
  geom_histogram(binwidth = 5) +
  xlab("fare") +
  ylab("Total Count") +
  ylim(0,50) +
  labs(fill = "Survived")

# Explore Fare distribution between train and test to see if they are overlapped?
ggplot(train, aes(x = Fare)) +
  geom_histogram(binwidth = 5) +
  ggtitle("Fare Distribution") +
  xlab("Fare") +
  ylab("Total Count") +
  ylim(0,200)

ggplot(test, aes(x = Fare)) +
  geom_histogram(binwidth = 5) +
  ggtitle("Fare Distribution") +
  xlab("Fare") +
  ylab("Total Count") +
  ylim(0,200)

### Cabin
# Examine cabin values
str(data$Cabin)
# Cabin really isn't a factor, make a string and the display first 100
data$Cabin <- as.character(data$Cabin)
data$Cabin[1:100]

# Find number of the missing value
table(train[which(train$Cabin ==""), "Cabin"])

# Analysis of the cabin variable
str(data$Cabin)

# Cabin really isn't a factor, make a string and the display first 100
data$Cabin <- as.character(data$Cabin)
data$Cabin[1:100]

# Find out number of the missing value in the train
train$Cabin <- as.character(train$Cabin)
table(train[which(train$Cabin ==""), "Cabin"])

# Replace empty cabins with a "U"
#data[which(data$Cabin == ""), "Cabin"] <- "U"
data$Cabin[1:100]

# Take a look at just the first char as a factor
cabin.first.char <- as.factor(substr(data$Cabin, 1, 1))
str(cabin.first.char)
levels(cabin.first.char)

# Add to combined data set and plot
data$cabin.first.char <- cabin.first.char

# High level plot
ggplot(data[1:891,], aes(x = cabin.first.char, fill = Survived)) +
  geom_bar() +
  ggtitle("Survivability by cabin.first.char") +
  xlab("cabin.first.char") +
  ylab("Total Count") +
  ylim(0,750) +
  labs(fill = "Survived")

### Examine Embark
str(data$Embarked)
summary(data$Embarked)

# Plot Embarked data distribution and the Survived data over it
ggplot(data, aes(x = Embarked)) +
  geom_bar(width=0.5) +
  xlab("Passenger embarked port") +
  ylab("Total Count")

ggplot(data[1:891,], aes(x = Embarked, fill = Survived)) +
  geom_bar(width=0.5) +
   xlab("embarked") +
  ylab("Total Count") +
  labs(fill = "Survived")

##Calculate death RATE distribution over Embarked port with Train data
# We use table in R, you can check with ?table. A good example is
# mytable <- table(A,B) # A will be rows, B will be columns
# mytable # print table

# margin.table(mytable, 1) # A frequencies (summed over B)
# margin.table(mytable, 2) # B frequencies (summed over A)

# prop.table(mytable) # cell percentages
# prop.table(mytable, 1) # row percentages
# prop.table(mytable, 2) # column percentages

# We need prop.table to get column percentage which is the survived

# creat Embarked and Survived contingency table
SurviveOverEmbarkedTable <- table(train$Embarked, train$Survived)
# Death-0/survived-1 value distribution (percentage) based on embarked ports
# prop.table(mytable, 2) give us column (Survived) percentages
Deathandsurvivepercentage <- prop.table(SurviveOverEmbarkedTable, 2)
# Plot
M <- c("c-Cherbourg", "Q-Queenstown", "S-Southampton")
barplot(Deathandsurvivepercentage[2:4,1]*100, xlab =(""), ylim=c(0,100), ylab="Death distribution in percentage %",  names.arg = M, col="steelblue", main="Death distribution", border="black", beside=TRUE)
barplot(Deathandsurvivepercentage[2:4,2]*100, xlab =(""), ylim=c(0,100), ylab="Death distribution in percentage %",  names.arg = M, col="blue", main="Death distribution", border="black", beside=TRUE)

## Calculate survived RATE distribution based on embarked ports
# Death-0/survived-1 value distribution (percentage) based on embarked ports
# prop.table(mytable, 1) give us row (Port) percentages
# col-1 (Survived=0, perished) and col-2 (Survived =1, survived)
DeathandsurviveRateforeachport <- prop.table(SurviveOverEmbarkedTable, 1)
#plot
barplot(Deathandsurvivepercentage[2:4,1]*100, xlab =(""), ylim=c(0,100), ylab="Death rate in percentage %",  names.arg = M, col="red", main="Death rate comparison among mebarked ports", border="black", beside=TRUE)

#End ###########################################################

```


<!--chapter:end:09-refer.Rmd-->

