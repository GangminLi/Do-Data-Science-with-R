# Titiannic Prediction with Random Forest

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,warning = FALSE)
```
```{r prediction1， out.width = "80%", fig.align ="center", echo =FALSE}
knitr::include_graphics(here::here("images", "forest-tree.jpg"))

```

>
> Can't see forest for the trees.
>
>                                     -- English Proverb
>

As we can see from the previous chapter, the decision tree models do not perform well in our prediction. In this chapter, we will try different models to see if we can improve the model's accuracy by using **Random Forest** model\index{Random Forest}. 

Random Forest is one of the powerful ensembling machine learning algorithms which works by creating multiple decision trees and combining the output generated by each of the decision trees through a voting mechanism to produce an output based on the majority of decision trees' votes [@Trevor2013]. Figure \@ref(fig:forest) is an example of a random forest.

```{r forest,  out.width = "80%", fig.align ="center", echo =FALSE, fig.cap="Example of the Random Forest."}
knitr::include_graphics(here::here("images", "Random_forest_diagram_complete.png"))
```
In the random forest, its decision tree classifier does not select all the data samples and attributes in each tree. Instead, every individual tree randomly selects data samples and attributes and combines the output at the end. The forest then removes the bias that a single decision tree might introduce in the model. The combination of the individual tree's result is done in a vote carried out to find the result with the highest frequency. A test dataset is evaluated based on these outputs to get the final predicted results. Because of the evaluation process, the random forest model will have an estimated prediction accuracy once constructed. This models' estimated accuracy can be used to compare between random forest models [@Aleksandra2021].  

## Steps to Build a Random Forest

1. Randomly select $k$ attributes from total $m$ attributes where $k < m$, the default value of $k$ is generally $\sqrt{m}$.
2. Among the $k$ attributes, calculate the node $d$ using the **best split point**
3. Split the node into a number of nodes using the **best split method**. See Section \@ref(best_split), by default R random Forest, uses Gini impurity values
4. Repeat the previous steps build an individual decision tree
5. Build a forest by repeating all steps for $n$ number times to create $n$ number of trees

After the random forest trees and classifiers are created, predictions can be made using the following steps:

1. Run the test data through the rules of each decision tree to predict the outcome and then 
2. Store that predicted target outcome
3. Calculate the votes for each of the predicted targets
4. Output the most highly voted predicted target as the final prediction 

Similar to the decision tree model, the random forest also has many implementations already built. You do not need to write code to do the actual model construction. In R, you can use a package called  'randomForest'. There are a number of terminologies that are used in random forest algorithms that need to be understood, such as:

1. **Variance**. When there is a change in the training data algorithm, this is the measure of that change. The most commonly used parameters to reflect changes are *ntree* and *mtry*. 

2. **Bagging**. This is a variance-reducing method that trains the model based on random sub-samples of training data. 

3. **Out-of-bag (OOB)** error estimate - The random forest classifier is trained using bootstrap aggregation, where each new tree is fit from a bootstrap sample of the training dataset. The out-of-bag (OOB) error is the average error for each calculation using predictions from the trees that do not contain their respective bootstrap sample. This enables the random forest classifier to be adjusted and validated during training. 

Let’s now look at how we can implement the random forest algorithm for our Titanic prediction. 
R provides `'randomForest'` package. You can check the details of the package for full usage. We will start with a direct function call with its default settings and we may change settings later. We will also use the original attributes first and then use re-engineered attributes to see if we can improve on the model.

## Random Forest with Key Predictors
```{r echo = FALSE, warning=FALSE, message=FALSE}
# Install the random forest library, if you have not
# install.packages('randomForest')
# load library

library(randomForest)
library(plyr)
library(caret)
# load data if you have not
RE_data <- read.csv("./data/RE_data.csv", header = TRUE)

# Change Survive type
RE_data$Survived <- factor(RE_data$Survived)
RE_data$Pclass <- as.factor(RE_data$Pclass)
# split tran and test                     
train <- RE_data[1:891, ]
test <- RE_data[892:1309, ]
```

The process of using `randomForest` package to build an RF model is the same as the decision tree package `rpart`. Note also if a dependent (response) variable is a factor, classification is assumed, otherwise, regression is assumed. So to uses `randomForest`, we need to convert the dependent variable into a factor. 

```{r FR_prep}
# convert variables into factor
# convert other attributes which really are categorical data but in form of numbers
train$Group_size <- as.factor(train$Group_size)
#confirm types
sapply(train, class)
```
Let us use the same five most related attributes: *Pclass*, *Sex*, *HasCabinNum*, *Deck* and *Fare_pp* in the decision tree model2. We use all default parameters of the `randomForest`.

```{r RF_model1_cons, cache=TRUE}
# Build the random forest model uses pclass, sex, HasCabinNum, Deck and Fare_pp
set.seed(1234) #for reproduction 
 RF_model1 <- randomForest(as.factor(Survived) ~ Sex + Pclass + HasCabinNum + Deck + Fare_pp, data=train, importance=TRUE)
 save(RF_model1, file = "./data/RF_model1.rda")
```

Let us check model's prediction accuracy.
```{r RF_model1_show, fig.cap = "The detials of RF_model1"}
load("./data/RF_model1.rda")
RF_model1
```
We can see that the model uses default parameters: `ntree = 500` and `mtry = 1`. The model's estimated accuracy is **80.7%**. It is 1 - 19.3% (`OOB error`).

Let us make a prediction on the training dataset and check the accuracy. 

```{r}
# Make your prediction using the validate dataset
RF_prediction1 <- predict(RF_model1, train)
#check up
conMat<- confusionMatrix(RF_prediction1, train$Survived)
conMat$table
# Misclassification error
paste('Accuracy =', round(conMat$overall["Accuracy"],2))
paste('Error =', round(mean(train$Survived != RF_prediction1), 2))
```
We can see that prediction on the training dataset has achieved **84%** accuracy. 
It has made 107 wrong predictions and 516 correct predictions on death. The prediction on survived is 33 wrong predictions out of 235 correct predictions. 

The model has an accuracy of 80% after learning, but our evaluation of the training dataset achieves 84%. It has been increased. Compare with the decision tree model2, in which the same attributes were used and the prediction accuracy on the train data was 81%, the accuracy is also increased. Let us make a prediction on the test dataset and submit it to Kaggle to obtain an accuracy score. 

```{r}
# produce a submit with Kaggle required format that is only two attributes: PassengerId and Survived
test$Pclass <- as.factor(test$Pclass)
test$Group_size <- as.factor(test$Group_size)

#make prediction
RF_prediction <- predict(RF_model1, test)
submit <- data.frame(PassengerId = test$PassengerId, Survived = RF_prediction)
# Write it into a file "RF_Result.CSV"
write.csv(submit, file = "./data/RF_Result1.CSV", row.names = FALSE)
```
We can see our random forest model has scored **0.76555** by the Kaggle competition. It is interesting to know that the random forest model has not improved on the test dataset compare with the decision tree model with the same predictors. The accuracy was also 0.76555.

Let us record these accuracies, 
```{r RE_model1_recd}
# Record the results
RF_model1_accuracy <- c(80, 84, 76.555)
```

## Random Forest with More Variables

Now let us see if We can build a better model if we use more predictors. The predictor we are using is identical to the decision tree model3. 

```{r RF_model2_Construction}
### RE_model2 with more predictors
set.seed(2222)
 RF_model2 <- randomForest(as.factor(Survived) ~ Sex + Fare_pp + Pclass + Title + Age_group + Group_size + Ticket_class  + Embarked, 
                          data = train,
                          importance=TRUE)
# # This model will be used in later chapters, so save it in a file and it can be loaded later.
 save(RF_model2, file = "./data/RF_model2.rda")
```
We can assess the new model,

```{r RF_model2_show}
load("./data/RF_model2.rda")
RF_model2
```

Notice that the default parameter 'mtry = 2' and `ntree = 500`. It means the number of variables tried at each split is now 2 and the number of trees that can be built is 500. The model's `estimated OOB error rate` is 16.5%. It has an increase in comparison with the first model which was 20%. So the overall accuracy of the model has reached **83.5%**.

Let us make a prediction on train Data to verify the model's training accuracy.

```{r RF_model2_pred}
# RF_model2 Prediction on train
RF_prediction2 <- predict(RF_model2, train)
#check up
conMat<- confusionMatrix(RF_prediction2, train$Survived)
conMat$table
# Misclassification error
paste('Accuracy =', round(conMat$overall["Accuracy"],2))
paste('Error =', round(mean(train$Survived != RF_prediction2), 2)) 
```

We can see the model's accuracy of the training dataset has reached 91%. The result shows that the prediction on survival has 55 wrong predictions out of 527 correct predictions; The prediction on death has 287 correct predictions and 22 wrong predictions. The overall accuracy reaches **91%**. It is again higher than the model learning accuracy **83.5%**.

It has also increased a bit comparing with the accuracy on the estimated accuracy **80%** and the accuracy on train dataset **84%** of the random forest RF_model1. Compare with the decision tree model3, which has identical predictors, the accuracy was **85%** on the training dataset. 

Let us make another submit to Kaggle to see if the prediction on unseen data has been improved. 

```{r}
# produce a submission and submit to Kaggle 
test$Pclass <- as.factor(test$Pclass)
test$Group_size <- as.factor(test$Group_size)

#make prediction
RF_prediction <- predict(RF_model2, test)
submit <- data.frame(PassengerId = test$PassengerId, Survived = RF_prediction)
# Write it into a file "RF_Result.CSV"
write.csv(submit, file = "./data/RF_Result2.CSV", row.names = FALSE)
```
The feedback shows the prediction only increased a lot with a scored **0.78947**! It has improved on the RF_model1 (0.76555) and decision tree model3 (0.77033).  

Let us record these various accuracy.

```{r RF_model2_record}
# Record RF_model2's results
RF_model2_accuracy <- c(83.16, 92, 78.95)
```

## Random Forest with All Variables

Now let use the random forest to build a model with the maximum predictors that can be used from attributes. We may not be able to use all the attributes since the `randomForest` function cannot handle an attribute that is not a factor and has over 53 levels. So, we will not use the attribute *Ticket*. 

```{r RF_model3_Construction}
# RF_model3 construction with the maximum predictors
set.seed(2233)
# RF_model3 <- randomForest(Survived ~ Sex + Pclass + Age 
#                           + SibSp + Parch + Embarked +
#                             HasCabinNum + Friend_size +
#                             Fare_pp + Title + Deck +
#                             Ticket_class + Family_size +
#                             Group_size + Age_group, 
#                           data = train, importance=TRUE)
# save(RF_model3, file = "./data/RF_model3.rda")
```
We can assess the new model,

```{r RF_model3_show}
# Display RE_model3's details
load("./data/RF_model3.rda")
RF_model3
```
Notice that the default parameter `mtry = 3` and `ntree = 500`. It means the number of variables tried at each split is now 3 and number of trees can be built is 500. The model's estimated OOB error rate is 17%. It has an increase in comparison with the model2 which was 18%. So the overall accuracy of the model has reached **83%**.

Let us make a prediction on train Data to verify the model's training accuracy.

```{r RF_model3_check}
# Make a prediction on Train
RF_prediction3 <- predict(RF_model3, train)
#check up
conMat<- confusionMatrix(RF_prediction3, train$Survived)
conMat$table
# Misclassification error
paste('Accuracy =', round(conMat$overall["Accuracy"],2))
paste('Error =', round(mean(train$Survived != RF_prediction3), 2)) 
```

We can see the accuracy on train dataset has reached 95%. The result shows that the prediction on survive has 38 wrong predictions out of 536 correct predictions; The prediction on death has 304 correct predictions and 13 wrong predictions. The overall accuracy reaches **95%**. It is again higher than the model learning accuracy **83%**.

Let us make another submit to Kaggle to see if the prediction on unseen data has been improved. 

```{r}
# produce a submit with Kaggle 
test$Pclass <- as.factor(test$Pclass)
test$Group_size <- as.factor(test$Group_size)

#make prediction
RF_prediction <- predict(RF_model3, test)
submit <- data.frame(PassengerId = test$PassengerId, Survived = RF_prediction)
# Write it into a file "RF_Result.CSV"
write.csv(submit, file = "./data/RF_Result3.CSV", row.names = FALSE)
```
The score is **0.77033**. It shows the decrease of the accuracy. 

Let us record these various accuracy.

```{r RF_model3_records}
# Record RF_model3's results
RF_model3_accuracy <- c(83, 94, 77)
```

## Comparision the Three Random Forest Models

We have produced three random forest models, each has different performance in terms of prediction accuracy on the test dataset. Let us make a quick comparison among them.

```{r RFmodelscomp}
library(tidyr)
Model <- c("RF_Model1","RF_Model2","RF_Model3")
Pre <- c("Sex, Pclass, HasCabinNum, Deck, Fare_pp", "Sex, Fare_pp, Pclass, Title, Age_group, Group_size, Ticket_class, Embarked", "Sex, Pclass, Age, SibSp, Parch, Embarked, HasCabinNum, Friend_size, Fare_pp, Title, Deck, Ticket_class, Family_size, Group_size, Age_group")

Learn <- c(80.0, 83.16, 83.0)
Train <- c(84, 92, 78)
Test <- c(76.555, 78.95, 77.03)
df1 <- data.frame(Model, Pre, Learn, Train, Test)
df2 <- data.frame(Model, Learn, Train, Test)
knitr::kable(df1, longtable = TRUE, booktabs = TRUE, digits = 2, col.names =c("Models", "Predictors", "Accuracy on Learn", "Accuracy on Train", "Accuracy on Test"), 
  caption = 'The Comparision among 3 Random Forest models'
)
```
```{r RFmodelcompare, fig.cap = "Random Froest models' accuracy on model learning, Train dataset and Test dataset."}
df.long <- gather(df2, Dataset, Accuracy, -Model, factor_key =TRUE)
ggplot(data = df.long, aes(x = Model, y = Accuracy, fill = Dataset)) +
  geom_col(position = position_dodge()) 
```

1. It is not true that the more predictors the better performance with Random Forest models.
2. The result of the model validation on the training dataset is not reliable. The higher accuracy on the training dataset does not mean a higher accuracy on the test dataset. 
3. All the model has a degree of overfitting. That is the accuracy on the test data is lower than the training dataset and even lower than the model its own estimated accuracy while learn or construct it.
4. the cause of the overfitting is a complicated issue. It may be related to all the factors: the number of predictors used to build the model, the dataset used to build the model, and the model default parameters.

In comparison with the decision tree models, we have built in the previous Chapter. The random forest models over-perform all the four models on the test dataset. The lowest accuracy is the same as the highest accuracy with the decision tree models (76.55%).  

## Summary {-}

In this chapter, we have demonstrated the use of random forest models for the Titanic problem. We have tried using different numbers of predictors. Their accuracy on the test dataset has been illustrated in the figure \@ref(fig:RFmodelcompare).

Despite the efforts in features' engineering, the careful selection of the predictors, the random forest models have higher accuracy on the train dataset but fall dramatically with the test dataset. It demonstrated the practical problem in a data science project that is overfitting. Overfitting is a serious problem because it is the test dataset (unseen data) matters in real practice. Overfitting can be discovered and eliminated with Cross-validation that is what we are going to discuss in the next Chapter. 

## Exercises {-}

1. Find out what is "OOB estimate of error rate"? How to reduce its value?

2. In a random forest model, its Confusion matrix shows misclassified samples and their error rate. Explain the concept of the "Positive error" and "Negative error" how to balance them? 

3. Try different sampling methods by using the different fold and repeat numbers in the Cross-Validation to see the effect of the tune parameters and model's accuracy.

4. Explore train method in caret with different models and methods.








