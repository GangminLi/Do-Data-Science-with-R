---
title: "10-Fine Tune models"
author: "Gangmin Li"
date: "9/26/2020"
output: html_document
---
### Model tunning for Logistic model
https://rpubs.com/dinnah88/titaniclogisticknn


Feature Engineering for create more accurate model.
step(titanic_m_all, direction = "backward")
## Start:  AIC=639.87
## Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked
## 
##            Df Deviance    AIC
## - Embarked  3   618.52 634.52
## - Parch     1   618.27 638.27
## - Fare      1   619.34 639.34
## <none>          617.87 639.87
## - SibSp     1   627.95 647.95
## - Age       1   642.11 662.11
## - Pclass    2   663.39 681.39
## - Sex       1   803.56 823.56
## 
## Step:  AIC=634.52
## Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare
## 
##          Df Deviance    AIC
## - Parch   1   619.05 633.05
## - Fare    1   620.21 634.21
## <none>        618.52 634.52
## - SibSp   1   629.32 643.32
## - Age     1   642.99 656.99
## - Pclass  2   665.53 677.53
## - Sex     1   812.87 826.87
## 
## Step:  AIC=633.05
## Survived ~ Pclass + Sex + Age + SibSp + Fare
## 
##          Df Deviance    AIC
## - Fare    1   620.43 632.43
## <none>        619.05 633.05
## - SibSp   1   632.89 644.89
## - Age     1   643.38 655.38
## - Pclass  2   668.62 678.62
## - Sex     1   815.52 827.52
## 
## Step:  AIC=632.43
## Survived ~ Pclass + Sex + Age + SibSp
## 
##          Df Deviance    AIC
## <none>        620.43 632.43
## - SibSp   1   633.16 643.16
## - Age     1   645.99 655.99
## - Pclass  2   708.40 716.40
## - Sex     1   821.16 831.16
## 
## Call:  glm(formula = Survived ~ Pclass + Sex + Age + SibSp, family = "binomial", 
##     data = titanic_train)
## 
## Coefficients:
## (Intercept)      Pclass2      Pclass3      Sexmale          Age        SibSp  
##     4.17317     -1.28645     -2.39060     -2.75570     -0.04331     -0.37014  
## 
## Degrees of Freedom: 711 Total (i.e. Null);  706 Residual
## Null Deviance:       944.1 
## Residual Deviance: 620.4     AIC: 632.4
using step backwards, the optimum AIC (this means, the smallest value of AIC), is using the variables: Pclass, Sex, Age, and SibSp.

titanic_m_back <- glm(formula = Survived ~ Pclass + Sex + Age + SibSp, family = "binomial", 
    data = titanic_train)
summary(titanic_m_back)
## 
## Call:
## glm(formula = Survived ~ Pclass + Sex + Age + SibSp, family = "binomial", 
##     data = titanic_train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.3597  -0.5797  -0.4058   0.6045   2.4692  
## 
## Coefficients:
##              Estimate Std. Error z value             Pr(>|z|)    
## (Intercept)  4.173168   0.459195   9.088 < 0.0000000000000002 ***
## Pclass2     -1.286446   0.296515  -4.339           0.00001434 ***
## Pclass3     -2.390600   0.277284  -8.621 < 0.0000000000000002 ***
## Sexmale     -2.755700   0.218720 -12.599 < 0.0000000000000002 ***
## Age         -0.043313   0.008982  -4.822           0.00000142 ***
## SibSp       -0.370135   0.116381  -3.180              0.00147 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 944.09  on 711  degrees of freedom
## Residual deviance: 620.43  on 706  degrees of freedom
## AIC: 632.43
## 
## Number of Fisher Scoring iterations: 5
titanic_m_back2 <- stepAIC(titanic_m_all, direction = "backward")
## Start:  AIC=639.87
## Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked
## 
##            Df Deviance    AIC
## - Embarked  3   618.52 634.52
## - Parch     1   618.27 638.27
## - Fare      1   619.34 639.34
## <none>          617.87 639.87
## - SibSp     1   627.95 647.95
## - Age       1   642.11 662.11
## - Pclass    2   663.39 681.39
## - Sex       1   803.56 823.56
## 
## Step:  AIC=634.52
## Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare
## 
##          Df Deviance    AIC
## - Parch   1   619.05 633.05
## - Fare    1   620.21 634.21
## <none>        618.52 634.52
## - SibSp   1   629.32 643.32
## - Age     1   642.99 656.99
## - Pclass  2   665.53 677.53
## - Sex     1   812.87 826.87
## 
## Step:  AIC=633.05
## Survived ~ Pclass + Sex + Age + SibSp + Fare
## 
##          Df Deviance    AIC
## - Fare    1   620.43 632.43
## <none>        619.05 633.05
## - SibSp   1   632.89 644.89
## - Age     1   643.38 655.38
## - Pclass  2   668.62 678.62
## - Sex     1   815.52 827.52
## 
## Step:  AIC=632.43
## Survived ~ Pclass + Sex + Age + SibSp
## 
##          Df Deviance    AIC
## <none>        620.43 632.43
## - SibSp   1   633.16 643.16
## - Age     1   645.99 655.99
## - Pclass  2   708.40 716.40
## - Sex     1   821.16 831.16
using stepAIC, the optimum AIC (this means, the smallest value of AIC), is using the variables: Pclass, Sex, Age, and SibSp. Same value with the step backwards, so next we only use the step backwards.

Predicting the model
titanic_predict <- predict(titanic_m_back, titanic_test)
class(titanic_predict)
## [1] "numeric"
titanic_test$titanic_prob <- predict(titanic_m_back, titanic_test, type = "response")
Visualize the probability distribution.
ggplot(titanic_test, aes(x=titanic_prob))+geom_density(lwd=0.5)+theme_minimal()
 on this plot, the probability on our test data is skewed to 0, means our data distribute to Not Survived.

titanic_test$titanic_predict <- factor(ifelse(titanic_test$titanic_prob > 0.5, "1", "0"))
head(titanic_test[1:10, c("titanic_predict", "Survived")])
##    titanic_predict Survived
## 3                1        1
## 4                1        1
## 20               1        1
## 28               0        0
## 30               0        0
## 40               1        1
Model Evaluation
confusionMatrix(titanic_test$titanic_predict, titanic_test$Survived, positive = "1")
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  0  1
##          0 85 21
##          1 21 52
##                                           
##                Accuracy : 0.7654          
##                  95% CI : (0.6964, 0.8254)
##     No Information Rate : 0.5922          
##     P-Value [Acc > NIR] : 0.0000008154    
##                                           
##                   Kappa : 0.5142          
##                                           
##  Mcnemar's Test P-Value : 1               
##                                           
##             Sensitivity : 0.7123          
##             Specificity : 0.8019          
##          Pos Pred Value : 0.7123          
##          Neg Pred Value : 0.8019          
##              Prevalence : 0.4078          
##          Detection Rate : 0.2905          
##    Detection Prevalence : 0.4078          
##       Balanced Accuracy : 0.7571          
##                                           
##        'Positive' Class : 1               
## 
Model Interpretaion
Model Interpretation: the ability of the model predicting the Y target is correct, (Accuracy of the model to predict ‘Survived’ and ‘Not Survived’) is at 76.5%. The ability of the model predicting the Actual Positive data (Sensitivity) is correct, is at 71.2%. And the ability of the model predicitng the Actual Negative data (Specificity) is at 80.2%.

### Adjust Random forest parameters {-}

Adjust model's parameters is a parameter optimization problem. 
Depending on the model, the parameters can be adjusted and the search space can be greatly different. sometimes the search space can be huge. It is important to understand the working principles of the model and fully understand how the parameters effect on the model's performance. For example, decision tree has two parameters: complexity parameter (CP) and tune length (TL). CP tells the algorithm to stop when the measure (generally is accuracy) does not improve by this factor. The default value is 0.1. TL tells how many instances to use for training. The default value for tuneLength is 3, meaning 3 different values will be used per control parameter. 

For random forest, there are many parameters can be adjusted but the two main tuning parameters in random forest model is **mtry** and **ntree**.

+ *mtry*: Number of variables randomly selected as testing conditions at each split of decision trees. default value is sqr(col). 
Increasing mtry generally improves the performance of the model as at each node have a higher number of options to be considered. However, this is not necessarily true as this decreases the diversity of individual tree. In the same time it will decrease the speed. Hence, it needs to strike the right balance.

+ *ntree*: Number of trees to grow. default value is 500. Higher number of trees give you better performance but makes your code slower. You should choose as high value as your processor can handle because this makes your predictions stronger and more stable.

The general method used to tune a model is also Cross Validation. When tuning a model's parameter, Cross Validation not only uses training dataset to build model but also uses taining dataset to validate the model where the different parameters can be tuned. To tune a model's parameter needs a baseline is needed for comparison. The baseline is usually the model's with the default parameters. WE demonstrate the process with the same sampling from the Cross validation used above, that is 10 folds and repeat 3 times. 

```{r}
# set seed for reproduction
set.seed(2307)
#sample control 10 folds repeat 3 times
control <- trainControl(method="repeatedcv", number=10, repeats=3)
#Metric compare model is Accuracy
metric <- "Accuracy"
#default mtry
mtry <- sqrt(ncol(trainData))
#because we are build baseline, so grid search is used. mtry has one fixed value 
tunegrid <- expand.grid(.mtry=mtry)
RE_default <- train(Survived ~., data=trainData, method="rf", tuneGrid=tunegrid, trControl=control, ntree=500)
print(RE_default)
print(RE_default$results)
```

We can see that our baseline model with the default parameters (mtry=3 and ntree=500) achieves 80.47% prediction accuracy. 

Now let us try various parameter settings to see if we can improve on the model's prediction accuracy. Fine tune model's parameter is an optimization problem. In general, there can be a range of different optimization strategy available to find optimal parameters for different models. The two of the simplest and most common methods for random forest are random search and grid search.

**Random Search**. Define a search space as a bounded domain of parameter values and randomly sample points in that domain.

**Grid Search**. Define a search space as a grid of parameter values and evaluate every position in the grid.

Grid search is great for spot-checking combinations that are known to perform well generally. Random search is great for discovery and getting parameter combinations that you would not have guessed intuitively, although it often requires more time to execute.

#### Random Search

Random search parameters is also depending on the training method. Method "rf" (random forest) can only tune parameter *mtry* in caret. Let us fix "ntree = 500" and randomly search mtry within a range. This can be good if we are unsure of what the value might be and we want to overcome any biases we may have for setting the parameter. One search strategy that we can use is to try use tunelength variable provided by the train. WE can set up a fairly large number if we want cover randomness. For example, tuneLength=15. 

Let’s try a random search for *mtry* using caret:

```{r paged.print=TRUE}
# Random Search
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="random")
set.seed(2222)
#In the train method for tune the parameters. Method rf can only tune mtry 
RF_random <- train(Survived ~., data=trainData, method="rf", trControl=control, ntree=500, tuneLength=15, metric="Accuracy")
# RF_random <- train(Survived ~., data=trainData, method="rf", tuneLength=15, trControl=control)
print(RF_random)
plot(RF_random)
```
We can see that the most accurate value is  *mtry = 25* with an accuracy of 81.6%. 

#### Grid Search
Another search is method is called Grid search. It defines a grid of parameters to try out. Each axis of the grid is an parameter, and points in the grid are specific combinations of parameters. Because we are only tuning one parameter, the grid search is a linear search through a vector of candidate values.

```{r}
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid")
set.seed(3333)
tunegrid <- expand.grid(.mtry=c(1:15))
RF_gridsearch <- train(Survived ~., data=trainData, method="rf", metric="Accuracy", tuneGrid=tunegrid, trControl=control)
print(RF_gridsearch)
plot(RF_gridsearch)
```
The Grid search method identified the best parameter for mtry is *mtry = 15* and the accuracy the model can reach is 81.65%.

We can see that each search method has different *mtry* suggestions. These mtry values also affected by other factors such as the samples. For example, if you change sampling method (folds and repeat times), you will see the same parameter tune method can produce different results and the model's accuracy (see exercise 7).

#### Number of trees *ntree*

Let us consider another parameter **ntree**. Since our tain cannot tune ntree, the only way to see its effect is fix *mtry* and try different number of trees.  
```{r }
# Manual Search
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid")
tunegrid <- expand.grid(.mtry=c(sqrt(ncol(trainData))))
modellist <- list()
for (ntree in c(500, 1000, 1500, 2000, 2500)) {
	set.seed(3333)
	fit <- train(Survived~., data=trainData, method="rf", metric="Accuracy", tuneGrid=tunegrid, trControl=control, ntree=ntree)
	key <- toString(ntree)
	modellist[[key]] <- fit
}
# compare results
results <- resamples(modellist)
summary(results)
dotplot(results)
```
We can see with the default *mtry =3* setting, the best *ntree* value is 1000. The model can reach 80.78% accuracy. Remember that *mtry = 3* and *ntree = 500* is the default setting with the accuracy of 80.47%. 

#### Consider *mtry* and *ntree* together

let us consider *mtry* and *ntree* in the same time. Since the method *rf* in train can only tune one parameter *mtry*, to tuen two parameter together we have to build our own train method. 

To define our own algorithm in caret, we need define a list that contains a number of custom named elements that the caret package looks for, such as how to fit and how to predict. See below for a definition of a custom random forest algorithm for use with caret that takes both an mtry and ntree parameters.

```{r}
# customRF <- list(type = "Classification", library = "randomForest", loop = NULL)
# customRF$parameters <- data.frame(parameter = c("mtry", "ntree"), class = rep("numeric", 2), label = c("mtry", "ntree"))
# customRF$grid <- function(x, y, len = NULL, search = "grid") {}
# customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
#   randomForest(x, y, mtry = param$mtry, ntree=param$ntree, ...)
# }
# customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
#    predict(modelFit, newdata)
# customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
#    predict(modelFit, newdata, type = "prob")
# customRF$sort <- function(x) x[order(x[,1]),]
# customRF$levels <- function(x) x$classes
# 
# # train model
# control <- trainControl(method="repeatedcv", number=10, repeats=3)
# tunegrid <- expand.grid(.mtry=c(1:5), .ntree=c(500, 1000))
# 
# custom <- train(Survived ~., data=trainData, method=customRF, metric=metric, tuneGrid=tunegrid, trControl=control)
# 
# summary(custom)
```
This will take a long while. Once it is complete we can check the results.

```{r}

#custom$results
knitr::kable(custom$results, booktabs = TRUE,
  caption = 'A table of the *mtry* and *ntree* with their accuracy.'
)
custom$bestTune
bestune <- custom$results[custom$results$mtry==custom$bestTune$mtry&custom$results$ntree==custom$bestTune$ntree, ]
knitr::kable(bestune, booktabs = TRUE,
  caption = 'The best accuracy of *mtry* and *ntree*.'
)
plot(custom)
```
The best result is obtained when two parameters are:
*mtry = 5* and *ntree = 1000*. The accuracy reaches: 81.78%.

We do see some interaction effects between the number of *mtry* and the value of *ntree*. Nevertheless, we have different *mtry* suggestions:
1. Random search mtry = 25, ntree =500, then the accuracy is 81.6%
2. Grid search mtry = 15, ntree =500, then the accuracy is 81.65%
Both suggestions are different with the default value (*mtry* = 3).

However, we we fix *mtry* = 3, the Cross Validation suggests the *ntree* value is 1000, with an accuracy of 80.78%, again is not the default value 500 that has an accuracy of 80.47%. The difference is very small though. 

When we tune both parameters together, we have got another pair of values: *mtry = 5* and *ntree = 1000*. The accuracy reaches 81.78%.

Among all the the different parameter value suggestions, the highest accuracy is 81.78. That is the combination tuning result. Let us use these parameters to produce a model on the train data and make a prediction on the test dataset. We can then submit the result to Kaggle for evaluation. 


```{r}
set.seed(1234)
RFmodel_final <- randomForest(Survived ~ Pclass + Title + Sex + Age_group + Group_size + Ticket_class  + Fare_pp + Deck + HasCabinNum + Embarked, data = trainData, mtry=5, ntree=1000, importance=TRUE)

RFmodel_final

#make a prediction on valiaData
prediction_Valid <- predict(RFmodel_final, validData) 
#check up
confusionMatrix(prediction_Valid, validData$Survived)
# Misclassification error 
paste('Error =', round(mean(validData$Survived != prediction_Valid), 4))

#make prediction on test
prediction_final <- predict(RFmodel_final, test)
#prapare for submit
submit <- data.frame(PassengerId = test$PassengerId, Survived = prediction_final)
# Write it into a file "RF_Result.CSV"
write.csv(submit, file = "RF_Result_final.CSV", row.names = FALSE)
```

We have got 0.74162 score. It does not improve!


