<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1.2 Process of Model Construction | 06-data-analysis.utf8</title>
  <meta name="description" content="" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="1.2 Process of Model Construction | 06-data-analysis.utf8" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1.2 Process of Model Construction | 06-data-analysis.utf8" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="predictive-data-analysis-pda.html"/>
<link rel="next" href="classification-as-a-specific-prediction.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Authoring Books with R Markdown</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="data-analysis.html"><a href="data-analysis.html"><i class="fa fa-check"></i><b>1</b> Data Analysis</a><ul>
<li class="chapter" data-level="1.1" data-path="predictive-data-analysis-pda.html"><a href="predictive-data-analysis-pda.html"><i class="fa fa-check"></i><b>1.1</b> Predictive data analysis (PDA)</a></li>
<li class="chapter" data-level="1.2" data-path="process-of-model-construction.html"><a href="process-of-model-construction.html"><i class="fa fa-check"></i><b>1.2</b> Process of Model Construction</a><ul>
<li class="chapter" data-level="1.2.1" data-path="process-of-model-construction.html"><a href="process-of-model-construction.html#predictor-selection"><i class="fa fa-check"></i><b>1.2.1</b> Predictor Selection</a></li>
<li class="chapter" data-level="1.2.2" data-path="process-of-model-construction.html"><a href="process-of-model-construction.html#model-construction"><i class="fa fa-check"></i><b>1.2.2</b> Model Construction</a></li>
<li class="chapter" data-level="1.2.3" data-path="process-of-model-construction.html"><a href="process-of-model-construction.html#model-validation"><i class="fa fa-check"></i><b>1.2.3</b> Model Validation</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="classification-as-a-specific-prediction.html"><a href="classification-as-a-specific-prediction.html"><i class="fa fa-check"></i><b>1.3</b> Classification as A Specific Prediction</a></li>
<li class="chapter" data-level="1.4" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>1.4</b> Summary</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="process-of-model-construction" class="section level2">
<h2><span class="header-section-number">1.2</span> Process of Model Construction</h2>
<p>The process of constructing a prediction model is called <strong>predictive modeling</strong>. Predictive modeling is generally involves three steps: <strong>Predictor selection</strong>, <strong>model construction</strong> and <strong>model evaluation</strong>.</p>
<div id="predictor-selection" class="section level3">
<h3><span class="header-section-number">1.2.1</span> Predictor Selection</h3>
<p>Predictor, in data science, is an attribute that a prediction model used to predict values of another attribute. The attribute to be predicted is called <strong>consequencer</strong> (or dependent). Generally, an data object can have a large number of attributes, which can potentially be used as predictors by a model to produce consequencer. Most models do not use all of the data attributes, instead only a number of selected attributes are used.</p>
<p>The selection is based on the relationship between predictor and the consequencer and also the relationship among predictors. <em>Filter</em> and <em>wrapper</em> are the most common methods used in the attributes selection:</p>
<ul>
<li><p><strong>Filters</strong>. Filters is a method that examines each predictor in turn. A numerical measure is calculated, representing the strength of the correlation<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> between the predictor attribute and the consequencer. This correlation is conventionally called prediction power of a predictor in the prediction modeling. Only predictor attributes where the correlation measure<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> exceeds a given threshold are selected or simple select the fixed number of the top attributes which has higher correlation measure.</p></li>
<li><p><strong>Wrappers</strong>. A wrapper takes a group of predictors and considers the “value add” of each attribute compared to other attributes in the group. If two attributes tell you more or less the same thing (e.g. age and date of birth) then one will be discarded because it adds no value. Step-wise linear regression<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> and principal component analysis<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> are two popular wrapper methods.</p></li>
</ul>
<p>It does not matter which methods you are going to use, it can only be effective once the basic analytical tasks are performed. These analytical tasks can provide you a clear understanding of the correlation and covariance of existing attributes. So you can be confident to selection suitable predictors. These tasks are:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Correlation Analysis</strong>. Analysis the correlation among the attributes, and ordering them based the correlation of attributes with the dependent attribute. Select appropriate number of the attributes from the highest value towards the lowest value of correlation.</p></li>
<li><p><strong>Principal component analysis (PCA)</strong>. PCA is a dimension reduction method by projecting each data point onto only the first few principal components to obtain lower-dimensional data while preserving as much of the data’s variation as possible. PCA only works on numerical variables.</p></li>
<li><p><strong>Possibly factor analysis (FA)</strong>. Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors. For example, it is possible that variations in six observed variables mainly reflect the variations in two unobserved (underlying) variables.</p></li>
</ol>
<p>There are other similar tasks such as <strong>MCA</strong>, <strong>FAMD</strong>, <strong>CA</strong>, and <strong>MFA</strong>. MCA stands for multiple correspondence analysis. It can only apply to categorical variables; FAMD stands for factor analysis of mixed data. It can apply to both numerical and categorical variables; CA is correspondence analysis, it can only works on two variables (contingency table); MFA is multiple factor analysis, it is needed only when you have variables set by group. These tasks are all specific of the PCA.</p>
<p>In this section we will demonstrate the basic Correlation analysis and principal component analysis to understand the relationship among attributes and between predictor and the dependent variable. We will continue to use the Titantic example.</p>
<div id="attributes-correlation-analysis" class="section level4 unnumbered">
<h4>Attributes Correlation Analysis</h4>
<p>We have re-engineered the Titanic dataset. So instead of using the original dataset, let us consider the correlation among attributes of our re-engineered dataset.</p>
<pre><code>## Rows: 1,309
## Columns: 18
## $ PassengerId  &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,...
## $ Survived     &lt;int&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, ...
## $ Pclass       &lt;int&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, ...
## $ Sex          &lt;fct&gt; male, female, female, female, male, male, male, male, ...
## $ Age          &lt;dbl&gt; 22.00000, 38.00000, 26.00000, 35.00000, 35.00000, 27.4...
## $ SibSp        &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, ...
## $ Parch        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, ...
## $ Ticket       &lt;fct&gt; A/5 21171, PC 17599, STON/O2. 3101282, 113803, 373450,...
## $ Embarked     &lt;fct&gt; S, C, S, S, S, Q, S, S, S, C, S, S, S, S, S, S, Q, S, ...
## $ HasCabinNum  &lt;fct&gt; HasNo, Has, HasNo, Has, HasNo, HasNo, Has, HasNo, HasN...
## $ Friend_size  &lt;int&gt; 1, 2, 1, 2, 1, 1, 2, 5, 3, 2, 3, 1, 1, 7, 1, 1, 6, 1, ...
## $ Fare_pp      &lt;dbl&gt; 7.250000, 35.641650, 7.925000, 26.550000, 8.050000, 8....
## $ Title        &lt;fct&gt; Mr, Mrs, Miss, Mrs, Mr, Mr, Mr, Master, Mrs, Mrs, Miss...
## $ Deck         &lt;fct&gt; U, C, U, C, U, U, E, U, U, U, G, C, U, U, U, U, U, U, ...
## $ Ticket_class &lt;fct&gt; A, P, S, 1, 3, 3, 1, 3, 3, 2, P, 1, A, 3, 3, 2, 3, 2, ...
## $ Family_size  &lt;int&gt; 2, 2, 1, 2, 1, 1, 1, 5, 3, 2, 3, 1, 1, 7, 1, 1, 6, 1, ...
## $ Group_size   &lt;int&gt; 2, 2, 1, 2, 1, 1, 2, 5, 3, 2, 3, 1, 1, 7, 1, 1, 6, 1, ...
## $ Age_group    &lt;fct&gt; 20-29, 30-39, 20-29, 30-39, 30-39, 20-29, 50-59, 0-9, ...</code></pre>
<pre><code>##   PassengerId      Survived          Pclass          Sex           Age       
##  Min.   :   1   Min.   :0.0000   Min.   :1.000   female:466   Min.   : 0.17  
##  1st Qu.: 328   1st Qu.:0.0000   1st Qu.:2.000   male  :843   1st Qu.:22.00  
##  Median : 655   Median :0.0000   Median :3.000                Median :27.43  
##  Mean   : 655   Mean   :0.3838   Mean   :2.295                Mean   :29.63  
##  3rd Qu.: 982   3rd Qu.:1.0000   3rd Qu.:3.000                3rd Qu.:37.00  
##  Max.   :1309   Max.   :1.0000   Max.   :3.000                Max.   :80.00  
##                 NA&#39;s   :418                                                  
##      SibSp            Parch            Ticket     Embarked HasCabinNum 
##  Min.   :0.0000   Min.   :0.000   CA. 2343:  11   C:272    Has  : 295  
##  1st Qu.:0.0000   1st Qu.:0.000   1601    :   8   Q:123    HasNo:1014  
##  Median :0.0000   Median :0.000   CA 2144 :   8   S:914                
##  Mean   :0.4989   Mean   :0.385   3101295 :   7                        
##  3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7                        
##  Max.   :8.0000   Max.   :9.000   347082  :   7                        
##                                   (Other) :1261                        
##   Friend_size      Fare_pp           Title          Deck       Ticket_class
##  Min.   : 1.0   Min.   :  0.000   Master: 61   U      :1014   3      :429  
##  1st Qu.: 1.0   1st Qu.:  7.579   Miss  :260   C      :  94   2      :278  
##  Median : 1.0   Median :  8.050   Mr    :757   B      :  65   1      :210  
##  Mean   : 2.1   Mean   : 14.765   Mrs   :197   D      :  46   P      : 98  
##  3rd Qu.: 3.0   3rd Qu.: 15.000   Other : 34   E      :  41   S      : 98  
##  Max.   :11.0   Max.   :128.082                A      :  22   C      : 77  
##                                                (Other):  27   (Other):119  
##   Family_size       Group_size       Age_group  
##  Min.   : 1.000   Min.   : 1.000   20-29  :552  
##  1st Qu.: 1.000   1st Qu.: 1.000   30-39  :229  
##  Median : 1.000   Median : 1.000   40-49  :171  
##  Mean   : 1.884   Mean   : 2.194   10-19  :162  
##  3rd Qu.: 2.000   3rd Qu.: 3.000   0-9    :100  
##  Max.   :11.000   Max.   :11.000   50-59  : 62  
##                                    (Other): 33</code></pre>
<p>A quick correlation plot of the numeric attributes to get an idea of how they might relate to one another. You can see that we have dropped two <code>chr</code> attributes: <em>Title</em> and <em>Deck</em>. We could include them if we convert the character value in to numbers. For example, title could be converted into 1-6 numbers as 1 represents <code>Mr</code>, 2 represents <code>Mrs</code> and so on.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-1"></span>
<img src="06-data-analysis_files/figure-html/unnamed-chunk-1-1.svg" alt="Correlation among numerical attributes" width="95%" />
<p class="caption">
Figure 1.2: Correlation among numerical attributes
</p>
</div>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="process-of-model-construction.html#cb4-1"></a><span class="co"># show in table</span></span>
<span id="cb4-2"><a href="process-of-model-construction.html#cb4-2"></a><span class="kw">library</span>(kableExtra) <span class="co"># markdown tables </span></span></code></pre></div>
<pre><code>## Warning: package &#39;kableExtra&#39; was built under R version 3.6.3</code></pre>
<pre><code>## 
## Attaching package: &#39;kableExtra&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     group_rows</code></pre>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="process-of-model-construction.html#cb8-1"></a>lower &lt;-<span class="st"> </span><span class="kw">round</span>(cor,<span class="dv">2</span>)</span>
<span id="cb8-2"><a href="process-of-model-construction.html#cb8-2"></a>lower[<span class="kw">lower.tri</span>(cor, <span class="dt">diag=</span><span class="ot">TRUE</span>)]&lt;-<span class="st">&quot;&quot;</span></span>
<span id="cb8-3"><a href="process-of-model-construction.html#cb8-3"></a>lower &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(lower)</span>
<span id="cb8-4"><a href="process-of-model-construction.html#cb8-4"></a>knitr<span class="op">::</span><span class="kw">kable</span>(lower, <span class="dt">booktabs =</span> <span class="ot">TRUE</span>,</span>
<span id="cb8-5"><a href="process-of-model-construction.html#cb8-5"></a>  <span class="dt">caption =</span> <span class="st">&#39;Coorelations among attributes&#39;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb8-6"><a href="process-of-model-construction.html#cb8-6"></a><span class="st">  </span><span class="kw">kable_styling</span>(<span class="dt">bootstrap_options =</span> <span class="kw">c</span>(<span class="st">&quot;striped&quot;</span>, <span class="st">&quot;hover&quot;</span>, <span class="st">&quot;condensed&quot;</span>, <span class="st">&quot;responsive&quot;</span>, <span class="dt">font_size =</span> <span class="dv">8</span>))</span></code></pre></div>
<table class="table table-striped table-hover table-condensed table-responsive" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:unnamed-chunk-2">Table 1.1: </span>Coorelations among attributes
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
Survived
</th>
<th style="text-align:left;">
Pclass
</th>
<th style="text-align:left;">
Sex
</th>
<th style="text-align:left;">
Age
</th>
<th style="text-align:left;">
SibSp
</th>
<th style="text-align:left;">
Parch
</th>
<th style="text-align:left;">
Embarked
</th>
<th style="text-align:left;">
HasCabinNum
</th>
<th style="text-align:left;">
Friend_size
</th>
<th style="text-align:left;">
Fare_pp
</th>
<th style="text-align:left;">
Title
</th>
<th style="text-align:left;">
Deck
</th>
<th style="text-align:left;">
Ticket_class
</th>
<th style="text-align:left;">
Family_size
</th>
<th style="text-align:left;">
Group_size
</th>
<th style="text-align:left;">
Age_group
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Survived
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
-0.34
</td>
<td style="text-align:left;">
-0.54
</td>
<td style="text-align:left;">
-0.05
</td>
<td style="text-align:left;">
-0.04
</td>
<td style="text-align:left;">
0.08
</td>
<td style="text-align:left;">
-0.17
</td>
<td style="text-align:left;">
-0.32
</td>
<td style="text-align:left;">
0.07
</td>
<td style="text-align:left;">
0.29
</td>
<td style="text-align:left;">
-0.05
</td>
<td style="text-align:left;">
-0.3
</td>
<td style="text-align:left;">
-0.04
</td>
<td style="text-align:left;">
0.02
</td>
<td style="text-align:left;">
0.08
</td>
<td style="text-align:left;">
-0.03
</td>
</tr>
<tr>
<td style="text-align:left;">
Pclass
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
0.12
</td>
<td style="text-align:left;">
-0.43
</td>
<td style="text-align:left;">
0.06
</td>
<td style="text-align:left;">
0.02
</td>
<td style="text-align:left;">
0.19
</td>
<td style="text-align:left;">
0.71
</td>
<td style="text-align:left;">
-0.08
</td>
<td style="text-align:left;">
-0.77
</td>
<td style="text-align:left;">
-0.22
</td>
<td style="text-align:left;">
0.73
</td>
<td style="text-align:left;">
-0.02
</td>
<td style="text-align:left;">
0.05
</td>
<td style="text-align:left;">
-0.07
</td>
<td style="text-align:left;">
-0.45
</td>
</tr>
<tr>
<td style="text-align:left;">
Sex
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
0.06
</td>
<td style="text-align:left;">
-0.11
</td>
<td style="text-align:left;">
-0.21
</td>
<td style="text-align:left;">
0.1
</td>
<td style="text-align:left;">
0.14
</td>
<td style="text-align:left;">
-0.17
</td>
<td style="text-align:left;">
-0.12
</td>
<td style="text-align:left;">
0.01
</td>
<td style="text-align:left;">
0.13
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
-0.19
</td>
<td style="text-align:left;">
-0.2
</td>
<td style="text-align:left;">
0.07
</td>
</tr>
<tr>
<td style="text-align:left;">
Age
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
-0.27
</td>
<td style="text-align:left;">
-0.15
</td>
<td style="text-align:left;">
-0.07
</td>
<td style="text-align:left;">
-0.3
</td>
<td style="text-align:left;">
-0.2
</td>
<td style="text-align:left;">
0.38
</td>
<td style="text-align:left;">
0.49
</td>
<td style="text-align:left;">
-0.32
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
-0.26
</td>
<td style="text-align:left;">
-0.2
</td>
<td style="text-align:left;">
0.98
</td>
</tr>
<tr>
<td style="text-align:left;">
SibSp
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
0.37
</td>
<td style="text-align:left;">
0.07
</td>
<td style="text-align:left;">
0.01
</td>
<td style="text-align:left;">
0.68
</td>
<td style="text-align:left;">
-0.05
</td>
<td style="text-align:left;">
-0.2
</td>
<td style="text-align:left;">
0.01
</td>
<td style="text-align:left;">
0.05
</td>
<td style="text-align:left;">
0.86
</td>
<td style="text-align:left;">
0.73
</td>
<td style="text-align:left;">
-0.27
</td>
</tr>
<tr>
<td style="text-align:left;">
Parch
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
0.05
</td>
<td style="text-align:left;">
-0.04
</td>
<td style="text-align:left;">
0.65
</td>
<td style="text-align:left;">
-0.03
</td>
<td style="text-align:left;">
-0.09
</td>
<td style="text-align:left;">
-0.03
</td>
<td style="text-align:left;">
0.06
</td>
<td style="text-align:left;">
0.79
</td>
<td style="text-align:left;">
0.67
</td>
<td style="text-align:left;">
-0.14
</td>
</tr>
<tr>
<td style="text-align:left;">
Embarked
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
0.21
</td>
<td style="text-align:left;">
0.01
</td>
<td style="text-align:left;">
-0.3
</td>
<td style="text-align:left;">
-0.03
</td>
<td style="text-align:left;">
0.24
</td>
<td style="text-align:left;">
-0.04
</td>
<td style="text-align:left;">
0.07
</td>
<td style="text-align:left;">
0.02
</td>
<td style="text-align:left;">
-0.06
</td>
</tr>
<tr>
<td style="text-align:left;">
HasCabinNum
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
-0.1
</td>
<td style="text-align:left;">
-0.65
</td>
<td style="text-align:left;">
-0.14
</td>
<td style="text-align:left;">
0.96
</td>
<td style="text-align:left;">
0.03
</td>
<td style="text-align:left;">
-0.01
</td>
<td style="text-align:left;">
-0.09
</td>
<td style="text-align:left;">
-0.3
</td>
</tr>
<tr>
<td style="text-align:left;">
Friend_size
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
0.09
</td>
<td style="text-align:left;">
-0.19
</td>
<td style="text-align:left;">
-0.1
</td>
<td style="text-align:left;">
0.12
</td>
<td style="text-align:left;">
0.8
</td>
<td style="text-align:left;">
0.97
</td>
<td style="text-align:left;">
-0.2
</td>
</tr>
<tr>
<td style="text-align:left;">
Fare_pp
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
0.18
</td>
<td style="text-align:left;">
-0.7
</td>
<td style="text-align:left;">
0.1
</td>
<td style="text-align:left;">
-0.05
</td>
<td style="text-align:left;">
0.09
</td>
<td style="text-align:left;">
0.38
</td>
</tr>
<tr>
<td style="text-align:left;">
Title
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
-0.15
</td>
<td style="text-align:left;">
0.01
</td>
<td style="text-align:left;">
-0.18
</td>
<td style="text-align:left;">
-0.18
</td>
<td style="text-align:left;">
0.48
</td>
</tr>
<tr>
<td style="text-align:left;">
Deck
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
0.02
</td>
<td style="text-align:left;">
-0.01
</td>
<td style="text-align:left;">
-0.1
</td>
<td style="text-align:left;">
-0.32
</td>
</tr>
<tr>
<td style="text-align:left;">
Ticket_class
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
0.06
</td>
<td style="text-align:left;">
0.11
</td>
<td style="text-align:left;">
0.01
</td>
</tr>
<tr>
<td style="text-align:left;">
Family_size
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
0.85
</td>
<td style="text-align:left;">
-0.26
</td>
</tr>
<tr>
<td style="text-align:left;">
Group_size
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
-0.2
</td>
</tr>
<tr>
<td style="text-align:left;">
Age_group
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
</tbody>
</table>
<p>The plot shows not only the correlation between other attributes, which can potentially be used as predictors, with the dependent attribute <em>Survived</em>, but also the correction among potential predictors. In terms of correlation with <em>Survived</em>, <em>Sex</em> has the largest value but in negative -0.54, the next is <em>Pclass</em> with -0.34. So if we can only have two predictors for survive, the first two we should use are <em>Sex</em> and <em>Pclass</em>. If I want to choose five predictors for survive, I would choos Sex, Pclass, HasCabinNum, Deck and Fare_PP.</p>
<p>The largest correlation value is between <em>Pclass</em> and <em>Farepp</em> with -0.77, which is even more than the value between <em>Survived</em> and <em>Sex</em>. One thing it tells us is that if we have <em>Pclass</em> in our model, we may not need to use <em>Farepp</em> since they are effectively tell us the same thing. In the same time, They illustrated and approved one thing that we have suspected in the beginning that is the social class of a passenger. This social class can be interpreted as the richer people, who paid more money on a ticket, has a better cabin. This also told us that if we want reduce the attributes number in a model we can choose one among the three <em>Pcalss</em>, <em>Farepp</em> and <em>Ticket_class</em>.</p>
<p>The important point is that the correlation analysis is very useful. It provide the basic reasons for our predictor selection. For example, if only choose three predictors, we should choose the three attributes which has the most absolute correlation values with <em>survived</em>. If in a model we have chosen <em>Pclass</em> we may not need to choose <em>Fare_pp</em>, <em>Deck</em> and <em>HasCabinNum</em> because these four have large correction values.</p>
</div>
<div id="pca-analysis" class="section level4 unnumbered">
<h4>PCA Analysis</h4>
<p>PCA and Factor analysis are most commonly used methods in dimension reduction. In a general data science project, it is possible that a given dataset can has tens or hundreds of features (attributes). For example in the text analysis, if we count words appearance in a document, we could easily have hundreds even thousands of dimensions. If we want reduce the dimension into a manageable numbers, PCA can be very useful. Particularly in visualization, human are not good with anything over three dimensions.</p>
<p>PCA uses <em>Eigenvalues</em> and <em>Eigenvectors</em><a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> to reserve the original data information and variation as much as possible. Therefore PCA is simple to calculate the given data’s Eigenvectors.</p>
<p>PCA normally has the following steps:</p>
<ol style="list-style-type: decimal">
<li>Calculate the Covariance Matrix<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> of the given dataset.</li>
<li>Calculate the Eigenvalues and Eigenvectors of the resulting Covariance Matrix.</li>
<li>The resulting Eigenvector that correspond to the largest Eigenvalue can then be used to reconstruct a large fraction of the variance of the original dataset.</li>
</ol>
<p>In R, we have a function called <code>prcomp()</code>. It takes numerical values. So for demonstration we only use the same 8 attributes we have used in our correlation analysis. Let us take the first</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="process-of-model-construction.html#cb9-1"></a><span class="co"># RE_data %&gt;%</span></span>
<span id="cb9-2"><a href="process-of-model-construction.html#cb9-2"></a><span class="co">#   select(Survived, Pclass, Sex, Age_group, Group_size, Ticket_class, Fare_pp, Embarked) %&gt;%</span></span>
<span id="cb9-3"><a href="process-of-model-construction.html#cb9-3"></a><span class="co"># RE_data[1:891, ]</span></span>
<span id="cb9-4"><a href="process-of-model-construction.html#cb9-4"></a><span class="co">#summary(RE_data[1:891,c(2:3,5:9,12)])</span></span>
<span id="cb9-5"><a href="process-of-model-construction.html#cb9-5"></a><span class="co">#summary(RE_data[1:891,c(-1, -8)])</span></span>
<span id="cb9-6"><a href="process-of-model-construction.html#cb9-6"></a><span class="co">#data.pca &lt;- prcomp(RE_data[1:891,c(2:3,5:9,12)], center = TRUE, scale = TRUE)</span></span>
<span id="cb9-7"><a href="process-of-model-construction.html#cb9-7"></a>data.pca &lt;-<span class="st"> </span><span class="kw">prcomp</span>(RE_data[<span class="dv">1</span><span class="op">:</span><span class="dv">891</span>,<span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">-8</span>)], <span class="dt">center =</span> <span class="ot">TRUE</span>, <span class="dt">scale =</span> <span class="ot">TRUE</span>)</span>
<span id="cb9-8"><a href="process-of-model-construction.html#cb9-8"></a><span class="kw">summary</span>(data.pca)</span></code></pre></div>
<pre><code>## Importance of components:
##                           PC1    PC2    PC3     PC4     PC5     PC6     PC7
## Standard deviation     2.1657 1.9563 1.3177 1.09265 1.03321 0.92535 0.79382
## Proportion of Variance 0.2931 0.2392 0.1085 0.07462 0.06672 0.05352 0.03938
## Cumulative Proportion  0.2931 0.5323 0.6409 0.71547 0.78219 0.83571 0.87509
##                            PC8    PC9    PC10    PC11    PC12    PC13    PC14
## Standard deviation     0.75698 0.6670 0.62061 0.55204 0.45560 0.19713 0.15089
## Proportion of Variance 0.03581 0.0278 0.02407 0.01905 0.01297 0.00243 0.00142
## Cumulative Proportion  0.91091 0.9387 0.96278 0.98183 0.99480 0.99723 0.99865
##                           PC15      PC16
## Standard deviation     0.14679 1.058e-15
## Proportion of Variance 0.00135 0.000e+00
## Cumulative Proportion  1.00000 1.000e+00</code></pre>
<p>We have obtained 8 principal components, which named as PC1 to PC16. Each of these explains a percentage of the total variation in the dataset. That is to say, PC1 explains 29% of the total variance, PC2 explains 24% of the variance. Together nearly half of the information in the dataset can be encapsulated by just these two principal components. So, by knowing the position of a sample in relation to just PC1 and PC2, you can get a very accurate view on where it stands in relation to other samples, as just PC1 and PC2 can explain 53% of the variance.</p>
<p>Let’s call str() to have a look at the PCA object.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="process-of-model-construction.html#cb11-1"></a><span class="kw">str</span>(data.pca)</span></code></pre></div>
<pre><code>## List of 5
##  $ sdev    : num [1:16] 2.17 1.96 1.32 1.09 1.03 ...
##  $ rotation: num [1:16, 1:16] 0.0202 -0.1892 0.0781 0.3107 -0.3583 ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..$ : chr [1:16] &quot;Survived&quot; &quot;Pclass&quot; &quot;Sex&quot; &quot;Age&quot; ...
##   .. ..$ : chr [1:16] &quot;PC1&quot; &quot;PC2&quot; &quot;PC3&quot; &quot;PC4&quot; ...
##  $ center  : Named num [1:16] 0.384 2.309 1.648 29.452 0.523 ...
##   ..- attr(*, &quot;names&quot;)= chr [1:16] &quot;Survived&quot; &quot;Pclass&quot; &quot;Sex&quot; &quot;Age&quot; ...
##  $ scale   : Named num [1:16] 0.487 0.836 0.478 13.432 1.103 ...
##   ..- attr(*, &quot;names&quot;)= chr [1:16] &quot;Survived&quot; &quot;Pclass&quot; &quot;Sex&quot; &quot;Age&quot; ...
##  $ x       : num [1:891, 1:16] -0.442 1.794 0.032 1.509 0.902 ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..$ : chr [1:891] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...
##   .. ..$ : chr [1:16] &quot;PC1&quot; &quot;PC2&quot; &quot;PC3&quot; &quot;PC4&quot; ...
##  - attr(*, &quot;class&quot;)= chr &quot;prcomp&quot;</code></pre>
<p>I won’t describe the results in detail, but the PCA object contains the following information:</p>
<ul>
<li>The center point (<code>$center</code>), scaling (<code>$scale</code>), standard deviation(<code>sdev</code>) of each principal component</li>
<li>The relationship (correlation or anti-correlation, etc) between the initial variables and the principal components (<code>$rotation</code>)</li>
<li>The values of each sample in terms of the principal components (<code>$x</code>)</li>
</ul>
Let us plot PCA, we need to use <strong>biplot</strong>, which includes both the position of each sample in terms of PC1 and PC2 and also will show how the initial variables map onto this. We need ggbiplot package, which offers a user-friendly and pretty function to plot biplots. A biplot is a type of plot that will allow you to visualize how the samples relate to one another in our PCA (which samples are similar and which are different) and will simultaneously reveal how each variable contributes to each principal component.
<div class="figure" style="text-align: center"><span id="fig:PCA"></span>
<img src="06-data-analysis_files/figure-html/PCA-1.svg" alt="The 1st and the 2nd PCs ploted with ggplot_pca" width="95%" />
<p class="caption">
Figure 1.3: The 1st and the 2nd PCs ploted with ggplot_pca
</p>
</div>
<p>The axes are seen as arrows originating from the center point. Here, you see that the variables <em>Fare_pp</em>, <em>Age_group</em> and <em>Survived</em> contribute to PC1, with higher values in those variables moving the records to the right on this plot. This lets you see how the data points relate to the axes.</p>
We also have other principal components available although they may have less weights in comparison with the first two. Each of other components map differently to the original variables. We can also plot these other components, for example PC3 and PC4. If you look into the PC3 and PC4, they are <em>Sex</em> and <em>Age_group</em>. You may wondering what do they do with our prediction. Well, it can show at least the contribution between them with the dependent variable <em>Survived</em>, in addition, it can also show the covariance of the both with other variables.<br />

<div class="figure" style="text-align: center"><span id="fig:PCA2"></span>
<img src="06-data-analysis_files/figure-html/PCA2-1.svg" alt="The 3rd and the 4th PC ploted with ggplot_pca" width="95%" />
<p class="caption">
Figure 1.4: The 3rd and the 4th PC ploted with ggplot_pca
</p>
</div>
<p>This Plot shows that original attributes <em>Ticket_class</em>, <em>Sex</em>, <em>Fare_PP</em> and <em>Group_size</em> contribute to PC3, which is <em>Sex</em>, in a negative way. It means that with lower values in those variables, the records will move to the left on this plot.</p>
<p>The relationship between original attributes with the newly created Principle Components also indicates the relationship between original attributes and correlation among them.</p>
<p>With these correlation and PCA analyses, We can have a pretty good idea about the attributes. Depending on the models we are constructing, we can be confident to select the number of the predictors and specific predictors to ensure our model has a good performance.</p>
<p>Attribute selection is a parsimonious process that aims to identify a minimal set of predictors for the maximum gain (predictive accuracy). This approach is the opposite of data pre-process where as many meaningful attributes as possible are considered for potential use.</p>
<p>It is also important to recognize that attribute selection cloud be an iterative process that occurs throughout the model building process. It finishes after no more improvement can be achieved in terms of model accuracy.</p>
</div>
</div>
<div id="model-construction" class="section level3">
<h3><span class="header-section-number">1.2.2</span> Model Construction</h3>
<p>Model construction normally involves two phases: <strong>induction</strong> and <strong>deduction</strong>.</p>
<ul>
<li><p>Induction is also called model learning, which means learn to predict;</p></li>
<li><p>Deduction is called model application, which means model applied to predict.</p></li>
</ul>
<p>The division of model learn and model application allows a predictive model to be mature while induction using <strong>training dataset</strong> to construct a model and deduction using <strong>testing dataset</strong> to test and adjust the model constructed.</p>
<p>There are many predictive models exists for different purposes. Many different methods can be used to create a model, and more are being developed all the time. Three broad predictive models based on the model format and the way it is built are <strong>Math model</strong>, <strong>Rule-based model</strong> and <strong>Machine Learning model</strong>.</p>
<div id="math-model" class="section level4 unnumbered">
<h4>Math model</h4>
<p>Mathematical formulated model is the model produced by mathematical formula which combines multiple predictors (attributes) to predict a response (we called it targeted attribute). A predictor is a single attribute in a data object that contributes to the result of the prediction, which is consequencer (also called dependents in same applications).</p>
<p>A well-known example of math model is <strong>Regression model</strong>. A linear regression model is a target function <span class="math inline">\(f\)</span> that maps each attribute set <span class="math inline">\(X\)</span> into a continuous-valued output <span class="math inline">\(y\)</span> with minimum error.</p>
<p><span class="math display" id="eq:binom">\[\begin{equation} 
  y = f(x) = f(x)= ω_1 x+ω_0,
  \tag{1.1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(ω_0\)</span> and <span class="math inline">\(ω_1\)</span> are parameters of the model and are called the <em>regression coefficients</em>. The model is to find the parameters <span class="math inline">\((ω_1, ω_0)\)</span> that minimize the sum of the squared error (SSE),</p>
<p><span class="math display" id="eq:sse">\[\begin{equation} 
 SSE= \Sigma^{N}_{i=1}[y_i-f(x_i)]^2 = \Sigma^{N}_{i=1}[y_i - ω_1 x + ω_0 ]^2
  \tag{1.2}
\end{equation}\]</span></p>
<p>Clearly the linear regression is very simple, its prediction is also limited. So you can have more complicated models like <strong>Logistic Regression</strong> and <strong>Support victor machine (SVM)</strong>.</p>
</div>
<div id="rule-based-model" class="section level4 unnumbered">
<h4>Rule-based model</h4>
<p>In a rule-based model, the model is a collection of rules. For example a model for customer retention may be something like,</p>
<p><code>if the customer is rural, and her monthly usage is high, then the customer will probably renew.</code></p>
<p>In rule-based model, a model is a collection of <code>if … then …</code> rules. List below shows an example of a classification model generated by a rule-based classifier for the vertebrate classification problem.</p>
<p><span class="math display">\[\begin{equation} 
r_1:  (Gives Birth = no) ∧ (Aerial Creature = yes) → Birds\\
r_2:    (Gives Birth = no) ∧ (Aquatic Creature = yes) → Fishes\\
r_3:    (Gives Birth = yes) ∧ (Body Temperature = warm-blooded) → Mammals\\
r_4:    (Gives Birth = no) ∧ (Aerial Creature = no) → Reptiles\\
r_5:    (Aquatic Creature = semi) → Amphibians
\end{equation}\]</span></p>
<p>The rules for the model are represented in a disjunctive normal form <span class="math inline">\(R=(r_1 \vee r_2\vee … \vee r_k)\)</span>, where <span class="math inline">\(R\)</span> is known as the rule set and <span class="math inline">\(r_i\)</span> are the model rules.
Each rule is expressed in a form of:</p>
<p><span class="math display" id="eq:rule">\[\begin{equation} 
r_i:   (Condition_i) →  y_i.
  \tag{1.3}
\end{equation}\]</span></p>
<p>The left-hand side of the rule is called the <strong>rule antecedent or precondition</strong>. It contains a conjunction of attribute test:</p>
<p><span class="math display" id="eq:condition">\[\begin{equation} 
condition_i = (A_1\quad op\quad v_1 ) ∧ (A_2\quad op\quad v_2 ) ∧ … ∧(A_k\quad  op\quad v_k ),
  \tag{1.4}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\((A_j\quad op\quad v_j )\)</span> is an attribute-value pair and <span class="math inline">\(op\)</span> is a relation operator chosen from the set <span class="math inline">\(\{ =, ≠, &lt;, &gt;, ≤, ≥ \}\)</span>. Each attribute test <span class="math inline">\((A_j\quad op \quad v_j )\)</span> is known as a conjunct. The right hand of the rule is called the rule consequent which contains the value of conceqencer <span class="math inline">\(y_i\)</span>.</p>
</div>
<div id="machine-learning-model" class="section level4 unnumbered">
<h4>Machine Learning Model</h4>
<p>In many applications the relationship between the predictor and the concequencer is non-deterministic or is too difficult to either formulate a model or figure out rules by human. In these cases, advanced technologies are used to generate prediction models automatically taking advantage of massive computer storage and fast computation power of distributed and cloud based computing infrastructure. The models used in these situations are mostly mathematical formula and even Neural Networks (NN). Expression <a href="process-of-model-construction.html#eq:ML">(1.5)</a> is a good illustration.</p>
<p><span class="math display" id="eq:ML">\[\begin{equation} 

Input → f(w_1,w_2, ...,w_n) → Output

\tag{1.5}
\end{equation}\]</span></p>
<p>In the machine learning, different predictive models are utilized and tested to produce a valid prediction such as <strong>regression</strong>, <strong>decision tree</strong>, and <strong>decision forest</strong>, etc.</p>
<p>The Machine Leaning approach also takes a <em>“black box”</em> approach that is ignoring the detailed transformation between predictors and the consequence, and simply simulating input and out through NN. Neural Network modeling heavily relies on features engineering that is features extraction and features selection. One way to overcome this problem is an approach called <strong>Deep Leaning</strong>. Deep learning is built based on the concept of NN and adds extra layers between the input and the output layers. Figure <a href="process-of-model-construction.html#fig:diff">1.5</a> shows the differences between Machine Learning and Deep Learning.</p>
<div class="figure" style="text-align: center"><span id="fig:diff"></span>
<img src="images/Diff-ML-DL.jpeg" alt="Comparison between Machine Learning &amp; Deep Learning" width="100%" />
<p class="caption">
Figure 1.5: Comparison between Machine Learning &amp; Deep Learning
</p>
</div>
<p>There are three important types of neural networks that are also called pre-trained models in deep learning: <strong>Artificial Neural Networks (ANN)</strong>, <strong>Convolution Neural Networks (CNN)</strong> and <strong>Recurrent Neural Networks (RNN)</strong>. We will not use them but it is good to understand what are they.</p>
<p>In practices, predictive models used in a Data Science project are mostly mathematics formulated models. They are implemented in different computer program with different languages. They are normally packed into integrated software packages. This software undertakes a mixture of training data and goes through number crunching, parameter adjustment and error correction, which normally called trial or training, and finally produces a working prediction model. During the process of machine generating model, human involvement is much less but needed. It enables fine tune the model and improving on its performance.</p>
</div>
</div>
<div id="model-validation" class="section level3">
<h3><span class="header-section-number">1.2.3</span> Model Validation</h3>
<p>A major problem when building models is that it is relatively easy to build a prediction model but it is not easy to prove the model is useful. That is to say finding relationships that are exist and are the result of random patterns in the training and testing datasets, but this relationship may not exist in the unseen datasets. Model validation is the task of confirming that the outputs of a model have enough fidelity to the outputs of the model building process that the objectives of the model can be achieved.</p>
<p>In practice, a model can be <strong>over-fitted</strong> or <strong>under-fitted</strong>. An over-fitted model can perform extremely good in the test with the test dataset but perform significantly worse with new unseen dataset. In other words, the over-fitted model remembers a huge number of examples from training dataset instead of learning to notice features of the training dataset. On other hand, an under-fitted model misses some features or patterns that exist in the training dataset. Under-fitting would occur, for example, when fitting a linear model to non-linear data. Both over-fitted and under-fitted model will tend to have poor predictive performance.</p>
<p>To determine if over-fitting has occurred, the model needs to be tested on “<strong>validation dataset</strong>”. Validation datasets is a subset from the given datasets that have targeted attributes values. This subset was not used to construct the model. Validation dataset is genially taken from the training datasets with certain percentage.</p>
<p>Over-fitting is quite common and this is not necessarily a problem. However, if the degree of over-fitting is large, the model may need to be reconstructed using a different set of attributes.</p>
<p>Apart from checking model’s over-fitting, Depends on the model being constructed, there a number of evaluation methods are available to perform the model validation such as <strong>Confusion Matrix</strong> for nominal output like class labels, <strong>AUC (Area Under Curve)</strong>, <strong>accuracy</strong> and other evaluation metrics are used for evaluate different models.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Correlation, in statistics, is a measurement of any statistical relationship two attributes. It can be any associations. It commonly refers to the degree to which a pair of attributes are linearly related.<a href="process-of-model-construction.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>The most commonly used measurement of correlation between two attributes is the “Pearson’s correlation coefficient”, commonly called simply “the correlation coefficient”.<a href="process-of-model-construction.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>In statistics, step-wise linear regression is a method of fitting regression models in which the selection of predictors is carried out by a procedure that in each step, one attribute is considered for addition to or subtraction from the set of selected attributes based on some pre-specified criterion.<a href="process-of-model-construction.html#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>Principal component analysis (PCA) is the process of computing the principal components and using only the first few principal components and ignoring the rest in a prediction or data dimension reduction. The principal components are often computed by eigendecomposition of the data covariance matrix or singular value decomposition of the data matrix.<a href="process-of-model-construction.html#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>In linear algebra, an Eigenvector or characteristic vector of a linear transformation is a nonzero vector that changes by a scalar factor when that linear transformation is applied to it.<a href="process-of-model-construction.html#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>Covariance matrix is a square matrix giving the covariance between each pair of elements of a given random vector.<a href="process-of-model-construction.html#fnref6" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="predictive-data-analysis-pda.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="classification-as-a-specific-prediction.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown/edit/master/inst/examples/%s",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "none"
},
"search": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
