% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Do A Data Science Project in 10 Days},
  pdfauthor={Gangmin Li},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage[bf,singlelinecheck=off]{caption}


\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{248,248,248}

\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

\renewenvironment{quote}{\begin{VF}}{\end{VF}}
\let\oldhref\href
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}

\ifxetex
  \usepackage{letltxmacro}
  \setlength{\XeTeXLinkMargin}{1pt}
  \LetLtxMacro\SavedIncludeGraphics\includegraphics
  \def\includegraphics#1#{% #1 catches optional stuff (star/opt. arg.)
    \IncludeGraphicsAux{#1}%
  }%
  \newcommand*{\IncludeGraphicsAux}[2]{%
    \XeTeXLinkBox{%
      \SavedIncludeGraphics#1{#2}%
    }%
  }%
\fi

\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\makeatletter
\@ifundefined{Shaded}{
}{\renewenvironment{Shaded}{\begin{kframe}}{\end{kframe}}}
\makeatother

\newenvironment{rmdblock}[1]
  {
  \begin{itemize}
  \renewcommand{\labelitemi}{
    \raisebox{-.7\height}[0pt][0pt]{
      {\setkeys{Gin}{width=3em,keepaspectratio}\includegraphics{images/#1}}
    }
  }
  \setlength{\fboxsep}{1em}
  \begin{kframe}
  \item
  }
  {
  \end{kframe}
  \end{itemize}
  }
\newenvironment{rmdaction}
  {\begin{rmdblock}{action}}
  {\end{rmdblock}}
\newenvironment{rmdinstruction}
  {\begin{rmdblock}{instruction}}
  {\end{rmdblock}}
\newenvironment{rmdinfo}
  {\begin{rmdblock}{info}}
  {\end{rmdblock}}
\newenvironment{rmdnote}
  {\begin{rmdblock}{note}}
  {\end{rmdblock}}
\newenvironment{rmdcaution}
  {\begin{rmdblock}{caution}}
  {\end{rmdblock}}
\newenvironment{rmdimportant}
  {\begin{rmdblock}{important}}
  {\end{rmdblock}}
\newenvironment{rmdtip}
  {\begin{rmdblock}{tip}}
  {\end{rmdblock}}
\newenvironment{rmdwarning}
  {\begin{rmdblock}{warning}}
  {\end{rmdblock}}

\usepackage{makeidx}
\makeindex

\urlstyle{tt}

\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\frontmatter
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Do A Data Science Project in 10 Days}
\author{Gangmin Li}
\date{2021-03-07}

\begin{document}
\maketitle

%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage
\thispagestyle{empty}
\begin{center}

\end{center}

\setlength{\abovedisplayskip}{-5pt}
\setlength{\abovedisplayshortskip}{-5pt}

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\hypertarget{section}{%
\chapter*{}\label{section}}
\addcontentsline{toc}{chapter}{}

\begin{center}\includegraphics[width=0.6\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/images/hotstove} \end{center}

\begin{verbatim}
"Dont't touch it, It's hot!",
...

You now know it is hot. don't you?
How?

Because, you'v touched it!
\end{verbatim}

\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}


Data science has gain popularity in recent years. Harvard Business Review called it the Sexiest Job of the 21st Century. I am not sure whether it is sexy or not but one thing is sure is that data science subjects are very popular among university students. This can be seen from the course selection. Data science-related courses like Big Data, Machine Learning, Statistics, and algorithms have huge student enrollment numbers.

Data science is a multidisciplinary field. It blends data mining, data analysis, statistics, algorithm development, machine learning, and advanced computing and software technology together in order to solve analytically complex problems. Its ultimate goal is to reveal insight into data and get the data value for the business. It is obvious that gaining the related knowledge is essential but my observation is that a lot of students shy away from doing any data science projects is because they are lacking hands-on experience in any full cycle of doing a data science project.

There is a Chinese saying that ``Practice makes perfect''. It is true but it is even more true that practices can gain the first hand of knowledge about practical issues and techniques to resolve the issues. Furthermore, it can build confidence in doing a data science project. That is what this book is intended to bring about.

\hypertarget{what-is-this-book}{%
\section*{What is This Book?}\label{what-is-this-book}}


This book is originated from my Data Science course. It was in the lab sessions, where students practice different steps in the workflow of Data Science projects. Students enjoyed the detailed practices using different methods, algorithms, and techniques to solve analytical problems. To my surprise, we only find out later, that the student failed to grasp the real meaning of doing data science, which is NOT to provide an absolutely perfect working solution to a problem, rather, an experimental interpretation of data at hand. In other words, a data science project is normally aimed to provide an explanation of what the data is telling you. Even you are making a prediction model, there will never be a perfect model that produces 100 percent prediction accuracy. You are not using data you have to solve problems that data has not to provide any solution to you!

I did a short tutorial for my students. The tutorial was emphasizing on the process and workflow of doing a data science project. That short tutorial was extremely successful and welcomed by all students, particularly the students who are not from Computer Science, Software Engineering, Statistics, Applied Mathematics, etc. rather, from Information Science and Management Science. I figured out the student's satisfaction comes from the practical skills and particularly the hands-on experience of doing a data science project rather than learning methods, algorithms, and parallel computation platforms without doing any.

So, I suppose this book is practical for students who have no background in computing and programming knowledge but interested in doing a Data Science project or moving to Data Science in the future.

In summary, this book is an introduction level book for novelty students who want to learn Data Science in a short period of time perhaps a few days or during their winter or summer holidays.

\hypertarget{structure-of-the-book}{%
\section*{Structure of the Book}\label{structure-of-the-book}}


This book intended to follow the process of doing a typical data science project. That is the six steps of the data analytical process:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Understand the problem
\item
  Understand the data
\item
  Data Preprocess
\item
  Data analysis
\item
  Validation
\item
  Report
\end{enumerate}

Each step, the tasks that need to be performed will be introduced and also practiced by an example project.

Before the example project to be kicked start, the tools and the platform are used in the example project will be introduced and practiced. The practice is basically mimicking the instructions on the book or the copy and paste code and run them in the environment.

\hypertarget{what-can-this-book-offer-you}{%
\section*{What Can This Book Offer You?}\label{what-can-this-book-offer-you}}


This book offers a short practical course. When you finish the course, you will:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Understand the data analysis procedure
\item
  Familiar with R language
\item
  Using RStudio to do your prototype of data project
\item
  Data preprocess - manipulate data and get it ready for further analysis
\item
  Manage basic data analytical methods like \textbf{Descriptive Data Analysis}, \textbf{Exploratory Data Analysis} and \textbf{Predictive Data Analysis}
\item
  Have basic skills of visualize data results
\item
  Basic interpretation of your analyzing results
\item
  Report and communicate your results
\item
  Enter the data science community
\end{enumerate}

\hypertarget{notes}{%
\section*{Notes}\label{notes}}


Our goal is not to teach you R, but to teach you the basic process of doing a Data Science project that many other programming languages like Java and Python can do. We use R in our lessons because:

\begin{itemize}
\tightlist
\item
  we have to use something for examples;
\item
  it's free, well-documented, and runs almost everywhere;
\item
  it has a large (and growing) user base among scientists; and
\item
  it has a large library of external packages available for performing diverse tasks.
\end{itemize}

But the two most important things are to use whatever language your colleagues are using, so you can share your work with them easily, and to use that language well. apparently. R is the most used language in Data Science.

\hypertarget{acknowledgements}{%
\section*{Acknowledgements}\label{acknowledgements}}


During the process of writing this book, I have gained tremendous inspiration from many materials including but not exhausted the following, for which I owe them great gratitude.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  A beginner's guide to Kaggle's Titanic problem
  Sumit Mukhija. (\url{https://towardsdatascience.com/a-beginners-guide-to-kaggle-s-titanic-problem-3193cb56f6ca})
\item
  Kaggle competition.
  \url{https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy}
\item
  Machine Learning for Dummies by John Mueller and Luca Massaron - Easy to understand for a beginner book, but detailed to actually learn the fundamentals of the topic
\item
  Data camp tutorials.
  \url{https://www.datacamp.com/community/}
\end{enumerate}

\hypertarget{intro}{%
\chapter{Introduction}\label{intro}}

\begin{center}\includegraphics[width=0.5\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/images/dataScientist} \end{center}

\begin{quote}
\begin{verbatim}
"What profession did Harvard call the Sexiest Job of the 21st Century?"

  That’s right. You guessed it,

  The data scientist.


                              -- "Data Scientist"
\end{verbatim}

The Sexiest Job of the 21st Century.
(\url{https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century}).
\end{quote}

Ah yes, the ever-mysterious data scientist. So what exactly is the data scientist's secret sauce, and what does this ``sexy'' person actually do at work every day? How they do it?

\hypertarget{what-is-data-science}{%
\section{What is Data Science?}\label{what-is-data-science}}

Data science is a multidisciplinary filed\footnote{123}. It blends data mining, data analysis, statistics, algorithm development, machine learning, and advanced computing and software technology together in order to solve analytically complex problems. Its ultimate goal is to reveal insight into data and get the data value for the business.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{Capture/whatisdatascience} 

}

\caption{Concept of Data Science}\label{fig:unnamed-chunk-2}
\end{figure}

\hypertarget{data-science-as-discovery-of-data-insight}{%
\subsection*{Data Science as Discovery of Data Insight}\label{data-science-as-discovery-of-data-insight}}


This aspect of data science is all about uncovering hidden patterns from data. Diving in at a granular level to mine and understand complex patterns, trends, and relations. It's about surfacing hidden insight that can help and enable companies to make smarter business decisions and take appropriate actions to gain competitive advantages in the market. For example:

\begin{itemize}
\tightlist
\item
  Amazon build recommendation system to provide users suggestion on the purchase based on the user's shopping history.
\item
  Netflix data mines movie viewing patterns to understand what drives user interest, and uses that to make decisions on which Netflix original series to produce.
\item
  Target identifies are major customer segments within its base and the unique shopping behaviors within those segments, which helps to guide messaging to different market audiences.
\item
  Proctor \& Gamble utilizes time series models to more clearly understand future demand, which helps plan for production levels more optimally.
  How do data scientists mine out insights? It starts with data exploration. When given a challenging question, data scientists become detectives. They investigate leads and try to understand patterns or characteristics within the data. This requires a big dose of analytical creativity.
\end{itemize}

How do data scientists mine data insights? there is a procedure to follow. It generally starts with data description it is called Described data analysis (DDA) to get first sight on the data sets available. DDS will help data scientists to grasp the quantity and quality of the data. so they can decide how to deal with the data. it then generally followed by data cleaning, manipulation, transform and attributes engineering, etc, together called preprocess. Data preprocessing is also generally combined with exploratory data analysis (EDA). When given a challenging question, data scientists normally become detectives. They investigate all the information available and follow any possible leads and try to understand patterns or characteristics within the data. This not only requires a huge amount of tools and techniques but also demands analytical creativity.

Then as needed, data scientists may apply quantitative techniques in order to get a level deeper -- e.g.~statistical methods, projections, inferential models, segmentation analysis, time series forecasting, synthetic control experiments, etc. The intent is to scientifically piece together a forensic view of what the data is really saying.

This data-driven insight is central to providing strategic guidance. In this sense, data scientists act as consultants, information providers help business stakeholders on how to act on findings.

\hypertarget{data-science-as-development-of-data-product}{%
\subsection*{Data Science as Development of Data Product}\label{data-science-as-development-of-data-product}}


A ``data product'' is a technical asset that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  utilizes data as input, and
\item
  processes that data to return algorithmically-generated results.
\end{enumerate}

A typical example is the users' scoring system. It takes users' profile or/and behavior data as input and with a complex scoring engine, it produces a credit score of the users for business decision making.
Another example of a data product is a recommendation engine, which ingests user data, and makes personalized recommendations based on that data.
Here are some examples of data products:

\begin{itemize}
\tightlist
\item
  Amazon's recommendation engines suggest items for you to buy, determined by their algorithms.
\item
  Netflix recommends movies to you. Spotify recommends music to you.
\item
  Gmail's spam filter is a data product -- an algorithm behind the scenes processes incoming mail and determines if a message is junk or not.
\item
  Computer vision used for self-driving cars is also a data product -- machine learning algorithms are able to recognize traffic lights, other cars on the road, pedestrians, etc.
\end{itemize}

This is different from the ``data insights'' section above, where the outcome to that is to perhaps provide advice to an executive to make a smarter business decision. In contrast, a data product is a technical functionality that encapsulates an algorithm and is designed to integrate directly into core applications. Respective examples of applications that incorporate data products behind the scenes: Amazon's homepage, Gmail's inbox, and autonomous driving software.

Data scientists play a central role in developing data products. This involves building out algorithms, as well as testing, refinement, and technical deployment into production systems. In this sense, data scientists serve as technical developers, building assets that can be leveraged at a wide scale.

\hypertarget{what-is-data-scientist}{%
\section{What is Data Scientist?}\label{what-is-data-scientist}}

Data scientists are a new breed of analytical data experts who has the technical skills to solve complex problems -- and the curiosity to explore what problems need to be solved. They are part mathematician, part computer scientist, and part business trend-spotter. They straddle in both the business and IT worlds with mathematical and programming weaponry.

\hypertarget{the-requisite-skill-set}{%
\subsection*{The Requisite Skill Set}\label{the-requisite-skill-set}}


A data scientist needs a blend of skills in three major areas:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Mathematics
\item
  Computing and Software Engineering
\item
  Business
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Capture/datascientist} 

}

\caption{Quality of Data Scientists}\label{fig:unnamed-chunk-3}
\end{figure}

\hypertarget{mathematics-narrator}{%
\subsubsection*{Mathematics Narrator}\label{mathematics-narrator}}


The heart of mining data insight and building data product is the ability to view the data through a quantitative lens. There are textures, dimensions, and correlations in data that can be expressed mathematically. Finding solutions utilizing data becomes a brain teaser of heuristics and quantitative techniques. Solutions to many business problems involve building analytic models grounded in the hard math, where being able to understand the underlying mechanics of those models is key to success in building them.

Also, a misconception is that data science all about \textbf{statistics}. While statistics are important, it is not the only type of math utilized. First, there are two branches of statistics -- classical statistics and Bayesian statistics. When most people refer to stats they are generally referring to classical statistics, but knowledge of both types is helpful. Furthermore, many inferential techniques and machine learning algorithms lean on the knowledge of \textbf{linear algebra}. For example, a popular method to discover hidden characteristics in a data set is SVD, which is grounded in matrix math and has much less to do with classical stats. Overall, it is helpful for data scientists to have breadth and depth in their knowledge of mathematics.

\hypertarget{computing-and-software-engineer-skills}{%
\subsubsection*{Computing and Software Engineer Skills}\label{computing-and-software-engineer-skills}}


Data is now collected, stored, and processed with a computer. With the increase of data quantity, as termed as, we are entering the big data era. The conventional way of processing data facing an unprecedented challenge. The personal computer maybe not adequate to handle big data. Distributed storage, cloud computing, and computer clusters become commonly-used platforms for data access and controls. Basic computing environment configuration and settings are common skills need to handle data.

The data processing tools and languages like R or Python, and a database querying language like SQL is the commonly used languages in data process and data analyzing. It is also important to have strong software engineering knowledge so it can be comfortable to handle a large amount of data logging, and to develop data-driven products.

Data scientists need to utilize new technology in order to wrangle enormous data sets and work with complex algorithms and code or prototype quick solutions, as well as interact and integrate with complex data systems. Core languages associated with data science include SQL, Python, R, and SAS. On the periphery are Java, Scala, Julia, and others. But it is not just knowing language fundamentals. A data scientist is a technical ninja, able to creatively navigate their way through technical challenges in order to make their code work.

Along these lines, a data scientist is a solid algorithmic thinker, having the ability to break down messy problems and recompose them in ways that are solvable. This is critical because data scientists operate within a lot of algorithmic complexity. They need to have a strong mental comprehension of high-dimensional data and tricky data control flows. Full clarity on how all the pieces come together to form a cohesive solution.

\hypertarget{strong-business-acumen}{%
\subsubsection*{Strong Business Acumen}\label{strong-business-acumen}}


It is important for a data scientist to be a tactical business consultant, an operation narrator, and a storyteller. Working so closely with data, data scientists are positioned to learn from data in ways no one else can. They can understand the language the data speak and listen to the story the data tells. That creates the responsibility to translate observations, discovery to shared knowledge, and contribute to strategy on how to solve core business problems. This means a core competency of data science is using data to cogently tell a story. No data present a cohesive narrative of problem and solution, using data insights as supporting pillars, that lead to guidance.

Having this business acumen is just as important as having acumen for technology and math and algorithms. There needs to be a clear alignment between data science projects and business goals. Ultimately, the value doesn't come from data, math, and tech itself. It comes from leveraging all of the above to build valuable capabilities and have a strong business influence.

\hypertarget{how-to-become-a-data-scientist}{%
\subsection*{How to Become a Data Scientist?}\label{how-to-become-a-data-scientist}}


Many people start to Position themselves for a career in data science. Not only for good job opportunities but also for the excitement of work in the technology field with freedom for experimentation and creativity. To get to this position you need solid foundations.

A conventional way of becoming a data scientist is Choosing a university that offers a data science degree. Or register yourself for courses in data science and analytics fields. If you cannot do these, the option left to you is to learn by yourself.

The knowledge and skills you should have are:

\begin{itemize}
\tightlist
\item
  \textbf{Statistics and machine learning}. A good understanding of statistics is vital as a data scientist. You should be familiar with statistical tests, distributions, maximum likelihood estimators, etc. Statistics knowledge will also help you understand when different techniques are (or aren't) a valid approach. Machine learning (ML) is a good weapon when you involve a big data project. Algorithms are the core of machine learning, although many implementations with R or Python libraries do exist and convenient to use, It is still needed a thorough understanding of how the algorithms works and when it is appropriate to use different ones.
\item
  \textbf{Coding languages such as R or Python}. It is essential, a data scientist is competent with a number of computing and data querying languages like R, Python, and SQL.
\item
  \textbf{Databases such as MySQL and Postgres}. Data is generally stored in a Database. it is important to have the necessary skills for data access and control from a DBMS system. The most commonly used DBMS systems are MySql (\url{https://www.mysql.com/}) and Postgres (\url{https://www.postgresql.org/}) in addition to ACCESS and EXCEL.
\item
  \textbf{Visualization and reporting technologies}. Visualizing and communicating data is incredibly important, especially with companies that are making data-driven decisions, or companies where data scientists are viewed as people who help others make data-driven decisions. When it comes to communicating, this means describing your findings, or the way techniques work to audiences, both technical and non-technical. Visualization can be immensely helpful. Therefore familiar with data visualization tools like matplotlib, ggplot, or d3.js. Tableau and dashboarding have become popular data visualization tools. It is important to not just be familiar with the tools necessary to visualize data, but also the principles behind visually encoding data and communicating information.
\item
  \textbf{Big data platforms like Hadoop}.(\url{https://hadoop.apache.org/}) and \textbf{Spark} (\url{https://spark.apache.org/}). Although a lot of Data Science projects can be tried, or at least prototyped on PC or workstations, it is the reality that most large data analyzing is done on advanced computing platforms like distributed infrastructure or computer clusters. this advanced platform mostly deploy Hadoop ecosystems.
\end{itemize}

If you don't want to learn these skills on your own, take an online course or enroll in a Bootcamp. Like what you do now. It not only provides you the opportunity to gain knowledge quickly but also provides you the chance of networking with other people who has a similar situation as you do. Connect with other people can lead you into an online community. They all will help you gain fine grain and insider knowledge of solving problems.

\hypertarget{process}{%
\section{Process of Doing Data Science}\label{process}}

Understand what data science is about is just the start of becoming a data scientist. Once the goal is set. The next task is to select the correct path and work hard to reach your destination. The path is important which can be shorter or longer, or direct and smooth, or curvy and bumpy. It is vital to follow a short and smooth path. This path is the data science project process. Figure \ref{fig:process} is the 6 steps process, which is inspired by the CRISP (Cross Industry Standard Process for Data Mining) \citep{Chapman2000}, \citep{Shearer2000}, and KDD (knowledge discovery in databases) process \citep{Li2018}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{Capture/process} 

}

\caption{Process of doing Data Science}\label{fig:process}
\end{figure}

\hypertarget{step1}{%
\subsection*{Step 1: Understand the Problem - Define Objectives}\label{step1}}


Any data analysis must begin with business issues. With business issues, a number of questions should be asked. These questions have to be the right questions and measurable, clear, and concise. Define analysis question is regarded as define a Data Requirements engineering and to get a data project specification. It starts from a business issue and asking relevant questions, only after you fully understand the problem and the issues you may be able to turn a practical problem into analytical questions.

For example, start with a business issue: A contractor is experiencing rising costs and is no longer able to submit competitive contract proposals. One of many questions to solve this business problem might include: Can the company reduce its staff without compromising quality? Or, can the company find an alternative supplier on the production chain?

Once you have questions, you can start to think about the data required for analysis. The data required for analysis is based on questions. The data necessary as inputs to the analysis can then be identified (e.g., Staff skills and performance). Specific variables regarding a staff member (e.g., Age and Income) may be specified and obtained. The data type can be defined as numerical or categorical.

After you defined your analytical questions. It is important to set a clear evaluation of your project to measurement how the success of your project.

This generally breaks down into two sub-steps: A) Decide what to measure, and B) Decide how to measure it.

\textbf{A) What To Measure?}

Using the contractor example, consider what kind of data you'd need to answer your key question. In this case, you would need to know the number and cost of current staff and the percentage of time they spend on necessary business functions. This is what is called in business as KPI - Key performance indicators. In answering this question, you likely need to answer many sub-questions (e.g., Are staff currently under-utilized? If so, what process improvements would help?). Finally, in your decision on what to measure, be sure to include any reasonable objections any stakeholders might have (e.g., If staff is reduced, how would the company respond to surges in demand?).

\textbf{B) How To Measure? }

Thinking about how you measure the success of your data science project, the deep end is to measure some key performance indicators. They are the data you have chosen to use in the previous step. So measure your data is just as important, especially before the data collection phase, because your measuring process either backs up or discredits your project later on. Key questions to ask for this step include:

\begin{itemize}
\tightlist
\item
  What is your time frame? (e.g., annual versus quarterly costs)
\item
  What is your unit of measure? (e.g., USD versus Euro)
\item
  What factors should be included? (e.g., just annual salary versus annual salary plus the cost of staff benefits)
\end{itemize}

\hypertarget{step2}{%
\subsection*{Step 2: Understand Data - Knowing your Raw Materials}\label{step2}}


The second step is to understand data. It includes \textbf{Data collection} and \textbf{Data Validation}. With the problem understood and analytical questions defined and your validation criteria and measurements set, It is time to collect data.

\hypertarget{data-collection}{%
\subsubsection*{Data Collection}\label{data-collection}}


Before collect data, the data source has to be determined based on the relevance. A variety of data sources may be assessed and accessed to get relevant data. These data sources may include existing databases, or organization's file system, or a third-party service, or even open web sources. They could provide redundant, or complementary, sometimes conflicting data. it has to be cautious to select the right data source from the very beginning. sometimes you need to gather data via observation or interviews, then develop an interview template ahead of time to ensure consistency. it is a good idea to Keep your collected data organized in a log with collection dates and add any source notes as you go (including any data normalization performed). This practice validates your data and any conclusions down the road.

Data Collection is the actual process of gathering data on targeted variables identified as data requirements. The emphasis is on ensuring correct and accurate data collection, which means correct procedure was taken and appropriate measurements were adopted. the maximum efforts were spent to ensure the data quality. Remember that data Collection provides both a baseline to measure and a target to improve for a successful data science project.

\hypertarget{data-validation}{%
\subsubsection*{Data Validation}\label{data-validation}}


Data validation is the process to Assess data quality. It is to ensure the collected data have reached quality requirements identified in step 1, that is, they are useful and correct. The usefulness is the most important aspect. Regardless of how accurate your data collections can be, if it is not useful, anything that follows is just a waste. It is hard to define the usefulness. depends on the problem at hand and the requirements for the find al delivery. The usefulness can vary from a strong correlation between the raw data and the expected outcomes, to direct prediction power from the raw data to the consequence variables. Generally, data validation can include:

\begin{itemize}
\tightlist
\item
  Data type validation
\item
  Range and constraint validation
\item
  Code and cross-reference validation
\item
  Structured validation
\item
  Consistency validation
\item
  Relevancy validation
\end{itemize}

\hypertarget{preprocess}{%
\subsection*{Step 3: Data Preprocessing - Get your Data Ready}\label{preprocess}}


Data preprocessing is a step that takes data processing methods and techniques to transforms raw data into a formatted and understandable form and ready for analyzing. Real-world data is often incomplete, inconsistent, and is likely to contain many errors. Data preprocessing is a proven method of resolving such issues. Tasks of data preprocessing may include:

\begin{itemize}
\item
  \textbf{Data cleaning}. The process of detecting and correcting (or removing) corrupt or inaccurate records from a recordset, table, or database. It normally includes identifying incomplete, incorrect, inaccurate, or irrelevant data and then replacing, modifying, or deleting the dirty or coarse data. After cleansing, a data set should be consistent with other data sets.
\item
  \textbf{Data editing}. The process involves changing and adjusting of collected data. The purpose is to ensure the quality of the collected data. Data editing should be done by fully understand the data collected and the data requirement specification. Editing data without them can be disastrous.
\item
  \textbf{Data reduction}. The process and methods used to reduce data quantity to fit for analyzing. Raw data set collected or selected for analysis can be huge, then it could drastically slow down the analysis process. Reducing the size of the data set without jeopardizing the data analysis results is often desired. It includes records' number reduction and data attributes reduction. Methods used to reduce data records size include \textbf{Sampling} and \textbf{Modelings} (e.g., regression or log-linear models or histograms, clusters, etc). Methods used for attributes reduction include \textbf{Feature selection} and \textbf{Dimension reduction}. Feature selection means the removal of irrelevant and redundant features. such an operation should not lose information the data set has. Data analysis algorithms work better if the dimensionality, which is the number of attributes in a data object is low. Data compression techniques (e.g., wavelet transforms and principal components analysis), attribute subset selection (e.g., removing irrelevant attributes as discussed in the previous paragraph), and attribute construction (e.g., where a small set of more useful attributes is derived from the large numbers of attributes in the original data set) are useful techniques.
\item
  \textbf{Data transformation} sometimes referred to as \textbf{data munging} or \textbf{data wrangling}. It is the process of transforming and mapping data from one data form into another format with the intent of making it more appropriate and valuable for downstream analytics. It is often that data analysis method requires data to be analyzed have a certain format or possesses certain attributes. For example, classification algorithms require that the data be in the form of categorical (nominal) attributes; algorithms that find association patterns require that the data be in the form of binary attributes. Thus, it is often necessary to transform a continuous attribute into a categorical attribute, which is called \textbf{Discretization}, and both continuous and discrete attributes may need to be transformed into one or more binary attributes, which is called \textbf{Banalization}. Other methods include \textbf{Scaling} and \textbf{normalization}. Scaling changes the bounds of the data and can be useful, for example, when you are working with data in different units. Normalization scales data sets to a smaller range such as {[}0.0, 1.0{]}.
\item
  \textbf{Data re-engineering}
  Re-engineering data is necessary when raw data come from many different data sources and in a different format. Data re-engineering similar to data transformation can be done at both a record level and an attribute level. Record level re-engineering is also called data \textbf{Integration}, which integrates a variety of data into one file or place and in one format for analysis. for predictive analysis with a model, data re-engineering is also including split a given data set into two subsets called ``Training'' and ``Test'' Set.
\end{itemize}

\hypertarget{analyse}{%
\subsection*{Step 4: Data Analyese - Building Models}\label{analyse}}


After your collected data being preprocessed and suitable for analysis. Now you can drill down and attempt to answer your question from \protect\hyperlink{step1}{Step 1} with the actions called Data Analyzing. It is the core activity in the data science project process by writing, executing, and refining computer programs that utilize some analytical methods and algorithms to obtain insights from data sets. There are three broad categories of data analytical methods: \textbf{Descriptive data analysis (DDA)}, \textbf{Exploratory data analysis (EDA)}, and \textbf{Predictive data analysis (PDA)}. DDA and EDA use
quantitative and statistical methods on data sets and data attribute measurements and their value distributions while DDA focus on numeric summary and EDA emphasis on graphical (plot) mean, PDA involves model building and machine learning.
In a data science project, data analyzing is generally starting from Descriptive analysis and goes further with Exploratory analysis, and finally end up with a tested and optimized prediction model for predicting. However, it does not necessarily mean that the methods used in a data analysis project have to stick in this order. As matter of fact, the most project involves a recursive process and a mixture of all the three methods. For example, An exploratory analysis can be utilized in a feature engineering step to prepare a predictor of a prediction model, which is used to predict some missing values of a particular attribute in a given dataset for an accurate description of the attribute value distribution.

\hypertarget{descriptive-data-analysis}{%
\subsubsection*{\texorpdfstring{\textbf{Descriptive data analysis}}{Descriptive data analysis}}\label{descriptive-data-analysis}}


It is the simplest type of analysis. It describes and summarizes data sets quantitatively. Descriptive analysis\index{Descriptive analysis} generally starts with univariate analysis, meaning describing a single variable (can also be called an attribute, column, or field) of the data. The appropriate depends on the level of measurement. For nominal variables, a frequency table and a listing of the modes are sufficient. For ordinal variables, the median can be calculated as a measure of central tendency and the range (and variations of it) as a measure of dispersion. For interval level variables, the arithmetic mean (average) and standard deviation are added to the toolbox and, for ratio level variables, we could add the geometric mean and harmonic mean as measures of central tendency and the coefficient of variation as a measure of dispersion. However, there are many other possible statistics that cover areas such as location (``middle'' of the data), dispersion (range or spread of data), and shape of the distribution. Moving up to two variables, descriptive analysis can involve measures of association such as computing a correlation coefficient or covariance. Descriptive analysis's goal is to describe the key features of the sample numerically. It should shed light on the key numbers that summarize distributions within the data, may describe or show the relationships among variables with metrics that describe association, or by tables that cross-tabulation counts. Descriptive analysis is typically the first step on the data analysis ladder, which only tries to get a sense of the data.

\hypertarget{explorative-data-analysis}{%
\subsubsection*{\texorpdfstring{\textbf{Explorative data analysis}}{Explorative data analysis}}\label{explorative-data-analysis}}


Descriptive analysis\index{Descriptive analysis} is very important. However, numerical summaries can only get you so far. One problem is that it can only convert a large number of values down to a few summary numbers. Unsurprisingly, different samples with different distributions, shapes, and properties can result in the same summary statistics. This will cause problems. When you are looking at a simple single summary statistic, the mean of a single variable, there can be a lot of possible ``solutions'' or samples. The typical example is Anscombe's quartet \citep{Anscombe1973}, it comprises four datasets that have nearly identical simple statistical properties, yet appear very different when graphed. Most kinds of statistical calculations rest on assumptions about the behavior of the data. Those assumptions may be false, and then the calculations may be misleading. We ought always to try and check whether the assumptions are reasonably correct, and if they are wrong we ought to be able to perceive in what ways they are wrong. Graphs are very valuable for these purposes.

EDA allows us to challenge or confirm our assumptions about the data. It is a good tool to be used in \protect\hyperlink{preprocess}{data prerpocess}. We often have pretty good expectations of what unclean data might look like, such as outliers, missing data, and other anomalies, perhaps more so than our expectations of what clean data might look like. The more we understood data, we could develop our intuition of what factors and possible relations at are play. EDA, with its broad suite of ways to view the data points and relationships, provides us a range of lenses with which to study the story that data is telling us. That in turn, helps us to come up with new hypotheses of what might be happening. Further, if we understood which variables we can control, which levers we have to work within a system to drive the metrics such as business revenue or customer conversion in the desired direction. EDA can also highlight gaps in our knowledge and which experiments might make sense to run to fill in those gaps.

The basic tools of EDA are plots, graphs, and summary statistics. Generally speaking, it's a method of systematically going through the data, plotting distributions of all variables (using box plots), plotting time series of data, transforming variables, looking at all pairwise relationships between variables using scatterplot matrices, and generating summary statistics for all of them or identifying outliers.

\hypertarget{predictive}{%
\subsubsection*{\texorpdfstring{\textbf{Predictive data analysis }}{Predictive data analysis }}\label{predictive}}


Predictive analysis\index{Predictive analysis} builds upon \textbf{inferential analysis}, which is to learn about relationships among variables from an existing training data set and develop a model that can predict values of attributes for new, incomplete, or future data points. The inferential analysis is a type of analysis that from a dataset sample in hand infer some information, which might be parameters, distributions, or relationships about the broader population from which the sample came. We typically infer metrics about the population from a sample because data collection is too expensive, impractical, or even impossible to obtain all data. The typical process of inferential analysis includes testing hypotheses and deriving estimates.
There is a whole slew of approaches and tools in predictive analysis. \textbf{Regression} is the broadest family of tools. Within that, however, are a number of variants (lasso, ridge, robust, etc.) to deal with different characteristics of the data. Of particular interest and power is \textbf{Logistic Regression} which can be used to predict classes. For instance, spam/not spam used to be mostly predicted with a \textbf{Naïve Bayes predictor} but nowadays logistic regression is more common. Other techniques and that come under the term \textbf{Machine Learning} include neural networks, tree-based approaches such as classification and regression trees, random forests, support vector machines (SVM), and k-nearest neighbors.

\hypertarget{step-5-results-interpretation-and-evaluation}{%
\subsection*{Step 5: Results Interpretation and Evaluation}\label{step-5-results-interpretation-and-evaluation}}


After analyzing your data and get some answers to your original questions, it is possible that you need to conduct further research and more analysis. Let us suppose that you are happy with the analysis results you have. It is finally time to interpret your results. As you interpret your analysis, keep in mind that you cannot ever prove a hypothesis true: rather, you can only fail to reject the hypothesis. Meaning that no matter how much data you collect, chance could always interfere with your results. Interpreting the results of the analysis, you should think of how close the results address the original problems by asking yourself these key questions:

\begin{itemize}
\tightlist
\item
  Does the data answer your original question? How?
\item
  Does the data help you defend against any objections? How?
\item
  Are there any limitations on your conclusions, any angles you haven't considered?
\end{itemize}

If your interpretation of the data holds up under all of these questions and considerations, then you likely have come to a productive conclusion. However, there could be a chance that you may find you might need to revise your original question or collect more data and you may need to roll the ball from the starting line. Again. Either way, this initial analysis of trends, correlations, variations, and outliers is not completely wasted. They help you focus your data analysis on better answering your question and any objections others might have. That is the next step report and communication.

\hypertarget{step-6-data-report-and-communication}{%
\subsection*{Step 6: Data Report and Communication}\label{step-6-data-report-and-communication}}


Whereas the analysis phase involves programming and runs programs on different computer platforms, the reporting involves narrative the results of the analysis, thinking about how close the results address the original problems, and communicating about the outputs of analyses with interesting parties in many cases in visual formats.
During this step, data analysis tools and software are helpful but visual tools are intuitive and worth a lot of words. Visio, Tableau (\url{https://www.tableau.com/}), Minitab (\url{https://www.minitab.com/}), and Stata (\url{https://www.stata.com/}) are all good software packages for advanced statistical data analysis. There are also plenty of open source data visualization tools available.

It is important to note that the above 6 steps process is not a linear process. Any discovery of useful relationships and valuable patterns is enabled by a set of iterative activities. Iteration can occur in a single step or in a few steps at any point in the process.

\hypertarget{tools-used-in-doing-a-data-science-project}{%
\section{Tools used in Doing a Data Science Project}\label{tools-used-in-doing-a-data-science-project}}

Data Scientists use traditional statistical methodologies that form the core backbone of Machine Learning algorithms. They also use Deep Learning algorithms to generate robust predictions. Data Scientists use the following tools and programming languages:

\hypertarget{r}{%
\subsection*{R}\label{r}}


R (\url{https://www.r-project.org/}) is a scripting language that is specifically tailored for statistical computing and data. It is widely used for data analysis, statistical modeling, time-series forecasting, clustering, etc. R is mostly used for statistical operations. It also possesses the features of an object-oriented programming language. R is an interpreter-based language and is widely popular across multiple industries particularly for doing data science projects.

\hypertarget{python}{%
\subsection*{Python}\label{python}}


Like R, Python (\url{https://www.python.org/}) is an interpreter-based high-level programming language. Python is a versatile language. It is mostly used for Data Science and Software Development. Python has gained popularity due to its ease of use and code readability. As a result, Python is widely used for Data Analysis, Natural Language Processing, and Computer Vision. Python comes with various graphical and statistical packages like Matplotlib, Numpy, SciPy, and more advanced packages for Deep Learning such as TensorFlow, PyTorch, Keras, etc. For the purpose of data mining, wrangling, visualizations, and developing predictive models, we utilize Python. This makes Python a very flexible programming language.

\hypertarget{sql}{%
\subsection*{SQL}\label{sql}}


SQL stands for Structured Query Language. Data Scientists use SQL for managing and querying data stored in databases. Being able to extract data from databases is the first step towards analyzing the data. Relational Databases are a collection of data organized in tables. We use SQL for extracting, managing, and manipulating the data. For example, A Data Scientist working in the banking industry uses SQL for extracting information from customers. While Relational Databases use SQL, \textbf{NoSQL} is a popular choice for non-relational or distributed databases. Recently NoSQL has been gaining popularity due to its flexible scalability, dynamic design, and open-source nature. MongoDB, Redis, and Cassandra are some of the popular NoSQL databases.

\hypertarget{hadoop}{%
\subsection*{Hadoop}\label{hadoop}}


Big data is another trending term that deals with the management and storage of a huge amount of data. Data is either structured or unstructured. A Data Scientist must have a familiarity with complex data and must-know tools that regulate the storage of massive datasets. One such tool is Hadoop (\url{https://hadoop.apache.org/}). While being open-source software, Hadoop utilizes a distributed storage system using a model called \textbf{MapReduce}. There are several other packages in Hadoop that together formed an Apache ecosystem, such as Apache Pig, Hive, HBase, etc. Due to its ability to process colossal data quickly, its scalable architecture, and low-cost deployment, Hadoop has grown to become the most popular software for Big Data.

\hypertarget{tableau}{%
\subsection*{Tableau}\label{tableau}}


Tableau (\url{https://www.tableau.com/}) is a Data Visualization software specializing in graphical analysis of data. It allows its users to create interactive visualizations and dashboards. This makes Tableau an ideal choice for showing various trends and insights of the data in the form of interactable charts such as Treemaps, Histograms, Box plots, etc. An important feature of Tableau is its ability to connect with spreadsheets, relational databases, and cloud platforms. This allows Tableau to process data directly, making it easier for the users.

\hypertarget{weka}{%
\subsection*{Weka}\label{weka}}


For Data Scientists looking forward to getting familiar with Machine Learning in action, Weka (\url{https://www.cs.waikato.ac.nz/ml/weka/}) is, can be, an ideal option. Weka is generally used for Data Mining but also consists of various tools required for Machine Learning operations. It is completely open-source software that uses GUI Interface making it easier for users to interact with, without requiring any line of code.

\hypertarget{applications-of-data-science}{%
\section{Applications of Data Science}\label{applications-of-data-science}}

Data Science has created a strong foothold in several industries such as Government and education, Healthcare and medicine, banking and commerce, manufacturing and transportation, etc. It has immense applications and has a variety of uses. Some of the applications of Data Science are listed below:

\hypertarget{data-science-in-healthcare}{%
\subsection*{Data Science in Healthcare}\label{data-science-in-healthcare}}


Data Science has been playing a pivotal role in the Healthcare Industry. With the help of classification algorithms, doctors are able to detect cancer and tumors at an early stage using Image Recognition software. Genetic Industries use Data Science for analyzing and classifying patterns of genomic sequences. Various virtual assistants are also helping patients to resolve their physical and mental ailments.

\hypertarget{data-science-in-e-commerce}{%
\subsection*{Data Science in E-commerce}\label{data-science-in-e-commerce}}


Amazon uses a recommendation system that recommends users various products based on their historical purchases. Data Scientists have developed recommendation systems to predict user preferences using Machine Learning.

\hypertarget{data-science-in-manufacturing}{%
\subsection*{Data Science in Manufacturing}\label{data-science-in-manufacturing}}


Industrial robots have made taken over mundane and repetitive roles required in the manufacturing unit. These industrial robots are autonomous in nature and use Data Science technologies such as Reinforcement Learning and Image Recognition.

\hypertarget{data-science-as-conversational-agents}{%
\subsection*{Data Science as Conversational Agents}\label{data-science-as-conversational-agents}}


Amazon's Alexa and Siri by Apple use Speech Recognition to understand users. Data Scientists develop this speech recognition system, that converts human speech into textual data. Also, it uses various Machine Learning algorithms to classify user queries and provide an appropriate response.

\hypertarget{data-science-in-transport}{%
\subsection*{Data Science in Transport}\label{data-science-in-transport}}


Self Driving Cars use autonomous agents that utilize Reinforcement Learning and Detection algorithms. Self-Driving Cars are no longer fiction due to advancements in Data Science.

\hypertarget{data-science-related-terms}{%
\section{Data Science Related Terms}\label{data-science-related-terms}}

There is a slew of terms closely related to data science that we hope to add some clarity around.

\hypertarget{dataanalyst-and-data-scientist}{%
\subsection*{DataAnalyst and Data Scientist}\label{dataanalyst-and-data-scientist}}


Analytics has risen quickly in popular business lingo over the past several years; the term is used loosely, but generally meant to describe critical thinking that is quantitative in nature. Technically, analytics is the ``science of analysis'' --- put another way, the practice of analyzing information to make decisions.

Is ``analytics'' the same thing as data science? Depends on context. Sometimes it is synonymous with the definition of data science that we have described, and sometimes it represents something else. A data scientist using raw data to build a predictive algorithm falls into the scope of analytics. At the same time, a non-technical business user interpreting pre-built dashboard reports (e.g.~GA) is also in the realm of analytics but does not cross into the skill set needed in data science. Analytics has come to have a fairly broad meaning. At the end of the day, as long as you understand beyond the buzzword level, the exact semantics don't matter much.

``Analyst'' is somewhat of an ambiguous job title that can represent many different types of roles (data analyst, marketing analyst, operations analyst, financial analyst, etc). What does this mean in comparison to a data scientist?

Data Scientist: Specialty role with abilities in math, technology, and business acumen. Data scientists work at the raw database level to derive insights and build data products.
Analyst: This can mean a lot of things. The common thread is that analysts look at data to try to gain insights. Analysts may interact with data at both the database level or the summarized report level.

Thus, ``analyst'' and ``data scientist'' are not exactly synonymous, but also not mutually exclusive. Here is our interpretation of how these job titles map to skills and scope of responsibilities:

\hypertarget{machine-learning-and-data-science}{%
\subsection*{Machine Learning and Data Science}\label{machine-learning-and-data-science}}


Machine learning is a term closely associated with data science. It refers to a broad class of methods that revolve around data modeling to (1) algorithmically make predictions, and (2) algorithmically decipher patterns in data.

Machine learning for making predictions --- The core concept is to use tagged data to train predictive models. Tagged data means observations where ground truth is already known. Training models means automatically characterizing tagged data in ways to predict tags for unknown data points. E.g. a credit card fraud detection model can be trained using a historical record of tagged fraud purchases. The resultant model estimates the likelihood that any new purchase is fraudulent. Common methods for training models range from basic regressions to complex neural nets. All follow the same paradigm known as supervised learning.

Machine learning for pattern discovery --- Another modeling paradigm is known as unsupervised learning tries to surface underlying patterns and associations in data when no existing ground truth is known (i.e.~no observations are tagged). Within this broad category of methods, the most commonly used are clustering techniques, which algorithmically detect what are the natural groupings that exist in a data set. For example, clustering can be used to programmatically learn the natural customer segments in a company's user base. Other unsupervised methods for mining underlying characteristics include principal component analysis, hidden Markov models, topic models, and more.

Not all machine learning methods fit neatly into the above two categories. For example, collaborative filtering is a type of recommendations algorithm with elements related to both supervised and unsupervised learning. Contextual bandits are a twist on supervised learning where predictions get adaptively modified on-the-fly using live feedback.

This wide-ranging breadth of machine learning techniques comprises an important part of the data science toolbox. It is up to the data scientist to figure out which tool to use in different circumstances (as well as how to use the tool correctly) in order to solve analytically open-ended problems.

\hypertarget{data-mining-and-data-science}{%
\subsection*{Data Mining and Data Science}\label{data-mining-and-data-science}}


Raw data can be unstructured and messy, with information coming from disparate data sources, mismatched or missing records, and a slew of other tricky issues. Data munging is a term to describe the data wrangling to bring together data into cohesive views, as well as the janitorial work of cleaning up data so that it is polished and ready for downstream usage. This requires good pattern-recognition sense and clever hacking skills to merge and transform masses of database-level information. If not properly done, dirty data can obfuscate the `truth' hidden in the data set and completely mislead results. Thus, any data scientist must be skillful and nimble at data munging in order to have accurate, usable data before applying more sophisticated analytical tactics.

\hypertarget{summary}{%
\section*{Summary}\label{summary}}


While Data Science is a vast subject, being an aggregate of several technologies and disciplines, it is possible to acquire these skills with the right approach. In the end, Data Science is a very robust field that best fits people who have a knack for experimentation and problem-solving. With a large number of applications, Data Science has become the most versatile career.

\hypertarget{exercises}{%
\section*{Exercises}\label{exercises}}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Explain Data Science in your own term. What is the relation between data science and Data mining, Data Science with Data Analytics?
\item
  What a Data Scientist do? What skills a data SCientist should have?
\item
  How do you interpret the saying that ``Data Scientist is a detective. An investigation into datasets may not result in a plausible conclusion''? How do you explain the value of doing a data science project if your efforts resulted in an unwelcome result?
\end{enumerate}

\hypertarget{tools}{%
\chapter{Get Your Tools Ready}\label{tools}}

\begin{center}\includegraphics[width=0.6\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/images/tools} \end{center}

\begin{quote}
\begin{verbatim}
      工欲善其事，必先利其器
\end{verbatim}

An artisan must first sharpen his tools if he is to do
his work well.

\begin{verbatim}
                       -- 孔子《论语》
                         Confucius <Analects>
\end{verbatim}
\end{quote}

Since this book is ``Do Data Science \ldots{}''. It means learn data science by doing. First of all, we need to get our weaponry or tools ready.

We already knew that there is a list of tools used by data scientists. Apart from personal preference, the most used tool is R. This book will use R as the tool to do a complete data science project. However this is not an R language book\citep{Black2021}, it will not teach you about R language and how to use it. It will simply demonstrate a data science project completion step by step, which is completed with R language\citep{Wickham2021}.

By doing, I mean that you can simply mimic what I have done and follow along by typing or copy paste my code into your working space, observe the effects and the results of each line of code execution. Thinking of why I have to do this and what results can I expect along the line of the data science project's process. monitoring the issue raised and the methods used to resolve the issues. It is a hope that at some points you can have your own thoughts, perhaps your own code, methods, and experiments. Once that is achieved. the goals are reached.

\hypertarget{brief-introductiuon-about-r-and-rstudio}{%
\section{Brief introductiuon about R and RStudio}\label{brief-introductiuon-about-r-and-rstudio}}

R is one of the most widely used programming languages for statistical modelling \citep{Dalpiaz2021}. It has become the lingua franca of Data Science. Being open-source, R enjoys community support of avid developers who work on releasing new packages, updating R, and making it a steady and fast programming package for Data Science\citep{Wickham2021}.

\hypertarget{features-of-r-programming}{%
\subsection*{Features of R Programming}\label{features-of-r-programming}}


R Programming has the following features:

\begin{itemize}
\tightlist
\item
  R is a comprehensive programming language that provides support for procedural programming involving functions as well as object-oriented programming with generic functions.
\item
  R can be extended easily. There are over 10,000 packages in the repository of R programming. With these packages, one can make use of extended functions to facilitate easier programming.
\item
  Being an interpreter-based language, R produces a machine-independent code that is portable in nature. Furthermore, it facilitates easy debugging of the code.
\item
  R supports complex operations with vectors, arrays, data frames as well as other data objects that have varying sizes.
\item
  R can be easily integrated with many other technologies and frameworks like Hadoop and Spark. It can also integrate with other programming languages like C, C++, Python, Java, FORTRAN, and JavaScript.
\item
  R provides robust facilities for data handling and storage.
  As discussed in the above section, R has extensive community support that provides technical assistance, seminars, and several boot camps to get you started with R.
\item
  R is cross-platform compatible. R packages can be installed and used on any OS in any software environment without any changes.
\end{itemize}

\hypertarget{r-scripts}{%
\subsection*{R Scripts}\label{r-scripts}}


R is the primary statistical programming language for performing modelling and graphical tasks. so it can run in the command line as an interpreting language. However, With its extensive support for performing increasingly complex computations such as manipulations on matrix and dataframe\index{dataframe}, R is now mostly running in the script for a variety of tasks that involve complex datasets with complex operations.

There is plenty of editing tools that perform interactions with the native R console. With any one of them, you can edit and run R script. You can also simply import extra packages and use the provided functions to achieve results with minimal number lines of code. There are several editors and IDEs that facilitate GUI features for authoring and executing R scripts. Some of the useful editors that support the R programming language are RGui (R Graphical User Interface) and RStudio\index{RStudio}, an integrated R script development environment.

This book will NOT teach you how to code in R. Learning R and to code in R language is not so hard. It just requires a lot of trials and time-spending. You can always go online and searching on Google, Baidu, or StackOverflow (\url{https://stackoverflow.com/}). There are also plenty of examples and codes. The chances are if you're trying to figure out how to do something in R, other people have tried as well, so rather than banging your head against the wall, look online. There are also some books available to help you out on this front as well. I suggest looking at other people's code and run it to see the results. R manual (\url{https://cran.r-project.org/manuals.html}) is always handy and is available.

If you want to learn R systematically, there are many sources online providing good tutorials. You can try to learn more R language from R tutorials. Tutorialspoint (\url{http://www.tutorialspoint.com/r/index.htm}), Codecademy (\url{https://www.codecademy.com/}). If you prefer an online interactive environment to learn R, this free R tutorial by DataCamp (\url{https://www.datacamp.com/courses/free-introduction-to-r}) is a great way to get started.

\hypertarget{r-graphical-user-interface-rgui}{%
\subsection*{R Graphical User Interface (RGui)}\label{r-graphical-user-interface-rgui}}


RGui is a standard GUI\index{GUI} (Graphic User Interface) platform that comes with an R release. By default, it provides two windows: R Console (on the left) and R Editor (on the right). See: Figure \ref{fig:rgui}

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{Capture/RGui} 

}

\caption{Screen capture of RGui: where Console i son the left and Editor is on the right}\label{fig:rgui}
\end{figure}

\textbf{R Console} is an essential part of the RGui. In this window, we input various instructions, commands, and scripts for different operations. The results of any operation or instruction execution are displayed at the console window including warning and error messages. Console window utilizes several other useful tools embedded to facilitate and ease various operations. The console window appears whenever you access the RGui.

\textbf{R Editor} is a simple built-in text editor. Where you can create a new R script, edit, test, and debug the script and save it into a file. To lunch R Editor, in the main panel of RGui, go to the ``\texttt{File}'' menu and select the ``\texttt{New\ Script}'' option. This will lunch R Editor and allow you to create a new script in R. R Editor has a function of ``\texttt{Run\ line\ or\ selection}''. It means you can debug your code by line or selection. It is a very convenient tool for debugging.

\hypertarget{rstudio}{%
\subsection*{RStudio}\label{rstudio}}


RStudio \index{RStudio}(\url{https://rstudio.com/products/rstudio/}) is an integrated and comprehensive Integrated Development Environment (IDE)\index{IDE} for R. It facilitates extensive code editing, debugging, and development. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging, and workspace management. Figure \ref{fig:rstudio} is a screen shot of the RStudio.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{Capture/RStudio} 

}

\caption{Screen capture of RStudio with integrated R code developemtn environment}\label{fig:rstudio}
\end{figure}

Here are some distinctive features provided by the RStudio:

\begin{itemize}
\tightlist
\item
  \textbf{An IDE that was built just for R}. With Syntax highlighting, code completion, and smart indentation. It can execute R code directly from the source editor. it can quickly jump to function definitions
\item
  \textbf{Bring your workflow together}. Integrated R help and documentation with easily manage multiple working directories using projects and Workspace browser and data viewer
\item
  \textbf{Powerful authoring \& Debugging}. Interactive debugger to diagnose and fix errors quickly and extensive package development tools can authoring with Sweave\index{Sweave} and R Markdown\index{Markdown}
\end{itemize}

RStudio is available in open source and commercial editions and runs on the desktop (Windows, Mac, and Linux) or in a browser connected to RStudio Server or RStudio Server Pro.

We will use RStudio for the whole book. The detailed RStudio IDE is explained in (\url{https://rstudio.com/products/rstudio/}).

\hypertarget{downlaod-and-install-r-and-rstudio}{%
\section{Downlaod and Install R and RStudio}\label{downlaod-and-install-r-and-rstudio}}

It is simple to download and install both R and RStudio.

\hypertarget{r-download-and-installation}{%
\subsection*{R Download and Installation}\label{r-download-and-installation}}


To download R, please either directly from here (\url{http://cran.us.r-project.org/bin/windows/base}) or your preferred CRAN mirror (\url{https://cran.r-project.org/mirrors.html}). If you have questions about R like how to download and install the software, or what the license terms are, please read the answers to frequently asked questions (\url{http://cran.r-project.org/faqs.html}).

Once you have chosen a site and click the download, you will will see Figure \ref{fig:rd},

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{Capture/R} 

}

\caption{Screen capture of R dowanload papge from CRAN}\label{fig:rd}
\end{figure}

Pick up your platform and download the latest version (\texttt{4.0.2}), follow instructions to install it (assume you choose Windows). In Windows, double click downloaded executable file, you will see this (as shown in Figure \ref{fig:rinstall}),

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Capture/Rinstall} 

}

\caption{Screen capture of R install in Windows}\label{fig:rinstall}
\end{figure}

Click `\texttt{Run}', and answer the security message with `\texttt{Yes}'. Choose your language (\texttt{English}),

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{Capture/Rinstlang} 

}

\caption{Screen capture of R install in Windows}\label{fig:rinstlang}
\end{figure}

Click `\texttt{Ok}'. And follow the instructions on screen by click `\texttt{Next}', until the whole process is complete, click `\texttt{Finish}'. You now have a version (choose 64bit) R installed. The installation program will create the directory ``\texttt{C:}\textbackslash{}\texttt{Program\ Files}\textbackslash{}\texttt{R}\textbackslash{}\texttt{\textless{}your\ version\textgreater{}}'', according to the version of R that you have installed.
The actual R program will be ``\texttt{C:}\textbackslash{}\texttt{Program\ Files}\textbackslash{}\texttt{R}\textbackslash`\texttt{\textbackslash{}}bin\texttt{\textbackslash{}\textbackslash{}}Rgui.exe\texttt{".\ A\ windows\ "}shortcut`'' should have been created on the desktop and/or in the start menu. You can launch it any time you want by click on it.

\hypertarget{rstudio-download-and-installation}{%
\subsection*{RStudio Download and Installation}\label{rstudio-download-and-installation}}


To download RStudio, to go Rstudio products Web page (\url{https://rstudio.com/products/rstudio/}). Choose ``\texttt{RStudio\ Desktop}'' between ``\texttt{RStudio\ Serve}'' and ``\texttt{RStudio\ Desktop}''. See, Figure \ref{fig:rstudio1},

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{Capture/Rstudio1} 

}

\caption{Screen capture of RStudio selection}\label{fig:rstudio1}
\end{figure}

After choosing the desktop version it will take you to a page (\url{http://www.RStudio.org/download/desktop}). Where several possible downloads are displayed, a different one for each operating system. However, the webpage was designed that it can automatically recommend the download that is most appropriate for your computer. Click on the appropriate link, and the RStudio installer file will start downloading.

Once it is finished downloading, open the installer file and answer all on-screen questions or click ``\texttt{next}'' in the usual way to install RStudio.

After it is finished installing, you can launch RStudio from the windows \texttt{"start"} button.

As we explained in the previous section, Rstudio is a comprehensive and integrated development environment. It can be overwhelming for people who contact it for the first time. Next section we will introduce its interface in great detail.

\hypertarget{familiar-with-rstudio-interface}{%
\subsection*{Familiar with RStudio interface}\label{familiar-with-rstudio-interface}}


Open RStudio and you will see a rather sophisticated interface. Apart from the usual top-level manual like ``\texttt{File\ Edit\ ...}'', there are four panes. I labelled 1 to 4 on the Figure \ref{fig:RStudio}), these panels are called \textbf{pane}\index{pane} in RStudio.

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/Capture/Rstudio} 

}

\caption{RStudio interface}\label{fig:RStudio}
\end{figure}

RStudio does allow you to move panes around in the options menu, and also select tabs you want. Before you can miss around and lost yourself on the way. Let us stick to this default layout ion the moment. It is what you see when you first lunch it, so we'll act as though it's standard.

\hypertarget{pane-1-script-pane---view-files-and-data}{%
\subsubsection*{Pane 1: Script Pane - View Files and Data}\label{pane-1-script-pane---view-files-and-data}}


\textbf{Script pane} appears by default in the top left of the RStudio interface. it is where you enter your script and code, you can edit and debug your code or your script.

This pane also displays files When you click on a data file in the \texttt{Workspace\ pane} (top right, number 2 on the above image), or open a file from the \texttt{Files} pane (right bottom, number 3 on the above image), the results will appear in Pane 1. Each file opens in its own tab, and you can navigate between tabs by clicking on them (or using keyboard shortcuts).

\hypertarget{pane-2-workspace-pane---environment-and-history}{%
\subsubsection*{Pane 2: WorkSpace Pane - Environment and History}\label{pane-2-workspace-pane---environment-and-history}}


\textbf{Workspace pane} appears by default in the top right of the RStudio interface. It has four tabs by default: \textbf{\texttt{Environment}}, \textbf{\texttt{History}}, \textbf{\texttt{Connection}} and \textbf{\texttt{Tutorial}}. among these 4, the \textbf{\texttt{Environment}} is the default and it is selected. It shows a list of all the objects you have loaded into your workspace. For example, all datasets you have loaded will appear here, along with any other objects you have created (special text formats, vectors, etc.). see this image (Figure \ref{fig:RStudioEven}):

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/Capture/environment} 

}

\caption{RStudio Environemnt Tab in WorkSpace Pane}\label{fig:RStudioEven}
\end{figure}

If you click on the \textbf{\texttt{History}} tab, you will see the complete history of code you have typed, over all sessions, as in this image (Figure \ref{fig:RStudiohist}):

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/Capture/history} 

}

\caption{RStudio Histroy Tab in WorkSpace pane}\label{fig:RStudiohist}
\end{figure}

The \texttt{history} is searchable, so you can use the search box at the upper right of the pane to search through your code history. If you find a line of code you want to re-run, just select it and click the ``\texttt{To\ Console}'' button as shown below (\ref{fig:RStoconsole}):

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/Capture/toconsole} 

}

\caption{RStudio "To Console" button under History Tab in WorkSpace Pane}\label{fig:RStoconsole}
\end{figure}

You can also select any number of lines of scripts (by click with holding the shift key) and click the ``\texttt{To\ Source}'' button, they will inset into the source, See Figure \ref{fig:tosource},

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/Capture/tosource} 

}

\caption{RStudio "To Source" button under History Tab in WorkSpace Pane}\label{fig:tosource}
\end{figure}

\hypertarget{pane-3-console-pane}{%
\subsubsection*{Pane 3: Console Pane}\label{pane-3-console-pane}}


By default \textbf{\texttt{Console\ pane}} appears at the bottom left. \texttt{Console\ pane} is the most important pane -- the Console! This is where you enter your commands to be executed or your R code to do everything in the curriculum. The rest of the document will be largely concerned with working in the \texttt{Console}, with occasional references to other panes.
By default it also has 4 tabs: \textbf{\texttt{Console}}, \textbf{\texttt{Terminal}}, \textbf{\texttt{R\ markdown}} and \textbf{\texttt{Jobs}}. Apart from the console, Other three, as their name suggested, are the interface between you and other systems. The terminal is the interface between you and the operating system, where you can have direct interaction with OS, in our case it is Windows. R markdown\index{markdown}\footnote{R Markdown is an authoring framework for data science. Using a single R Markdown file, data Scientists can save, execute R code and generate high-quality reports that can be shared with other people.} is the interface between you and the markdown compiler, if authoring a markdown file, every time you compile (\texttt{knitr}\index{Knitr}\footnote{knitr is an engine for dynamic report generation with R. It is a package in the programming language R that enables integration of R code into LaTeX, LyX, HTML, Markdown, AsciiDoc, and reStructuredText documents.}) the code, the system will report status in that window. Jobs is the interface between you and your job execution system. it is generally running on a remote server.

Basically, the \texttt{Console\ pane} is the communication interface between you and the systems. The information that appears here is generally important if any problem occurs.

\hypertarget{pane-4-multifunction-pane}{%
\subsubsection*{Pane 4: Multifunction Pane}\label{pane-4-multifunction-pane}}


The \textbf{\texttt{multifunction\ pane}} appears by default at the bottom right. It has many tabs. By default it opens the \textbf{\texttt{Files}} tab. My version it has \textbf{\texttt{File}}, \textbf{\texttt{Plots}}, \textbf{\texttt{Package}}, \textbf{\texttt{Help}} and \textbf{\texttt{Viewer}} tabs.

\hypertarget{files-tab}{%
\paragraph{\texorpdfstring{\textbf{Files tab}}{Files tab}}\label{files-tab}}
\addcontentsline{toc}{paragraph}{\textbf{Files tab}}

This tab works like your file explorer. It shows you all the files you have in your RStudio account (your document in windows). The buttons underneath the tab allow you to do operations on the files like create a new folder, delete files. rename files and many more functions. which you normally do on the file system.

\hypertarget{plots}{%
\paragraph{\texorpdfstring{\textbf{Plots}}{Plots}}\label{plots}}
\addcontentsline{toc}{paragraph}{\textbf{Plots}}

When you run code in the \texttt{Console\ pane} that creates a plot, the plots tab will be automatically selected and the result of the plot generated will be displayed. See Figure \ref{fig:plot})

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/Capture/plot} 

}

\caption{RStudio **Plot** Tab under History Tab in Multifunction Pane}\label{fig:plot}
\end{figure}

Any time you want to view plots, you can select this tab manually, but it will be selected when you run plot code. Notice the arrow buttons at the top left of the pane; these allow you to scroll through all the plots you have created in a session.

\hypertarget{packages}{%
\paragraph{\texorpdfstring{\textbf{Packages}}{Packages}}\label{packages}}
\addcontentsline{toc}{paragraph}{\textbf{Packages}}

This tab allows you to see the list of all the packages (add-ons to the R code) you have access to, and which are loaded in already. You can also check packages in your system (installed) and the version of them.

\hypertarget{help}{%
\paragraph{\texorpdfstring{\textbf{Help}}{Help}}\label{help}}
\addcontentsline{toc}{paragraph}{\textbf{Help}}

This tab will be automatically selected whenever you run help code in the Console, by type in console \texttt{?\ function} or type in script \texttt{help(function)}. It is very useful for beginners to get quick references on any function or command you are not sure of. here is an example of asking for help with \texttt{plot} function:

\begin{verbatim}
help(plot)
\end{verbatim}

You can access it at any time by clicking on the tab ``\texttt{Help}'' to see what the ``\texttt{Help}'' tab can offer. See Figure \ref{fig:help},

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/Capture/help} 

}

\caption{RStudio **Help** Tab under in Multifunction Pane}\label{fig:help}
\end{figure}

If you want to use the ``\texttt{Help}'' without using the \texttt{help} command, you can also use the search bar at the upper right of the tab to search within the R documentation.

\hypertarget{viewer}{%
\paragraph{\texorpdfstring{\textbf{Viewer}}{Viewer}}\label{viewer}}
\addcontentsline{toc}{paragraph}{\textbf{Viewer}}

The \textbf{\texttt{Viewer}} tab in the multifunction pane is designed for view or display R markdown\index{R Markdown} results. If you are authoring an R notebook\index{R notebook}\footnote{R Notebooks are an implementation of Literate Programming that allows for direct interaction with R while producing a reproducible document with publication-quality output.} or any Markdown file, your \textbf{\texttt{Knit}}\index{knit} results can be viewed by select ``\texttt{Preview} in \texttt{Viewer} Pane''. Once this selection is made, you will see the notebook or your Markdown\index{Markdown} document will be displayed in the \texttt{Viewer} window and you will notice that the \texttt{Viewer} tab is automatically selected and the viewer window is also maximized. See Figure \ref{fig:SSview}

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/Capture/RSview} 

}

\caption{RStudio **Viewer** Tab under in Multifunction Pane}\label{fig:SSview}
\end{figure}

RStudio allows a user to close or minimize certain panes or windows and focused on one or two panes. It also allows users to customize tabs in each pane. Check top-level menu ``\texttt{View}'' for details. Figure \ref{fig:RSview} illustrates the function.

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/Capture/RSPaneview} 

}

\caption{RStudio Pane change under the top level memu View}\label{fig:RSview}
\end{figure}

RStudio provides large number of help functions, which can be explored under \textbf{\texttt{Help}} top level menu. One help is the \texttt{keyBoard\ shortcuts} help. I find it is very useful. Figure \ref{fig:rssc} shows the \texttt{shortcuts}.

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/Capture/RSsc} 

}

\caption{RStudio KeyBoard Shortcuts}\label{fig:rssc}
\end{figure}

RStudio is a complicated, comprehensive IED for R, R Markdown, R Notebook and many other R language developments, and other languages like Java, Python developments too. Its powerful functions can only be revealed and made useful after you have used it for a while. The more use it, the more likely you will find it is so easy to use. I will leave this for you to explore.

\hypertarget{bootsup-your-rstudio}{%
\section{Bootsup your RStudio}\label{bootsup-your-rstudio}}

Once you boot up your RStudio, you are ready to kick off your R coding. However, the first thing you may want to do is to set up your working directory. This will change the default location for all file input and output that you will do in the current session.

RStudio makes this easy, simply click ``\texttt{Session\ -\textgreater{}\ Set\ Working\ Directory\ -\textgreater{}\ Choose\ Directory\ldots{}}''. See figure Figure \ref{fig:setwk} below,

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/Capture/setwk} 

}

\caption{Set workling Directory by Session}\label{fig:setwk}
\end{figure}

Then you need to navigate to where you want your project to be sit. For example, in my case I used \texttt{"D:/Teach2020/short\ course/Data\ analysis\ -\ prediction\ with\ Rstudio/IntroToDataScience-master"}, it is silly to be so long, you can certainly set up for a shorter one. Anyway, the point is choose your won directory and remember it. If you tried it, you should notice that once you have chosen a directory, A command appeared in the Console pane and this is the command R executes when you set your working directory from the session menu. To achieve the same result you normally would have typed this manually in the console. See Figure \ref{fig:setwkc} below,

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/Capture/setwkc} 

}

\caption{Set a working directory with R command in Console}\label{fig:setwkc}
\end{figure}

Type command or instructions on command line at \texttt{Console} is what the general data scientists do when they try to analyse some data or prove some ideas. You can complete this tutorial at the command line in \texttt{Console\ pane}. I would suggest you, instead, creating a script to save all your hard work. This way you can easily reproduce the results or make changes without retyping everything.

To do so, you need to create a new file by click the ``\texttt{File\ -\textgreater{}New\ file}'', and select ``\texttt{R\ Script}''. See Figure \ref{fig:newrfile},

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/Capture/RStudionew} 

}

\caption{Creat a new file in RStudio}\label{fig:newrfile}
\end{figure}

If you do so, you should notice that a new tab appeared on the script pane with the name of ``\texttt{Untitled1}'' and the script editor is now opened for you with the cursor flashes on line number 1. See Figure \ref{fig:newrcfile},

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/Capture/RStudionewfile} 

}

\caption{Creat a new file in RStudio and ready to enter code}\label{fig:newrcfile}
\end{figure}

Now inside the script editor, you can type your code! Let us try this first, type

\begin{verbatim}
# This is my first R code
\end{verbatim}

and hit ``\texttt{Return}'', see next image (Figure \ref{fig:newrcode}),

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/Capture/new} 

}

\caption{New R Script file with one line}\label{fig:newrcode}
\end{figure}

Notice that the tab ``\texttt{Untitled1}'' has changed to red colour and with a "*" as superscript. It means that the current file has been changed and not saved.

Go ahead and copy the \texttt{setwd} command from the console and paste it into your script.

Now save the script to your working directory, give it a name my first R, or any name you prefer\footnote{The entire code is in the Appendix and is available online to download too}.

Now you have your first R code!

\hypertarget{instructions}{%
\section{Instructions}\label{instructions}}

This book is intended to work in two ways: one way is to be used as a manual, you can follow along to accomplish an entire data science project; Another way is to be used as a company to my online video recordings. If you can get the video that is great. But if you cannot, it is also fine, The only drawback is you have to read the whole contents line by line.

I will use the following stickers to indicate the text is an explanation or an instruction or actions need you to do. So you know what you have to read word by word and what you can skip.

\hypertarget{code}{%
\subsection*{Code}\label{code}}


Code appears with code sticker. Like this,

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Load raw data}
\NormalTok{train <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"train.csv"}\NormalTok{, }\DataTypeTok{header =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{test <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"test.csv"}\NormalTok{, }\DataTypeTok{header =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

They are the ones you have to read word by word and type (copy-paste) into your script and run them. It is also a good idea to record the results or plot (graph results) into a file. So you can always come back to check them.

\hypertarget{tips}{%
\subsection*{Tips}\label{tips}}


Tips, like this one,

\begin{rmdtip}
Within the console, you can use the up and down arrows to find recent commands, and then hitting tab will auto-complete commands and object names if possible.
\end{rmdtip}

are general advice. You can skip them if you already know. They can save your time but not affect your learning.

\hypertarget{actions}{%
\subsection*{Actions}\label{actions}}


Any actions, by default, are assumed you will act upon. It appears in action sticker,

\begin{rmdaction}
Change data type
Go back to look into Kaggle to explain pclass: proxy for social class: richer or poor. It should be factor, it does not make sense to stay in int, we are not add or calculate with them

\texttt{data.combined\$pclass\ \textless{}-\ as.factor(data.combined\$pclass)}
\end{rmdaction}

Particularly, they are in sequential order. If you did not take previous actions you cannot do the current. It is possible you have processed some datasets and it is used later on. So you must carry out actions one by one, and not jump to the later ones without accomplishing the earlier ones.

\hypertarget{exercise}{%
\subsection*{Exercise}\label{exercise}}


Exercises at the end of each chapter, are provided for you to periodically explore alternatives of a solution or to enhance some key techniques. It is always good if you can do the exercises.

The default protocol is that I have some codes written (at the appendix) and you will download them and open in your RStudio. Then you need to run (or copy and past) line by line into your Rstudio. You can understand their functions and the reason to function like that. After you understand them you can change them or write some new code. While you are doing that, you simply comment out my code rather than delete them just in case you need to come back to look at them again. Once you can write your own code, it shows you have learned.

Before you go, let's try it,

\begin{rmdaction}
Open a new project called ``MyDataSciece'',
Set up working directory as ``\textasciitilde/MyDataScienceWithR'',
Create a first R program called \texttt{DSPR1},
\texttt{setwk(\textasciitilde{}/MyDataScienceWithR)}
\end{rmdaction}

Okay. Save your file and move to the next chapter.

\hypertarget{exercises-1}{%
\section*{Exercises}\label{exercises-1}}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Learn R basics from R tutorials.
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Tutorialspoint (\url{http://www.tutorialspoint.com/r/index.htm}),
\item
  codecademy (\url{https://www.codecademy.com/}).
\item
  DataCamp (\url{https://www.datacamp.com/courses/free-introduction-to-r})
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Write simple R code with RStudio IDE.
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Try create a new R script and save it in a file
\item
  Open it from your file system and edit it
\item
  Run it line by line
\item
  Run it in one go
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Explore RStudio help functions.
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Try to type ``?plot'' in Console
\item
  Try run help(plot) in editor
\item
  Explore plot from RStudio help
\item
  Search ``R plot'' from Google or Bing
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  Explore project and working directory from RStudio.
\item
  Create a R project called ``MyDataScienceProject''.
\end{enumerate}

\hypertarget{prob}{%
\chapter{Understand Problem}\label{prob}}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/images/Titanic} 

}

\caption{The sink of Titanic. Credits: Canoe1967/wikipeida.org}\label{fig:unnamed-chunk-1}
\end{figure}

Most of Data Science projects initiated by business organizations and having specific objectives. The list of the objective can be endless ranging from finding out problems, abstract patterns, and predict the future. It is essential to have fully understood problems and well-defined objectives for a successful data science project. In this book, I will choose the famous ``Titanic problem''.\\
The sinking of the RMS Titanic occurred on the night of 14 April 1912 in the North Atlantic Ocean, four days into the ship's maiden voyage from Southampton, UK to New York City, USA. The largest passenger liner in service at the time, Titanic had an estimated 2,224 people on board when she struck an iceberg at around 23:40 (ship's time) on Sunday, 14 April 1912. Her sinking two hours and forty minutes later at 02:20 (05:18 GMT) on Monday, 15 April resulted in the deaths of more than 1,500 people, which made it one of the deadliest peacetime maritime disasters in history.

Later, in 1997 American film director James Cameron turned this disastrous and tragic event into an epic romance film. The film star Leonardo DiCaprio and Kate Winslet's outstanding performance in the film makes it a best-selling movie in the year 1997.

Perhaps people are touched not only by the love story but also by the humanity norms in the life and death situation that is famous - \textbf{``Lady and children first''}. Now it is considered ``the motto of the sea''.

\hypertarget{Competion}{%
\section{Kaggle Competion}\label{Competion}}

Kaggle (\url{https://www.kaggle.com/}), a subsidiary of Google LLC, is the world's largest data science community with powerful tools and resources to help you achieve your data science goals. Kaggle was founded in 2010 with the idea that data scientists need a place to come together and collaborate on projects, learning new techniques, and share each other's experiences. This has transformed into a network with more than 1,000,000 registered users and has created a safe place for data science learning, sharing, and competition.

Using the human competitive spirit, Kaggle created a platform for organizations to host competitions that have fuelled new methodology and techniques in data science, and given organizations new insights from the data they provided.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/images/Kagglecomp} 

}

\caption{Kaggle Competition web site}\label{fig:unnamed-chunk-2}
\end{figure}

Generally, each competition has a host, and each host has to prepare and provide data. When providing data, the host has the opportunity to give additional information such as a description, evaluation method, timeline, and prize for winning. Although this may not be an ideal real-world data problem, which data scientists may face in the business. But it provides a good starting point for learners. In the real world, you may need to start by understanding the business and find data sources by yourself. Although the competition host has provided data. You cannot assume the data provided are clean data and ready for analysis. Cleaning and preprocess data are part of the competition. Therefore, any solution can be tested to see how good a participant is with the whole process of a data science project.

\hypertarget{titianic-at-kaggel}{%
\section{Titianic at Kaggel}\label{titianic-at-kaggel}}

Titanic perhaps is the oldest and most participated competition on the Kaggle competition site. Even Kaggle used it as a sample project to show how people can participate in a competition and submit their results.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/images/Titaniccompetition} 

}

\caption{Kaggle Competition on Titanic}\label{fig:unnamed-chunk-3}
\end{figure}

We take Titanic as an example through this tutorial because of the following reasons:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The story is well known and easy to understand and communicates any actions and the cause of the actions in the analysis process.
\item
  The competition has the largest participants, so any issues are most likely have been studied already. So explore the discussion and other sources that can help to solve any problem you may have.
\item
  It is well studied, so there are plenty of alternative training materials available for your reference.
\item
  Lastly, the problem itself is an interesting one that has the characteristic of only has a better solution and no best solution. So people are still working in it and uses the latest technologies.
\end{enumerate}

\hypertarget{the-titanic-problem}{%
\section{The Titanic Problem}\label{the-titanic-problem}}

The objective of the Titanic problem defined on the Kaggle website as stated in the following:

"The sinking of the Titanic is one of the most infamous shipwrecks in history.

On April 15, 1912, during her maiden voyage, the widely considered ``unsinkable'' RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren't enough lifeboats for everyone on board, resulting in the death of 1502 out of 2224 passengers and crew.

While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.

In this challenge, we ask you to build a \textbf{predictive model}\index{predictive model} that answers the question: ``what sorts of people were more likely to survive?'' using passenger data (i.e.~name, age, gender, socio-economic class, etc.)."

\hypertarget{the-challenge}{%
\subsection*{The Challenge}\label{the-challenge}}


The competition is simple: we want you to \textbf{use the Titanic passenger data} (name, age, price of the ticket, etc.) to try to \textbf{predict who will survive and who will die}.

The requirement is to predict passengers' ** survival**. Like many other real data science problems, \protect\hyperlink{predictive}{Prediction} is to build a model which takes input data and produces an output. A prediction model is a mathematical formula that takes input from historical facts reflecting past events and produces an output that to make predictions about future or otherwise unknown events. A simple way to understand a model is to think a model in the following three ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The relationship between input and output can be expressed by some kind of a math formula. It is generally called a definable model, the math formula can be as simple as a function of Polynomial expression or as complected as a regression model, or other statistical models.
\item
  Some models can not be explicitly expressed with math formulas, instead, they are expressed in rules. those are rule-based models.
\item
  Other models can not be expressed in a math formula nor in rules. The solution is to build a neural networks\index {neural networks} to do prediction. A Neural network can be regarded as a ``black box'', which takes input and produce output, the internal connections are transparent to users. Machine learning is more focused on models rooted in Neural networks.
\end{enumerate}

Any model fundamentally expresses relationships between inputs and outputs. So as part of understanding the problem, We could interpret that the Kaggle Titanic challenge is to find creditable relationships between input data and output data (which survive or not). Once the relationship is found, we can express using either a math formula, a set of rules, or a Neural Network model.

\hypertarget{the-data}{%
\subsection*{The Data}\label{the-data}}


Kaggle competition usually provides competition data. There is a ``Data'' tab on any competition site. Click on the Data tab at the top of the competition page, you will find the raw data provided and most of the time there is a brief explanation of the data attributes\footnote{We have used Data Science terminology here. Data represent objects in the natural world. Object properties are represented by attributes. That is a data record has a number of attributes representing a natural object with a number of properties. records are also called observations or samples in statistics, the property is also called variables, parameters or dimensions} too.

There are three files in the Titanic Challenge:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  train.csv,
\item
  test.csv, and
\item
  gender\_submission.csv.
\end{enumerate}

The training set is supposedly used to build your models. The training set provides the outcome (also known as the ``ground truth'') for each passenger. Your model will be based on attributes like passengers' gender and class. You can also use feature engineering to create new features.

The test set should be used to see how well your model performs on unseen data. For the test set, there is no ground truth for each passenger is provided. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.

The data sets have also include gender\_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.

\hypertarget{the-submission}{%
\subsection*{The Submission}\label{the-submission}}


Submission at the Titanic competition is equivalent to the requirements on the final report of any data science project. that is one of the questions you need to understand at the beginning of the project.

Titanic competition requires the results need be submitted in the file. The file structure is demonstrated in the ``gender\_submission.csv''. It is also provided as an example that shows how you should structure your results, which means predictions.

The example submission in ``Gender\_submission'' predicts that all female passengers survived, and all male passengers died. It is clearly biased. Your hypotheses regarding survival will probably be different, which will lead to a different submission file. Properly it is a good idea now to rename the ``Gender\_submission.csv'' file into ``My\_submission.csv'' now. So you know that you have to submit ``my\_submission.csv'' as the final report of your project and the submission indicates the completion of your project.

\begin{rmdaction}
Do it yourself:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Download data file from Kaggel web site.(\url{https://www.kaggle.com/c/titanic/data})
\item
  Unzip it into your working directory (eg. ``./data/'').
\item
  Rename ``Gender\_submission.csv'' file into ``My\_submission.csv''.
\end{enumerate}
\end{rmdaction}

Make sure your submission should have:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  ``PassengerId'' column containing the IDs of each passenger from test.csv.
\item
  ``Survived'' column (that you will create!) with a ``1'' for the rows where you think the passenger survived, and a ``0'' where you predict that the passenger died.
\end{enumerate}

\hypertarget{summary-1}{%
\section*{Summary}\label{summary-1}}


The purpose of this book is to provide a hand on practical exercise in doing a data science project. Clearly, we cannot cover the complete available methods, models, and algorithms for a data science project. The most important thing is to understand the process of doing a data science project. The first step, as indicated by the 6-step process in section \ref{process}, is ``understand the problem''.

We have chosen to use the Titanic problem to demonstrate the whole data analytical process. However, a real-world problem is far more complicated than this well-defined problem. Most business organizations may not know the exact problem (that is part of the reason why they want to do data analysis or business analysis) or they know the problem (in general) but the problem can not be expressed explicitly.

I have met a situation that a business organization that has created a data center and collected all their business operational data. The boss asked to analyze these data and find:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Is there are problems?
\item
  If yes, how to overcome these problems?
\item
  If not, how to improve the business operations?
\end{enumerate}

You see, here the problem is how to define the problem? how to convert the business problem into a data science problem.

For example, the first problem in the above list needs to know what is the normal or expected performance? How to evaluate the performance? In terms of turnover or profit? In what time scale? It could be short of profit at the moment but it not causes alarm because of the recent investment for developing a new market. In a long run, it will have a great ROI (Return on Investment). The second problem demands to identify the cause of the problem and the third to identify the KIP (Key Performance Indicators). they are both to identify the relationships between predictor and dependent variables. But they can be completely different sets.

Understand problem is actually more complicated in the real world. Until you have completely understood it and turned it into a list of analytical problems you can move to the next step.

With the Titanic problem, combining the story and the requirements on the Kaggle website, I would consider these:

\begin{itemize}
\tightlist
\item
  On April 14 and 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. The overall survival rate is 32\%.
\item
  One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.
\item
  Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.
\item
  The story tells us that when they were getting on board the lifeboats, they applied a policy of ``women and children first'' and also ``the ship crew is the last''.
\item
  Sometimes the family was boarding the lifeboat together and some of the family members were swimming together too.
\end{itemize}

Those thoughts form some kinds of assumptions in mind. They will guide more detailed data explorations later.

\hypertarget{exercises-2}{%
\section*{Exercises}\label{exercises-2}}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Go to Kaggle website, explore the challenge and data provided for the Titantic problem.
\item
  Based on what you know about the Titanic story, who you think can survive? can you describe the people you believe can survive in terms of age, sex, cabin, title, social status?
\item
  Generate 3 assumptions of the Titanic problem, thinking of how could you possibly prove or disprove them.
\item
  Looking into data given by the Titanic competition, what is your impression of the data?
\end{enumerate}

\hypertarget{understand-data}{%
\chapter{Understand Data}\label{understand-data}}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/images/buildingmaterial} 

}

\end{figure}

\begin{quote}
Understand your building raw materials can help you
choose correct tools and make the most use of them to
construct your ideal buildings.

\begin{verbatim}
                                    -- Gangmin Li
\end{verbatim}
\end{quote}

\textbf{Understand data}\index {understand data} is a foundation of solving analytical problems. The two major purposes of understand data are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Access data quantity and quality
\item
  Set up objectives for \textbf{Data preprocess}
\end{enumerate}

In practice the initial data assessments can be done together or separately. The purpose of them is to setup objectives for \emph{Data preprocess} to accomplish. The methods used to understand data are usually both \textbf{Descriptive analysis}\index {Descriptive analysis} and \textbf{Exploratory analysis}\index {Exploratory analysis}. We will use Titanic problem as an example to understand its data.

\hypertarget{load-data}{%
\section{Load Data}\label{load-data}}

Here are a simple plan for understand the Titanic data:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Get Titanic data load into RStudio
\item
  Assess Data quantity (number of files, size of each file in number of records, number of attributes in each record)
\item
  Attributes types assessment
\item
  Attributes value assessment (numbers and summary, description).
\end{enumerate}

Now, get your RStudio ready.

If you have not done the Exercises 2.5, which asked you to create a new R project named ``MyDataScienceProject''. You can do it now.

Open your RStudio, Click ``\texttt{File-\textgreater{}\ New\ project-\textgreater{}New\ Directory\ -\textgreater{}\ choose\ New\ R\ Project}'', then, enter ``\texttt{MyDataScienceProject}'' in the Directory name box and select your directory. Click ``\texttt{Create\ Project}'' at the right bottom as shown in Figure \ref{fig:newproject}

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/Capture/NewProject} 

}

\caption{Create a new project in RStudio }\label{fig:newproject}
\end{figure}

Load file ``\texttt{TitanicDataAnalysis1\_UnderstandData.R}'' (from online repo or Appendix) into RStudio and create a new R file and name it ``\texttt{My\_TitanicDataAnalysis1\_UnderstandData.R}''.

The protocol is you copy sample code indicated from this book (or from the file you loaded) ``chunk'' by ``chunk'' into your R file and run them. Okay, let us start,

In your RStudio (WorkSpace), copy lines from ``TitanicDataAnalysis1\_UnderstandData.R'' into your file ``MyTitanicDataScience1'', They are the same code in this book. Alternatively, you can copy from here. For example, copy the following ``chunk'' (two lines of code) into your file.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Load raw data}
\NormalTok{train <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"./data/train.csv"}\NormalTok{, }\DataTypeTok{header =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{test <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"./data/test.csv"}\NormalTok{, }\DataTypeTok{header =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You may see this in your Console,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{>}\DataTypeTok{ train <- read.csv("train.csv", header = TRUE)}
\DataTypeTok{Error in file(file, "rt") : cannot open the connection}
\DataTypeTok{In addition: Warning message:}
\DataTypeTok{In file(file, "rt") :}
\DataTypeTok{  cannot open file 'train.csv': No such file or directory}
\DataTypeTok{> test <- read.csv("test.csv", header = TRUE)}
\DataTypeTok{Error in file(file, "rt") : cannot open the connection}
\DataTypeTok{In addition: Warning message:}
\DataTypeTok{In file(file, "rt") :}
\DataTypeTok{  cannot open file 'test.csv': No such file or directory}
\DataTypeTok{> }
\end{Highlighting}
\end{Shaded}

Don't panic. let us look into it. It is important to learn how to diagnose and correct problems as well as write and run code.

The first thing you need to learn is using RStudio help.

Now type \textbf{\texttt{?\ read.cvs}} in your Console, look at the Multifunction pane, the tab \textbf{\texttt{Help}} is auto selected and help message for \textbf{\texttt{read.cvs}} is appeared. See Figure \ref{fig:Rhelp}

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{Capture/Rhelp} 

}

\caption{Screen capture of Error and Help}\label{fig:Rhelp}
\end{figure}

Now, notice that the error message says, ``\texttt{cannot\ open\ file\ \textquotesingle{}test.csv\textquotesingle{}:\ No\ such\ file\ or\ directory}''. We don't have file \texttt{train.csv} and \texttt{test.csv} in our working directory.

Now, Download the \texttt{train.csv} and \texttt{test.csv} from Kaggle (if you did not download already) and stored into our project working directory\footnote{The data files were asked to be downloaded and unzipped in the previous chapter. If you simply unzip it into the working directory, it will exists in ``\textasciitilde/Titanic/'' directory. In this case, you need to move them into your working directory. My preference is story them in a dedicated \texttt{data} directory like ``\texttt{\textasciitilde{}data}''.}.

Please note that it is a common practice that data scientist download datasets from data sources and save to a local drive. Having a local copy of the raw datasets is good idea. But a lot of times, it is infeasible to do so either because the data is too big or there are some access restriction. So, you have to using service provider's API or data URI through HTTP protocol or other protocol like FTP etc.

Once, you have download the datasets from the Kaggle website and unzipped (or moved) them into your local working directory, run the same code again by select them all and click ``\texttt{Run}'' or type ``\texttt{Ctr\ +\ Enter}'' .

You will see the two new attributes have been created and displayed in the \textbf{\texttt{WorkSpace\ pane}}. See Figure \ref{fig:importdata}.

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{Capture/importdata} 

}

\caption{Screen capture of import raw data}\label{fig:importdata}
\end{figure}

\hypertarget{assess-data-quantity}{%
\section{Assess Data Quantity}\label{assess-data-quantity}}

After we have load the raw data into our WorkSpace, we can start to explore and exam the raw data.

In R code, the best way to explore a dataset and get the first impression on its size (number of records and numbers of attributes) is using \texttt{str()} function. If you want to know more about it, as I mentioned earlier, using help by typing \texttt{?str()} in your Console. There is an equivalent R code is called \texttt{help\ \textless{}statement\textgreater{}}, you can try \texttt{help\ str()}.

Now let us run the following code,

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# use str to explore train and test}
\KeywordTok{str}\NormalTok{(train)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    891 obs. of  12 variables:
##  $ PassengerId: int  1 2 3 4 5 6 7 8 9 10 ...
##  $ Survived   : int  0 1 1 1 0 0 0 0 1 1 ...
##  $ Pclass     : int  3 1 3 1 3 3 1 3 3 2 ...
##  $ Name       : Factor w/ 891 levels "Abbing, Mr. Anthony",..: 109 191 358 277 16 559 520 629 417 581 ...
##  $ Sex        : Factor w/ 2 levels "female","male": 2 1 1 1 2 2 2 2 1 1 ...
##  $ Age        : num  22 38 26 35 35 NA 54 2 27 14 ...
##  $ SibSp      : int  1 1 0 1 0 0 0 3 0 1 ...
##  $ Parch      : int  0 0 0 0 0 0 0 1 2 0 ...
##  $ Ticket     : Factor w/ 681 levels "110152","110413",..: 524 597 670 50 473 276 86 396 345 133 ...
##  $ Fare       : num  7.25 71.28 7.92 53.1 8.05 ...
##  $ Cabin      : Factor w/ 148 levels "","A10","A14",..: 1 83 1 57 1 1 131 1 1 1 ...
##  $ Embarked   : Factor w/ 4 levels "","C","Q","S": 4 2 4 4 4 3 4 4 4 2 ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(test)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    418 obs. of  11 variables:
##  $ PassengerId: int  892 893 894 895 896 897 898 899 900 901 ...
##  $ Pclass     : int  3 3 2 3 3 3 3 2 3 3 ...
##  $ Name       : Factor w/ 418 levels "Abbott, Master. Eugene Joseph",..: 210 409 273 414 182 370 85 58 5 104 ...
##  $ Sex        : Factor w/ 2 levels "female","male": 2 1 2 2 1 2 1 2 1 2 ...
##  $ Age        : num  34.5 47 62 27 22 14 30 26 18 21 ...
##  $ SibSp      : int  0 1 0 0 1 0 0 1 0 2 ...
##  $ Parch      : int  0 0 0 0 1 0 0 1 0 0 ...
##  $ Ticket     : Factor w/ 363 levels "110469","110489",..: 153 222 74 148 139 262 159 85 101 270 ...
##  $ Fare       : num  7.83 7 9.69 8.66 12.29 ...
##  $ Cabin      : Factor w/ 77 levels "","A11","A18",..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ Embarked   : Factor w/ 3 levels "C","Q","S": 2 3 2 3 3 3 2 3 1 3 ...
\end{verbatim}

You will see this in your console, Figure \ref{fig:Rstr}.

\begin{figure}

{\centering \includegraphics[width=1.3\linewidth]{Capture/RstrResult} 

}

\caption{Screen capture of str(train) and str(test)}\label{fig:Rstr}
\end{figure}

Firstly, you will see the size of the two datasets:

\begin{itemize}
\tightlist
\item
  \texttt{train} has \textbf{891 records} and each record has \textbf{12 attributes}.
\end{itemize}

Notice that R uses statistics terminology, observation is record in data science term, properties of an observation are attributes of a record\footnote{The same terms have different names in Machine Learning, which we will mention later, but observation is called samples, properties are called variables}. Notice that train has a type of \texttt{data.frame}. \textbf{Data.frame} is the most used data type in R. Try type \texttt{?data.frame} to explore more. We will talk about a dataframe, we usually say dataset. So \texttt{train} is called ``train dataset'' and \texttt{test} is called ``test dataset''.

\begin{itemize}
\tightlist
\item
  \texttt{test} has \textbf{418 records} and each record has \textbf{11 attributes}, which are less than train's in both number of records and attributes.
\end{itemize}

Dataset \texttt{test} has less number of records makes sense because any data model\index{data model}\footnote{A data model is an abstract model that organizes elements of data and standardizes how they relate to one another and to the properties of real-world entities. } generally needs large data to train and less data to test (it will become clear later).

However, why \texttt{test} has one less attribute in comparison with the \texttt{train} dataset? It is also easy to find out that the missing attribute is \emph{Survived}. The dataset \texttt{test} is supposedly to be used for testing our prediction model (we will build it later) for predicting passengers' have lived (Survived) and dead (Perished). So, it should not have a value now. \textbf{The entire problem is for us to come up with a value on the attribute}.

RStudio has a conveniently build-in function to explore data size. At the \textbf{\texttt{WorkSpace\ pane}}, you can see the under \textbf{\texttt{Environment}} tab, the two attributes we have created are listed there. In font of each attribute there is a
\includegraphics{Capture/arrow.png} sign. click it you can exam its size and structure. It is equivalent to run \texttt{str()} R instruction. You can also lick on the attribute name to explore the entire dataset.

\begin{rmdaction}
Try yourself:

At RStudio \textbf{WorkSpace pane},

Click varaible name \texttt{train} and \texttt{test} to explore the contents of datasets.

Click on the \includegraphics{Capture/arrow.png} sign in front of attribute to explore it sstructure.
\end{rmdaction}

\hypertarget{general-data-attributes-assessment}{%
\section{General Data Attributes Assessment}\label{general-data-attributes-assessment}}

After a brief assessment on the data quantity, we know that the both datasets are not too big in terms of both number records (891 and 418) and number of attributes (12 and 11). We also have an intuitive understanding about the attributes, some obvious names like \emph{Name}, \emph{Sex} and \emph{Age}; and some not so obvious names like \emph{SibSp} and \emph{Parch}.

Before we looking into individual attributes (single variate analysis) in our datasets, let us get some general sense of all attributes and make sure we understand each of them.

We knew that dataset \texttt{test} has 11 attributes and \texttt{train} has 12 attributes. The one attribute short is the \emph{Survived}. The rest are the same. Let us look into those attributes, the following is from the Kaggle web site:

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{Capture/DatafrKaggle} 

}

\caption{Data Dectionary from Kaggle website.}\label{fig:unnamed-chunk-3}
\end{figure}

\begin{verbatim}
attribute Notes
Pclass: A proxy for socio-economic status (SES)

1st = Upper
2nd = Middle
3rd = Lower

Age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5

Sibsp: The dataset defines family relations in this way...

Sibling = brother, sister, stepbrother, stepsister
Spouse = husband, wife (mistresses and fiancés were ignored)

Parch: The dataset defines family relations in this way...

Parent = mother, father
Child = daughter, son, stepdaughter, stepson
Some children travelled only with a nanny, therefore parch=0 for them.
\end{verbatim}

Just looking into these attributes' description, a few thoughts are occurred:

\textbf{1. Attribute types}

There are attributes should be categorical types. The values of those attributes can be any types but the importance is that they can classify the records into sets of similar groups and this grouping make sense to the problem to be solved. In Titanic datasets, attributes should have \textbf{categorical} type are: \emph{Survived}, \emph{Sex}, \emph{Embarked}, and \emph{Pclass}.

Other attribute perhaps should have numerical type. Thsi is because these attributes values change from record to record. They can be the values of discrete, continuous, or time series. One thing in common is that these values can be manipulated and applied with many math functions and plotting tools for visualization. In Titanic datasets, attributes should have \textbf{numerical} type are: \emph{Age}, \emph{Fare}, \emph{SibSp}, \emph{Parch}.

\textbf{2. Contribution to Survive}

The final goal is to predict passenger's survived or not. It makes sense to assess the \textbf{prediction power} of each attribute, which is the contribution of an attribute to attribute \emph{Survived}. In other words, the potential relationships among these attributes and with the attribute \emph{Survived} need to be assessed. Here are some thoughts:

\begin{itemize}
\item
  \emph{Pclass} should somehow linked with \emph{Fare} and \emph{Cabin}. Generally, the higher the class is and the more expensive of the fare will be and the better cabin locations are. So those should have some sort of correlations among them. they together should have some affect on survive. You would think that the expensive ticket, means better cabin location and has privilege to escape first in the disaster.
\item
  What is the ticket number to do with survive? Is it just a random number? Or is associated with cabin? Or anything else like Port of embarkation? ticket number in some other systems could have more information rather than just an unique number.
\item
  Is the \emph{Fair} in some ways associated with journey length, which means the Port of embarkation and the port of disembarkation? Or cabin location and condition?
\end{itemize}

You may have other thoughts too. To prove or disprove these assumptions and thoughts, we need to look into the actual datasets at least to see:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What are the data types for various attributes?
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Which attributes are available in the dataset?
\item
  Which attributes are categorical?
\item
  Which attributes are numerical?
\item
  Which attributes are mixed data types?
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Any errors in the attributes values?
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Which features may contain errors or typos?
\item
  Which features contain blank, null or empty values?
\end{itemize}

These questions will be answered in the following two sections.

\hypertarget{actual-attributes-types-examination}{%
\section{Actual Attributes Types Examination}\label{actual-attributes-types-examination}}

Since we have our raw data in RStudio, We can exam attributes' types. From figure \ref{fig:Rstr}, we can see that all the attributes have three types, \texttt{int}, \texttt{Factor}, \texttt{num}.

\begin{itemize}
\item
  Attributes have \texttt{int} types are: \emph{PassengerId}, \emph{Survived}, \emph{SibSp}, \emph{Parch}.
\item
  Attributes has \texttt{Factor} types are: \emph{Name}, \emph{Sex}, \emph{Ticket}, \emph{Cabin} and \emph{Embarked}.
\item
  Attributes has \texttt{num} types are: \emph{Age} and \emph{Fare}.
\end{itemize}

We know that, the type \texttt{int} is for attribute that has an integer value; and \texttt{num} is for an numeric attribute, which has the values of real numbers.

Type \texttt{Factor} is R language's way to say category type. It is a attribute that can take on one of a limited, and usually fixed, number of possible values, such as blood type.

Attributes types affect the operations we can apply on that attributes. In other words inappropriate types can prevent us to do proper analysis on that attribute. For example, it does not make sense to calculate average on \emph{sex}, so it is better to be with a type of Category, in R is a \texttt{Factor}. Similarly, \emph{Survived} will have only two values 0 or 1, to represent death or live. It makes sense to be an \texttt{Factor} too. Being a \texttt{int} type, it will prevent us to apply many methods that only works for a \texttt{Factor} type attribute.

Another example is \emph{Name}, its original type is \emph{Factor} to reflect on its uniqueness. However, Type ``Factor'' is not good for string processing. It has been prevented that to apply regular expression\footnote{A regular expression is a sequence of characters that define a search pattern, which is used by string-searching algorithms to find a particular string or validate a input string.} on it. So, it is appropriate to change it into \texttt{chr} as a character.

There are other inappropriate or wrong attribute types too such as \emph{SibSp} and \emph{Parch} are currently typed \texttt{int}. May be they should be considered as \texttt{Factor}. It is a common practice that data scientists apply different analyses on a attribute and change the attribute type to apply other different algorithms again\footnote{You will see we have changed some attribute type between \texttt{num} and \texttt{factor} frequently later}. The goal is to dig the insight out of data.

So, looking into data attributes types, compare with the original meaning of each attributes can help us to spot any inappropriate types or wrong types.

\begin{rmdthinking}
Thinking:

Is \emph{Servived} typed \texttt{int} approriate?

What other attributes do you think are in a wrong type?
\end{rmdthinking}

\hypertarget{attvalue}{%
\section{Actual Data Attributes Value Examination}\label{attvalue}}

To understand given datasets needs to carefully examine the values of each data attributes to:

\begin{itemize}
\tightlist
\item
  find any errors and missing values
\item
  find value distribution
\item
  find potential relation with the attribute to be predicted (also called dependent or response variable)
\end{itemize}

Finding errors, typos and missing values can set up the goals for data preprocess.

Since the examine covers both datesets \texttt{train} and \texttt{test}, it make sense to combine the two datasets into one big dataset, so it can save us to run the same code twice on the different datasets.

Copy the following code into your script,

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Add a "Survived" attribute to the test dataset to allow for combining with train dataset}

\NormalTok{test <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(test[}\DecValTok{1}\NormalTok{], }\DataTypeTok{Survived =} \KeywordTok{rep}\NormalTok{(}\StringTok{"NA"}\NormalTok{, }\KeywordTok{nrow}\NormalTok{(test)), test[ , }\DecValTok{2}\OperatorTok{:}\KeywordTok{ncol}\NormalTok{(test)])}

\CommentTok{# Combine data sets. Append test.survived to train}
\NormalTok{data <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(train, test)}
\CommentTok{# We may need to keep the raw data into a file in case we need it later.}
\KeywordTok{write.csv}\NormalTok{(data, }\StringTok{"./data/data.csv"}\NormalTok{, }\DataTypeTok{row.names =} \OtherTok{FALSE}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

Now we have a dataset \texttt{data}, which combines both datasets \texttt{train} and \texttt{test} datasets. We assigned the value of attribute \emph{Survived} in the original dataset \texttt{test} as ``\texttt{NA}''. You can check them in the \textbf{\texttt{WorkSpace\ pane}} by click variable \texttt{data}.

\begin{rmdtry}
Thinking:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Can we combine \texttt{train} and \texttt{test} without add \emph{Survived} attribute to the \texttt{test}? Like,
\end{enumerate}

\texttt{data\ \textless{}-\ rbind(train,\ test)}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Why add attibute \emph{Survived} as the second attribute? Can we add it as the first one? Like,
\end{enumerate}

\texttt{test\ \textless{}-\ data.frame(Survived\ =\ rep("NA",\ nrow(test)),\ test{[},{]})}
\end{rmdtry}

It is good idea to have a bird eye's view on our combined dataset.

\begin{rmdnote}
From now on, whenever you see code chunk. You are supposed to copy and past it into your own R file. So you will have your own copy of code. You can edit and modify it as you wish. You can run them too. We will no long explicitly tell you to do so.
\end{rmdnote}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# check out data with a summary}
\KeywordTok{summary}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   PassengerId     Survived             Pclass     
##  Min.   :   1   Length:1309        Min.   :1.000  
##  1st Qu.: 328   Class :character   1st Qu.:2.000  
##  Median : 655   Mode  :character   Median :3.000  
##  Mean   : 655                      Mean   :2.295  
##  3rd Qu.: 982                      3rd Qu.:3.000  
##  Max.   :1309                      Max.   :3.000  
##                                                   
##                                Name          Sex           Age       
##  Connolly, Miss. Kate            :   2   female:466   Min.   : 0.17  
##  Kelly, Mr. James                :   2   male  :843   1st Qu.:21.00  
##  Abbing, Mr. Anthony             :   1                Median :28.00  
##  Abbott, Mr. Rossmore Edward     :   1                Mean   :29.88  
##  Abbott, Mrs. Stanton (Rosa Hunt):   1                3rd Qu.:39.00  
##  Abelson, Mr. Samuel             :   1                Max.   :80.00  
##  (Other)                         :1301                NA's   :263    
##      SibSp            Parch            Ticket          Fare        
##  Min.   :0.0000   Min.   :0.000   CA. 2343:  11   Min.   :  0.000  
##  1st Qu.:0.0000   1st Qu.:0.000   1601    :   8   1st Qu.:  7.896  
##  Median :0.0000   Median :0.000   CA 2144 :   8   Median : 14.454  
##  Mean   :0.4989   Mean   :0.385   3101295 :   7   Mean   : 33.295  
##  3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7   3rd Qu.: 31.275  
##  Max.   :8.0000   Max.   :9.000   347082  :   7   Max.   :512.329  
##                                   (Other) :1261   NA's   :1        
##              Cabin      Embarked
##                 :1014    :  2   
##  C23 C25 C27    :   6   C:270   
##  B57 B59 B63 B66:   5   Q:123   
##  G6             :   5   S:914   
##  B96 B98        :   4           
##  C22 C26        :   4           
##  (Other)        : 271
\end{verbatim}

This summary \ref{fig:sofdata} tell us a lot of information. Most obvious are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{PassengerID} is useless in terms of predicting survived or not. in addition, it is not much help that provide a statistical summary on it.
\item
  \emph{Survived} and \emph{Pclass} numbers are useful and interesting.
\item
  \emph{Name} is mostly unique, which comes a surprise that only 2 names are repeated twice.
\item
  \emph{Gender} distribution among passenger is unbalanced that male overweight female.
\item
  \emph{Age} is interesting that minimum age 0.17 is alarming and there is 263 missing values.
\item
  \emph{SibSp} tells us the largest relatives travel together is 8.
\item
  \emph{ParCh} tells us the largest family travel together is 9.
\item
  There are a number of \emph{ticket} has the same number. The most repeat number is \texttt{CA.\ 2343}, which has 11 duplicates.
\item
  Ticket \emph{Fare} shows the minimum is 0, which is interesting that someone take a free ride. The maximum is over 512, which is far too expensive when the mean value is only about 33.
\item
  \emph{Cabin} has a large number of missing values (identified by "").
\item
  \emph{Embarked} only has three values which is not a good sign for prediction. It also has 2 missing value.
\end{enumerate}

You can see now one function can provide so much information. \textbf{Quantitative summary is a great tool for a data scientist}.

Now, Let us exam each attribute,

\hypertarget{passengerid}{%
\subsection*{PassengerID}\label{passengerid}}


\emph{PassengerId} is an identifier, So only its uniqueness and missing value are considered.

There are many ways you can use to find out. I simply check its total number and its unique number. If the both equal to the number of records in the dataset, it shows that there is no duplication and no missing values in the attribute.

So we do,

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Exam PassengerID}
\KeywordTok{length}\NormalTok{(data}\OperatorTok{$}\NormalTok{PassengerId)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1309
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{length}\NormalTok{(}\KeywordTok{unique}\NormalTok{(data}\OperatorTok{$}\NormalTok{PassengerId))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1309
\end{verbatim}

The results shows the both number 1309, which is equal to the total number of records in the dataset. It proves the \emph{PassengerID} has no missing value and duplication.

\hypertarget{survive}{%
\subsection*{Survived}\label{survive}}


\emph{Survived} is the attribute that its value will be produced by a model for the dataset \texttt{test}\footnote{It is called \emph{Consequencer} or \emph{dependent variable} or \emph{response variable} in modelling contrast with other attributes, which are used to produce a prediction, are called \emph{Predictor}, \emph{independent variable}.}. So, our exam will be conducted only on dataset \texttt{train}. Again we can check the numbers to see whether they can add up or not. As we already mentioned that it makes sense to change the \emph{Servived} from type \texttt{chr} into \texttt{Factor}. We do,

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Exam Survived}
\NormalTok{data}\OperatorTok{$}\NormalTok{Survived <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{Survived)}
\KeywordTok{table}\NormalTok{(data}\OperatorTok{$}\NormalTok{Survived, }\DataTypeTok{dnn =} \StringTok{"Number of Survived in the Data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Number of Survived in the Data
##   0   1  NA 
## 549 342 418
\end{verbatim}

The results proved that the \emph{Survived} value has the correct numbers:

\begin{itemize}
\tightlist
\item
  418 `\texttt{NA}' values are the \emph{Survived}'s value in the test dataset, and
\item
  the 549 death and 342 survived, together made up the total number of train dataset, which is 891.
\end{itemize}

So we know the value of \emph{Survived} in the train dataset are correct and has no missing values. It is interesting here to think about the survival rate. How to calculate?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Calculate the survive rate in train data is 38% and the death rate is 62%}
\KeywordTok{prop.table}\NormalTok{(}\KeywordTok{table}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(train}\OperatorTok{$}\NormalTok{Survived), }\DataTypeTok{dnn =} \StringTok{"Survive and death ratio in the Train"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Survive and death ratio in the Train
##         0         1 
## 0.6161616 0.3838384
\end{verbatim}

So we know the survive rate in the train dataset is 61.62\%. This is interesting because it reflects the overall survival rate and this rate should be maintained in the \texttt{test} too.

\hypertarget{pclass}{%
\subsection*{Pclass}\label{pclass}}


\emph{Pclass} is the feature which splits the passengers into three division namely \texttt{class-1}, \texttt{class-2}, \texttt{class-3}. As we understood it should be in type of \texttt{Factor} rather than \texttt{int}. We shall change its type first and then to see if there missing value or errors. It is also good to know the survival rate in each class. So. we can compare with the overall survival rate in the dataset \texttt{train}. It will give us an impression about the social status on survival.

Run the following code.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Examine Pclass value,}
\CommentTok{# Look into Kaggle's explanation about Pclass: it is a proxy for social class i.e. rich or poor}
\CommentTok{# It should be factor rather than int.}
\NormalTok{data}\OperatorTok{$}\NormalTok{Pclass <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{Pclass)}

\CommentTok{# Distribution across classes into a table}
\KeywordTok{table}\NormalTok{(data}\OperatorTok{$}\NormalTok{Pclass, }\DataTypeTok{dnn =} \StringTok{"Pclass values in the Data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Pclass values in the Data
##   1   2   3 
## 323 277 709
\end{verbatim}

If you want, you can check the total of the three classes which is 1309. It equals to the total number of records in the \texttt{Data} (total number of passengers). And there is no other numbers than 1, 2 and 3. So we can conclude that there is no missing value and no errors in \emph{Pcalss}. These numbers tell us that the over half of passengers are in \texttt{class-3}. It is twice as much as passengers in \texttt{class-1} and \texttt{class-2}.

It will be interesting to see the survival rate for each class,

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Distribution across classes with survive}
\KeywordTok{table}\NormalTok{(data}\OperatorTok{$}\NormalTok{Pclass, data}\OperatorTok{$}\NormalTok{Survived)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    
##       0   1  NA
##   1  80 136 107
##   2  97  87  93
##   3 372 119 218
\end{verbatim}

These numbers tell us many things:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{The death distribution}. Among the three classes from \texttt{class-1} to \texttt{class-3} is: 80, 97 and 372. It confirms that the passengers in \texttt{Class-3} has largest number of death (372).
\item
  \textbf{The survival distribution}. Among the three classes, \texttt{class-1} has the highest number of survival (136) and highest survival rate too (nearly 2/3).
\item
  \textbf{The passengers distribution}. Among the three classes, \texttt{class-3} has the largest passenger numbers in total: \[372+119+218 = 709\] where, 218 is the number of passengers from the \texttt{test}. It overtakes other two classes together for both datasets \texttt{train} and \texttt{test}: \[372+119 = 491 > (80+97) + (136+87)= 400\].
\end{enumerate}

\begin{rmdtip}
Want to do a quick calculation? Need a calculator? R is statistic language. It can evaluate a math express instantly. Try this, Copy and past \texttt{372+119+218} into \texttt{Console} and hit return. You will see reult \texttt{709} straight way.
\end{rmdtip}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  The last column is the passenger distribution among the three glasses for the \texttt{test} dataset. This is because its \emph{Survived} value is ``\texttt{NA}'' (not defined).
\end{enumerate}

We can calculate distributions among the three classes in terms of percentage.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The overall passenger's distribution among the three classes:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Calculate the distribution on Pclass}
\CommentTok{# Overall passenger distribution on classes.}
\KeywordTok{prop.table}\NormalTok{(}\KeywordTok{table}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{Pclass), }\DataTypeTok{dnn =} \StringTok{"Pclass percentage in the Data"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Pclass percentage in the Data
##         1         2         3 
## 0.2467532 0.2116119 0.5416348
\end{verbatim}

That is 24.67\% passenger in \texttt{Class-1}, 21.16\% passenger is \texttt{class-2} and 54.16\% of passenger in \texttt{class-3}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  The passenger's distribution among the three classes given by dataset \texttt{train}:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Train data passenger distribution on classes.}
\KeywordTok{prop.table}\NormalTok{(}\KeywordTok{table}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(train}\OperatorTok{$}\NormalTok{Pclass),}\DataTypeTok{dnn =} \StringTok{"Pclass percentage in the Train"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Pclass percentage in the Train
##         1         2         3 
## 0.2424242 0.2065095 0.5510662
\end{verbatim}

The number tells us the distribution of passengers from dataset \texttt{train} is: \texttt{class-1}, 24.24\%; \texttt{class-2}, 20.65\% and \texttt{class-3} has 55.1\%.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  The passenger's distribution among the three classes in the \texttt{test} dataset:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Test data passenger distribution on classes.}
\KeywordTok{prop.table}\NormalTok{(}\KeywordTok{table}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(test}\OperatorTok{$}\NormalTok{Pclass), }\DataTypeTok{dnn =} \StringTok{"Pclass percentage in the Test"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Pclass percentage in the Test
##         1         2         3 
## 0.2559809 0.2224880 0.5215311
\end{verbatim}

Lastly, the passenger distribution from test dataset are: 25.6\% in \texttt{class-1}, 22.24\% in \texttt{class-2} and 52.15\% percent in \texttt{class-3}.

We can see that the distribution of passengers, in terms of percentage, among the three classes are almost identical for dataset \texttt{train} and \texttt{test} both in order and in proportion. That is the most passenger are in class-3, then class-1 and finally class-2.

Let us look into death and survive distribution among the three classes\footnote{This code is not brilliant. It used many intermediate variables, you can check their structure and contents from \textbf{\texttt{WorkSpace\ pane}}. You may come up with a better code.},

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Calculate death distribution across classes with Train data}
\NormalTok{SurviveOverClass <-}\StringTok{ }\KeywordTok{table}\NormalTok{(train}\OperatorTok{$}\NormalTok{Pclass, train}\OperatorTok{$}\NormalTok{Survived)}
\CommentTok{# Convert SurviveOverClass into data frame}
\NormalTok{SoC.data.fram <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(SurviveOverClass) }
\CommentTok{# Retrieve death distribution in classes}
\NormalTok{Death.distribution.on.class <-}\StringTok{ }\NormalTok{SoC.data.fram}\OperatorTok{$}\NormalTok{Freq[SoC.data.fram}\OperatorTok{$}\NormalTok{Var2}\OperatorTok{==}\DecValTok{0}\NormalTok{]}
\KeywordTok{prop.table}\NormalTok{(Death.distribution.on.class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1457195 0.1766849 0.6775956
\end{verbatim}

These numbers tell us the distribution of death among the three classes are: 14.57\% death from \texttt{class-1}, 17.66\% from \texttt{class-2} and 67.75\% death from \texttt{class-3}.

Similarly, we can calculate survive distribution among the three classes,

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# calculate survive distribution among the three classes}
\NormalTok{Survive.distribution.on.class <-}\StringTok{ }\NormalTok{SoC.data.fram}\OperatorTok{$}\NormalTok{Freq[SoC.data.fram}\OperatorTok{$}\NormalTok{Var2}\OperatorTok{==}\DecValTok{1}\NormalTok{]}
\KeywordTok{prop.table}\NormalTok{(Survive.distribution.on.class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.3976608 0.2543860 0.3479532
\end{verbatim}

The results tell us that 39.76\% of survived passenger are from \texttt{class-1}, and 25.43\% from \texttt{class-2}, and 34.79\% from \texttt{class-3}.

Let us thinking about this numbers. \texttt{Class-3} has 55.1\% of passenger distribution but has 34.79\% passenger survival distribution. Clearly, the survive rate in \texttt{class-3} is lower than other two classes. It is equivalent to say, \textbf{the survival chances of a passenger who is in \texttt{class-1} are higher than who is a \texttt{class-2} and \texttt{class-3}}.

\begin{rmdaction}
Do it yourself:

Calculate the Survival rate among the three classes. What conclusion you have by compare them?
\end{rmdaction}

Numbers are good to provide summary and test some assumptions. Analysing given data by means of statistical summary and other numbering methods is called \textbf{Descriptive analysis}.

Perhaps, it is a good time to introduce \textbf{Exploratory analysis} in our example, on the contrast with the \emph{Descriptive analysis}, it uses graphical tools to explore the inside of given datasets.

To do so, we need to import some useful graphical tools provided by R community. We can then use them to plot \emph{Survived} as an factor on \emph{Pclass} numbers.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Load up ggplot2 package to use for visualizations}
\KeywordTok{library}\NormalTok{(ggplot2)}

\KeywordTok{ggplot}\NormalTok{(train, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Pclass, }\DataTypeTok{fill =} \KeywordTok{factor}\NormalTok{(Survived))) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{width =} \FloatTok{0.3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Pclass"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Total Count"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"Survived"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{04-understand-data_files/figure-latex/Survivalrate-1} 

}

\caption{Total count and survive rate of passenger on Pcalss.}\label{fig:Survivalrate}
\end{figure}

Graph is better, isn't it? It is very intuitive.

Let's briefly interpret this graph. The graph shown \ref{fig:Survivalrate} tells us that the survive rate in \texttt{Class-3} is the worst, and followed by \texttt{class-2} and lastly, \texttt{class-1}. More people perished in the \texttt{class-3} than any other two classes. It provides an important point that the chance of survive is associated with the \textbf{``social glass''}, if we can prove the \texttt{Class-3} ticket is cheaper.

To sum up the analysis with \emph{Pclass}, We have used both \emph{Descriptive analysis} and \emph{Exploratory analysis} methods. The results suggested that \textbf{the \emph{Pclass} has a strong relation with death rate}. That is passengers in \texttt{Class-3} have a higher chance of death. The correlation with social class (richer or poor) is waiting to be proved if the \texttt{class-3} ticket is cheaper than others.

\hypertarget{name}{%
\subsection*{Name}\label{name}}


\emph{Name} attribute by definition shows peoples' name. It should not have any impact on passengers' live and death. Never heard of someone was survived because one's name!
However we still need to assess its quality.

Firstly, you may notice that the type of \emph{Name} is a \texttt{Factor}, which is contradicted with the conventional understanding that name is a string or a list characters. Type \texttt{chr} would be more appropriate. Change its type to \texttt{chr} will help us to apply character functions to it and get it contents easily. Factor shows the uniqueness. it could help us to assess if there is missing value or duplicated values.

Notice that attribute \emph{Name} only has 1307 levels\footnote{The level is a unit used in the statistics for factor} (can be observed from the \texttt{data} structure on the \textbf{`WorkSpace pane'}). In addition, the \texttt{data} summary (in the beginning of this section, which can also be accessed by \textbf{\texttt{History}} from the \textbf{`WorkSpace pane'} or by \textbf{\texttt{Console}} from the \textbf{\texttt{Console\ pane}}) not only confirmed the 1307 different names but also identified two duplicated names: ``\,`Connolly, Miss. Kate'\,'' and ``\,`Kelly, Mr.~James'\,'' that have been repeated twice each.

\begin{rmdaction}
Do it yourself:
We have run \texttt{summary(data)} a while ago. You can try to find the results of that run by either re-run the command or check its results from Console.
1. Re-run \texttt{summary(data)}. You can type the command in \texttt{console}, or you can find it from \texttt{history}, select it and click \texttt{to\ Console}, or you can at the console keep press up-arror key to find it.\\
2. Find result. You can switch to console pane and use virtical scroll control to find the results of \texttt{summary(data)} directly.
\end{rmdaction}

Let us explore \emph{Name} values in details. Firstly, let us convert \emph{Name} type into \texttt{chr}. We can then check duplicated names by using \texttt{which} function in R to get the duplicate names and store them into a vector \texttt{dup.names}. WE finally echo them out.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Convert Name type}
\NormalTok{data}\OperatorTok{$}\NormalTok{Name <-}\StringTok{ }\KeywordTok{as.character}\NormalTok{(data}\OperatorTok{$}\NormalTok{Name)}
\CommentTok{# Find the two duplicate names. First used which function to # get the duplicate names and store them in a vector dup.names }
\CommentTok{# check it up ?which.}
\NormalTok{dup.names <-}\StringTok{ }\NormalTok{data[}\KeywordTok{which}\NormalTok{(}\KeywordTok{duplicated}\NormalTok{(data}\OperatorTok{$}\NormalTok{Name)), }\StringTok{"Name"}\NormalTok{]}

\CommentTok{# Echo out }
\NormalTok{dup.names}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Kelly, Mr. James"     "Connolly, Miss. Kate"
\end{verbatim}

Our code confirmed that the two duplicated names are indeed ``\texttt{Kelly,\ Mr.\ James}'' and ``\texttt{Connolly,\ Miss.\ Kate}''. It comes no surprise that the both names are pretty common in UK and USA.

One discovery though is that the names appeared has a title in it! `Mr.' is used in \texttt{Kelly\ James} and `Miss.' is used in \texttt{Connolly\ Kate}. This could be interesting and important. We first said names cannot be a predictor because it has no generalization, but a title like \texttt{Mr.} does. From the numbers of \texttt{Mr.}`s death and survive we may come up with a prediction about how much chance a new \texttt{Mr.} can survive. We can leave this for 'features re-engineering' in the \textbf{Data Preprocess} to explore more. For the quality assessment it is mission accomplished.

\hypertarget{sex}{%
\subsection*{Sex}\label{sex}}


\emph{Sex} attribute assessment is simple. Its type \texttt{Factor} helps a lot. Since it only has two values ``\texttt{male}'' and ``\texttt{female}'', we could easily check if there are missing values and any errors.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Use summary to check numbers and distribution}
\KeywordTok{summary}\NormalTok{(data}\OperatorTok{$}\NormalTok{Sex)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## female   male 
##    466    843
\end{verbatim}

It is obvious that there is no error and missing values. The result confirms there are 843 male passengers and 466 female passengers, together 1309 passengers, which is the total numbers of the passenger we have from the data summary.

It is also simple to explore the relationship between gender and the survival rate. We had an assumption that the male passenger have a high death rate. We have plot tools in our disposal, let's make use of it. Since only dataset \texttt{train} has the values on \emph{Survived}, it makes sense that we only plot relation between gender and survival on dataset \texttt{train}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot Survived over Sex on dataset train}
\KeywordTok{ggplot}\NormalTok{(data[}\DecValTok{1}\OperatorTok{:}\DecValTok{891}\NormalTok{,], }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Sex, }\DataTypeTok{fill =}\NormalTok{ Survived)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{width =} \FloatTok{0.3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Sex"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Total Count"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"Survived"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{04-understand-data_files/figure-latex/Survivalrateonsex-1} 

}

\caption{Total count and survive rate of passenger on sex.}\label{fig:Survivalrateonsex}
\end{figure}

The graph shows that the male death rate is much higher than the female passenger's death rate.

\begin{rmdthinking}
Thinking:

We have used \texttt{data{[}1:891,{]}} in our \texttt{ggplot} code. Why we do not use dataset \texttt{train} instead? What are the differnce if there is any?
\end{rmdthinking}

\hypertarget{age}{%
\subsection*{Age}\label{age}}


To examine values of attribute \emph{Age}, we do this,

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Examine Age over data, train and test.}
\KeywordTok{summary}\NormalTok{(data}\OperatorTok{$}\NormalTok{Age)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
##    0.17   21.00   28.00   29.88   39.00   80.00     263
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(train}\OperatorTok{$}\NormalTok{Age)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
##    0.42   20.12   28.00   29.70   38.00   80.00     177
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(test}\OperatorTok{$}\NormalTok{Age)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
##    0.17   21.00   27.00   30.27   39.00   76.00      86
\end{verbatim}

These summary tell us that the minimum, median, mean, maximum and missing values (as \texttt{NA}). They are useful but they do tell us the age value distribution.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# It makes sense to change age type to Factor to see distribution}
\KeywordTok{summary}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{Age))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 0.17 0.33 0.42 0.67 0.75 0.83 0.92    1    2    3    4    5    6    7    8    9 
##    1    1    1    1    3    3    2   10   12    7   10    5    6    4    6   10 
##   10   11 11.5   12   13   14 14.5   15   16   17   18 18.5   19   20 20.5   21 
##    4    4    1    3    5    8    2    6   19   20   39    3   29   23    1   41 
##   22 22.5   23 23.5   24 24.5   25   26 26.5   27   28 28.5   29   30 30.5   31 
##   43    1   26    1   47    1   34   30    1   30   32    3   30   40    2   23 
##   32 32.5   33   34 34.5   35   36 36.5   37   38 38.5   39   40 40.5   41   42 
##   24    4   21   16    2   23   31    2    9   14    1   20   18    3   11   18 
##   43   44   45 45.5   46   47   48   49   50   51   52   53   54   55 55.5   56 
##    9   10   21    2    6   14   14    9   15    8    6    4   10    8    1    4 
##   57   58   59   60 60.5   61   62   63   64   65   66   67   70 70.5   71   74 
##    5    6    3    7    1    5    5    4    5    3    1    1    2    1    2    1 
##   76   80 NA's 
##    1    1  263
\end{verbatim}

We can see a few problems from the summary above:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Age values have a decimal point which is a kind of surprise and not sure if it is a mistake.
\item
  There are large number of missing values: 177 missing value in \texttt{train} and 86 missing value in \texttt{test}, total of 263 is missing, which count as 263/1309 = 20\%. A large number of missing values sets up a task for \textbf{Data preprocess}. In the same time, it make you think whether it can be a valid predictor or not.
\end{enumerate}

We can assess its impact on survive rate. So we need to look into dataset \texttt{train}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot distribution of age group}
\KeywordTok{ggplot}\NormalTok{(data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Age)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \DecValTok{10}\NormalTok{, }\DataTypeTok{fill=}\StringTok{"steelblue"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Age"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Total Count"}\NormalTok{)}

\CommentTok{# plot Survived on age group using train dataset}
\KeywordTok{ggplot}\NormalTok{(data[}\DecValTok{1}\OperatorTok{:}\DecValTok{891}\NormalTok{,], }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Age, }\DataTypeTok{fill =}\NormalTok{ Survived)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \DecValTok{10}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Age"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Total Count"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{04-understand-data_files/figure-latex/age-1} \includegraphics[width=0.5\linewidth]{04-understand-data_files/figure-latex/age-2} 

}

\caption{*Age* ditribution and its survive rate.}\label{fig:age}
\end{figure}

The graph shows the relationship between \emph{Age} and survival rate. It becomes apparent that age group between 15 and 25 has the worst survival rate.

With this, we could conclude that. The attribute \emph{Age} has a serious quality problem: \textbf{some age values are negative and large number 177 values are missing}. If it is to be used as a predictor in a prediction model, it needs a lot of work in the stage of preprocess.

\hypertarget{sibsp}{%
\subsection*{SibSp}\label{sibsp}}


Attribute \emph{SibSp} represents passenger's siblings and sprouts who travel with the passenger. We do this, 1. check its summary; 2. find unique numbers to see its variants; 3. check missing values; 4. check value distribution.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{### Exam SibSp, Its original type is int}
\KeywordTok{summary}\NormalTok{((data}\OperatorTok{$}\NormalTok{SibSp))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.0000  0.0000  0.0000  0.4989  1.0000  8.0000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# How many possible unique values?}
\KeywordTok{length}\NormalTok{(}\KeywordTok{unique}\NormalTok{(data}\OperatorTok{$}\NormalTok{SibSp))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 7
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#is there any missing values? check the total number}
\KeywordTok{length}\NormalTok{(data}\OperatorTok{$}\NormalTok{SibSp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1309
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Treat it as a factor, so we know the value distribution}
\NormalTok{data}\OperatorTok{$}\NormalTok{SibSp <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{SibSp)}
\KeywordTok{summary}\NormalTok{(data}\OperatorTok{$}\NormalTok{SibSp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   0   1   2   3   4   5   8 
## 891 319  42  20  22   6   9
\end{verbatim}

The above operations are pretty standard quality check for any number variable. The results have provided us with good evidence for accessing its values:

\begin{itemize}
\tightlist
\item
  Firstly, we know the minimum value is 0, and there are 891 records have 0 values. It means that there are 891 passenger who travel without siblings and sprouts;
\item
  secondly, apart from the value 0, the 3 quarters of the passengers who have 1 company; and
\item
  lastly the maximum number of company is 8. There are 9 of them.
\item
  There are totally 7 different numbers of company a passenger can have. It has not error or missing value since the total number are correct.
\end{itemize}

We can assess its prediction power by looking into the relationship between \emph{SibSp} and \emph{Suvivied},

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot entire SibSp distribution among the 7 values}
\KeywordTok{ggplot}\NormalTok{(data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ SibSp)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{width =} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"SibSp"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Total Count"}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_cartesian}\NormalTok{()}

\CommentTok{# Plot on the survive on SibSp}
\KeywordTok{ggplot}\NormalTok{(data[}\DecValTok{1}\OperatorTok{:}\DecValTok{891}\NormalTok{,], }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ SibSp, }\DataTypeTok{fill =}\NormalTok{ Survived)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{width =} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"SibSp"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Total Count"}\NormalTok{)  }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"Survived"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{04-understand-data_files/figure-latex/SibSp-1} \includegraphics[width=0.5\linewidth]{04-understand-data_files/figure-latex/SibSp-2} 

}

\caption{Plot *SibSp* distribution among the 7 values and its survive rate.}\label{fig:SibSp}
\end{figure}

Similar with the \emph{Age}, we run two plots: the first one is the value distribution on entire dataset to have an impression on its distribution shape; and the second one is the survival rate over its distribution groups according to dataset \texttt{train}. It seems that passenger who have two companies tend to have a better survival rate. This could be an interesting pattern to explore.

\begin{rmdaction}
Do it yourself:

Calculate the Survival rate among the 7 possibilities in terms of have siblings or sprouds treval with them. What conclusion you have by compare them?
\end{rmdaction}

We can conclude that the attribute \texttt{SibSp} has a pretty good quality and there is no apparent error and missing values. Its predication power needs further investigation but it is informative.

\hypertarget{parch}{%
\subsection*{Parch}\label{parch}}


Attribute \emph{Parch}, similar with \emph{SibSp}, is representing the travel company or groups. \emph{Parch} specifically represents parents or children. I don't know why Kaggle separate them but it seems reasonable to think they together represent one thing that is \textbf{``travel with family''}.

To access its value, we will do the same as we did on \texttt{SibSp}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{### Exam Parch, Its original type is int}
\KeywordTok{summary}\NormalTok{((data}\OperatorTok{$}\NormalTok{Parch))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   0.000   0.000   0.000   0.385   0.000   9.000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# How many possible unique values?}
\KeywordTok{length}\NormalTok{(}\KeywordTok{unique}\NormalTok{(data}\OperatorTok{$}\NormalTok{Parch))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 8
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#is there an y missing values? check the total number}
\KeywordTok{length}\NormalTok{(data}\OperatorTok{$}\NormalTok{Parch)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1309
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Treat it as a factor, so we know the value distribution}
\NormalTok{data}\OperatorTok{$}\NormalTok{Parch <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{Parch)}
\KeywordTok{summary}\NormalTok{(data}\OperatorTok{$}\NormalTok{Parch)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    0    1    2    3    4    5    6    9 
## 1002  170  113    8    6    6    2    2
\end{verbatim}

The discovery is similar again with \emph{SibSp}, that is:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The minimum value is 0, and there are 1002 records have 0 values. It means that there are 1002 passenger who travel without without parents or children.
\item
  The maximum number is 9. There are 2 of them.
\item
  Apart from the value 0, the largest company number is 1. There are 170.
\item
  There are totally 8 possibilities in terms of the numbers of company a passenger can have.
\item
  It has not error or missing value since the total number are correct.
\end{enumerate}

\begin{rmdthinking}
Thinking:

We cannot say passenger who travel without without parents or children is travel alone, he or she could travel with a sibling or a sprout, However, this rise an idea to look into passenger who travel alone, which means no sibling, sprout, parents and children.
\end{rmdthinking}

We can assess its prediction power too by looking into the relationship between \emph{Parch} and \emph{Survived},

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot entire Parch distribution among the 7 values}
\KeywordTok{ggplot}\NormalTok{(data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Parch)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{width =} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Parch"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Total Count"}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_cartesian}\NormalTok{()}

\CommentTok{# Plot on the survive on Parch}
\KeywordTok{ggplot}\NormalTok{(data[}\DecValTok{1}\OperatorTok{:}\DecValTok{891}\NormalTok{,], }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Parch, }\DataTypeTok{fill =}\NormalTok{ Survived)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{width =} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Parch"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Total Count"}\NormalTok{)  }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"Survived"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{04-understand-data_files/figure-latex/Parch-1} \includegraphics[width=0.5\linewidth]{04-understand-data_files/figure-latex/Parch-2} 

}

\caption{Plot *Parch* distribution among the 8 values and its survive rate.}\label{fig:Parch}
\end{figure}

The plot shows us that it is definitely have an impact on survival. But it is not clear the prediction power in comparison with \emph{SibSp}. I am not sure there are difference between ``\textbf{travel with parents or children}'' and ``\textbf{travel with siblings and sprout}''. In addition, value 0 in each attributes does not excludes the other attributes. Travel without parents or children does not mean travel without siblings or sprout, vice versa. If we try to see the impact on survived in terms of travel alone or with a company, we need to re-engineer these attributes. It is a good point anyway and give another task for \textbf{Data preprocess } to do.

\hypertarget{ticket}{%
\subsection*{Ticket}\label{ticket}}


Intuitively, as mentioned before, \emph{Ticket} number like passenger names, should not be considered as bounded with the survival of a passenger. Unless the ticket number has other hidden information such as class or location on the boat. \emph{Ticket} is a type \texttt{factor} attribute which shows its uniqueness. It has 929 different levels (values). We know there are 1309 passengers. The number difference indicated that either there are missing values or there are duplicated ticket numbers. Bearing this in mind, let us assess its value.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# check top 30 Ticket summary values}
\KeywordTok{head}\NormalTok{(}\KeywordTok{summary}\NormalTok{(data}\OperatorTok{$}\NormalTok{Ticket),}\DecValTok{30}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     CA. 2343         1601      CA 2144      3101295       347077       347082 
##           11            8            8            7            7            7 
##     PC 17608 S.O.C. 14879       113781        19950       347088       382652 
##            7            7            6            6            6            6 
##       113503        16966       220845       349909         4133     PC 17757 
##            5            5            5            5            5            5 
##   W./C. 6608       113760        12749        17421       230136        24160 
##            5            4            4            4            4            4 
##         2666        36928    C.A. 2315   C.A. 33112   C.A. 34651         LINE 
##            4            4            4            4            4            4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Take a look at the ticket value}
\KeywordTok{str}\NormalTok{(data}\OperatorTok{$}\NormalTok{Ticket)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Factor w/ 929 levels "110152","110413",..: 524 597 670 50 473 276 86 396 345 133 ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Find missing values}
\KeywordTok{which}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(data}\OperatorTok{$}\NormalTok{Ticket))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## integer(0)
\end{verbatim}

The value of \emph{Ticket} appears has no missing value and there are 929 different numbers. Together they indicate that there are passengers who share the same ticket number.

Looking into actual ticket number's format, it appeared in two major forms: one with letters and special characters like ``.'' and ``/'' and the others just numbers. There is no immediately apparent structure in the data.

Let us plot them and also see if there is any pattern associated with survival.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#plot ticket values }
\KeywordTok{ggplot}\NormalTok{(data[}\DecValTok{1}\OperatorTok{:}\DecValTok{891}\NormalTok{,], }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Ticket)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Ticket"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Total Count"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{04-understand-data_files/figure-latex/tecketnumber-1} 

}

\caption{Plot of *Ticket* value distribution.}\label{fig:tecketnumber}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Plot on the survive on Ticket}
\KeywordTok{ggplot}\NormalTok{(data[}\DecValTok{1}\OperatorTok{:}\DecValTok{891}\NormalTok{,], }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Ticket, }\DataTypeTok{fill =}\NormalTok{ Survived)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Ticket Number"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Total Count"}\NormalTok{)  }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"Survived"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{04-understand-data_files/figure-latex/tecketratio-1} 

}

\caption{Plot of *Ticket* survival rate.}\label{fig:tecketratio}
\end{figure}

Both Figure \ref{fig:tecketnumber} and Figure \ref{fig:tecketratio} shows that the same ticket number has such a small number of passenger sharing. On other hand, the ticket number's uniqueness (fine grain) reduce its prediction power. It does not have any statistical meaning. It is possible to re-engineer \emph{ticket} number into groups like ``number only'', ``with letter'' or ``with special characters'', or simply group them with the length of the ticket or with the initials, etc. There is a lot of thing you can do to see if there is any patterns connected with the survival.

Over all, \emph{Ticket} has a good quality and has no missing value and errors (we don't count repeated ticket number is an error). However, there is no obvious relations with the survive.

\begin{rmdthinking}
Thinking:

Ticket number exposed another important issues with attributes prediction power. That is you want attibute to have a good blanced between the uniqueness and the generalization. If an attribute is too specifric that has same number of the values with the record numbers like \texttt{PassengerId} (1309), it has no prediction power; if an attribute is too general that only has 1 value it also has no prediction power. The ticket now has 929 differnt values. Its statistical meaning is in series doublt.\\
\end{rmdthinking}

\hypertarget{fare}{%
\subsection*{Fare}\label{fare}}


The attribute \emph{Fare} is the money a passenger paid to get on board the ship. They are expected to reflect a passenger's ``wealth''. The higher fare means the more money a passenger can afford. You would naturally associate the fare with the location of the cabin and the cabin condition. Let us assess its value to confirm or reject our assumptions. We do the summary and checking the uniqueness.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(data}\OperatorTok{$}\NormalTok{Fare)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
##   0.000   7.896  14.454  33.295  31.275 512.329       1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{length}\NormalTok{(}\KeywordTok{unique}\NormalTok{(data}\OperatorTok{$}\NormalTok{Fare))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 282
\end{verbatim}

The assessment tells us that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The value of \texttt{Fare} has one missing value.
\item
  They are 282 different prices among 1308 tickets.
\item
  The minimum value is 0 (Free ride?) and the maximum value is 512.329.
\item
  The mean value is 33.295 and the median is only 14.454.
\item
  There are two potential issues in here: 512.329 is extremely higher than others, it could be considered as an outlier or an error; another potential issue is the precision. Any currency cannot have a physical money which carry value three digits after the decimal point. So any value has three digits after the decimal point could be an error.
\end{enumerate}

Let us examine the prediction power of attribute \emph{Fare}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#plot fare values}
\KeywordTok{ggplot}\NormalTok{(data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Fare)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \DecValTok{5}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Fare Distribution"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Fare"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Total Count"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylim}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{200}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: Removed 1 rows containing non-finite values (stat_bin).
\end{verbatim}

\begin{verbatim}
## Warning: Removed 1 rows containing missing values (geom_bar).
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot fare relation with survive}
\KeywordTok{ggplot}\NormalTok{(data[}\DecValTok{1}\OperatorTok{:}\DecValTok{891}\NormalTok{,], }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Fare, }\DataTypeTok{fill =}\NormalTok{ Survived)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \DecValTok{5}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Fare"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Total Count"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylim}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{50}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"Survived"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: Removed 6 rows containing missing values (geom_bar).
\end{verbatim}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{04-understand-data_files/figure-latex/Fareplot-1} \includegraphics[width=0.5\linewidth]{04-understand-data_files/figure-latex/Fareplot-2} 

}

\caption{Plot of *Fare* distribution and survival rate.}\label{fig:Fareplot}
\end{figure}

It is not clear about the prediction power of the \emph{Fare}. One thing is clear that to be useful for prediction, \emph{Fare} needs more engineering work such as grouping it into different groups such \textless5, 5 to 10, 10 to 15, \ldots, etc. This technique is called \textbf{bagging} or \textbf{binning}. The purpose is to increase its generalization.

\hypertarget{cabin}{%
\subsection*{Cabin}\label{cabin}}


\emph{Cabin} has a large number of missing values as we noticed from the beginning of this section (\texttt{summary(data)}). So its quality is expected to be bed. Let us find out how many missing values in the dataset \texttt{train}, and is there anything interesting. How is the cabin value is formed. With a good understand of its value, we can assess its predictive power over survive or any re-engineering work should be done.

Again we can look into its summary and structure to get a general impression and then probably we can look into its detailed formation.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Examine cabin values in general}
\KeywordTok{str}\NormalTok{(data}\OperatorTok{$}\NormalTok{Cabin)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Factor w/ 187 levels "","A10","A14",..: 1 83 1 57 1 1 131 1 1 1 ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(data}\OperatorTok{$}\NormalTok{Cabin)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                     C23 C25 C27 B57 B59 B63 B66              G6         B96 B98 
##            1014               6               5               5               4 
##         C22 C26             C78               D              F2             F33 
##               4               4               4               4               4 
##              F4             A34     B51 B53 B55         B58 B60            C101 
##               4               3               3               3               3 
##            E101             E34             B18             B20             B22 
##               3               3               2               2               2 
##             B28             B35             B41             B49              B5 
##               2               2               2               2               2 
##             B69             B71             B77             B78            C106 
##               2               2               2               2               2 
##            C123            C124            C125            C126              C2 
##               2               2               2               2               2 
##             C32             C46             C52             C54         C62 C64 
##               2               2               2               2               2 
##             C65             C68              C7             C83             C85 
##               2               2               2               2               2 
##             C86             C92             C93         D10 D12             D15 
##               2               2               2               2               2 
##             D17             D19             D20             D21             D26 
##               2               2               2               2               2 
##             D28             D30             D33             D35             D36 
##               2               2               2               2               2 
##             D37            E121             E24             E25             E31 
##               2               2               2               2               2 
##             E33             E44             E46             E50             E67 
##               2               2               2               2               2 
##              E8           F G63           F G73             B45            C116 
##               2               2               2               2               2 
##             C31         C55 C57              C6             C80             C89 
##               2               2               2               2               2 
##             A10             A14             A16             A19             A20 
##               1               1               1               1               1 
##             A23             A24             A26             A31             A32 
##               1               1               1               1               1 
##             A36              A5              A6              A7            B101 
##               1               1               1               1               1 
##            B102             B19              B3             B30         (Other) 
##               1               1               1               1              88
\end{verbatim}

From the summary and structure of \emph{Cabin}, we can see,

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  From \texttt{str()}, we can find that \emph{Cabin} is in a type of \texttt{Factor} and has 187 unique values including empty string "" and string start with letter like ``A10'', ``B30'', and ``D56''.
\item
  From \texttt{summary()}, we can see that it has 1014 missing values (empty string "");
\item
  There are small numbers of cabin(s) has been shared by multiple passengers. The maximum number of passengers sharing cabins is 6 (6 passenger share cabin \texttt{C23\ C25\ C27}), the frequent number of the passengers share a cabin is 2, which has 33. It means there are 33 cabins shared by two passengers; and 5 cabins share by 3 passengers and 8 cabins shared by 4 passenger and only 2 cabins shared by 5 passengers. There multiple passenger share one cabin (5 passengers share one cabin G6) and there are multiple passenger share multiple cabins (5 passengers share cabin B57 B59 B63 B66)
\end{enumerate}

Now, let us looking into records' cabin values to figure out its formation.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Cabin really isn't a factor, make a string and the display first 100}
\NormalTok{data}\OperatorTok{$}\NormalTok{Cabin <-}\StringTok{ }\KeywordTok{as.character}\NormalTok{(data}\OperatorTok{$}\NormalTok{Cabin)}
\NormalTok{data}\OperatorTok{$}\NormalTok{Cabin[}\DecValTok{1}\OperatorTok{:}\DecValTok{100}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   [1] ""            "C85"         ""            "C123"        ""           
##   [6] ""            "E46"         ""            ""            ""           
##  [11] "G6"          "C103"        ""            ""            ""           
##  [16] ""            ""            ""            ""            ""           
##  [21] ""            "D56"         ""            "A6"          ""           
##  [26] ""            ""            "C23 C25 C27" ""            ""           
##  [31] ""            "B78"         ""            ""            ""           
##  [36] ""            ""            ""            ""            ""           
##  [41] ""            ""            ""            ""            ""           
##  [46] ""            ""            ""            ""            ""           
##  [51] ""            ""            "D33"         ""            "B30"        
##  [56] "C52"         ""            ""            ""            ""           
##  [61] ""            "B28"         "C83"         ""            ""           
##  [66] ""            "F33"         ""            ""            ""           
##  [71] ""            ""            ""            ""            ""           
##  [76] "F G73"       ""            ""            ""            ""           
##  [81] ""            ""            ""            ""            ""           
##  [86] ""            ""            ""            "C23 C25 C27" ""           
##  [91] ""            ""            "E31"         ""            ""           
##  [96] ""            "A5"          "D10 D12"     ""            ""
\end{verbatim}

By looking into the first 100 cabin values, we find that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Some values have multiple numbers, for instance ``C23 C25 C27'', ``D10 D12'' and ``F G73''. It means some passenger (one passenger) has multiple cabins.
  Considering that we already knew that there are cabins share by multiple passengers. This one passenger has multiple cabin and one cabin shared by multiple passenger make cabin value extremely informative.
\end{enumerate}

\begin{rmdthinking}
Thinking:
What realations could passenger have if they share one or muiplte cabins?
Considering the earlier 20 centry ther are may still family sevent exists.
find out the relations among the people share cabin sould reveal family relationship and social status. Those information can be very useful for prediciton.\\
\end{rmdthinking}

Now let us looking into large number of missing values.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Find out number of the missing value in the train}
\NormalTok{train}\OperatorTok{$}\NormalTok{Cabin <-}\StringTok{ }\KeywordTok{as.character}\NormalTok{(train}\OperatorTok{$}\NormalTok{Cabin)}
\CommentTok{# number of the missing value in the train}
\KeywordTok{length}\NormalTok{(train[}\KeywordTok{which}\NormalTok{(train}\OperatorTok{$}\NormalTok{Cabin }\OperatorTok{==}\StringTok{""}\NormalTok{), }\StringTok{"Cabin"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 687
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# percentage of the missing value in the train}
\KeywordTok{length}\NormalTok{(train[}\KeywordTok{which}\NormalTok{(train}\OperatorTok{$}\NormalTok{Cabin }\OperatorTok{==}\StringTok{""}\NormalTok{), }\StringTok{"Cabin"}\NormalTok{])}\OperatorTok{/}\KeywordTok{length}\NormalTok{(train}\OperatorTok{$}\NormalTok{Cabin)}\OperatorTok{*}\DecValTok{100}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 77.10438
\end{verbatim}

The above code tells us that in the dataset \texttt{train}, there are 687 records has no \emph{Cabin} value and it count as 77.1\% of the total value. This is significant number. Generally it will write off the attribute for any meaningful use.

\begin{rmdthinking}
Thinking:
Why there 77.1 percent of records has no cabin number? is it a mistake or ther are something meaningful? Whenever a large propotion of missing value always asking why? may be finding the anwser copuld lead to a important discover. A lot of times has a value and has no value itself is an important value.
\end{rmdthinking}

Since the small number of passenger in each cabin in case of sharing, we can simply use the first letter of the cabin number as passengers cabin. That means we bin (group) the passengers based on the first letter of their cabin number. Although it may over simplified but it is a way of grouping. Let us see the relationship between it and the survive.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Take a look at just the first char as a factor and add to data as a new attribute}
\NormalTok{data}\OperatorTok{$}\NormalTok{cabinfirstchar<-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(}\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{Cabin, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\CommentTok{# first cabin letter survival plot}
\KeywordTok{ggplot}\NormalTok{(data[}\DecValTok{1}\OperatorTok{:}\DecValTok{891}\NormalTok{,], }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ cabinfirstchar, }\DataTypeTok{fill =}\NormalTok{ Survived)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"First Cabin Letter"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Total Count"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylim}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{750}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"Survived"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{04-understand-data_files/figure-latex/Cabin-1} 

}

\caption{Plot of the passenger number and  the survived number based on the first letter of their cabin number.}\label{fig:Cabin}
\end{figure}

It is clearly seriously skewed to the left since the large number of missing value. However the missing value seems reflect the over all survive rate too. It means it has prediction power on its own.

To sum up, \emph{Cabin} attribute has large number of missing value (1014). The dataset \texttt{train} has 687 missing value and it counts as 71 percent of total value. Except its missing value, \emph{Cabin} has 186 different values. These values are single value and multiple values too. To make things more complicated, it permits duplicated values too. It means that multiple records share the same value as multiple passengers share one or more cabins.\\
Its prediction power is in serious doubt since it only has very small number for each cabin. But it has so many information buried into it. It could be teh deciding factor for prediction models after attribute re-engineering.

\hypertarget{embarkded}{%
\subsection*{Embarkded}\label{embarkded}}


Attribute \emph{Embarked} records where a passenger get on board. From the Kaggle description we know that there are three possible values: Southampton (S), Cherbourg (C), and Queenstown (Q). Let's check the attribute quality.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Examine Embark values}
\KeywordTok{summary}\NormalTok{(data}\OperatorTok{$}\NormalTok{Embarked)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       C   Q   S 
##   2 270 123 914
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{length}\NormalTok{(data}\OperatorTok{$}\NormalTok{Embarked)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1309
\end{verbatim}

The results confirms that there two missing values and three ports. Southampton as its initial depart port has largest passenger numbers. Let's see its distribution and the survival rate.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Plot data distribution and the survival rate for analysis}
\KeywordTok{ggplot}\NormalTok{(data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Embarked)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{width=}\FloatTok{0.5}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Passenger embarked port"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Total Count"}\NormalTok{) }


\KeywordTok{ggplot}\NormalTok{(data[}\DecValTok{1}\OperatorTok{:}\DecValTok{891}\NormalTok{,], }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Embarked, }\DataTypeTok{fill =}\NormalTok{ Survived)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{width=}\FloatTok{0.5}\NormalTok{) }\OperatorTok{+}
\StringTok{   }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Embarked port"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Total Count"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"Survived"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{04-understand-data_files/figure-latex/Fare-1} \includegraphics[width=0.5\linewidth]{04-understand-data_files/figure-latex/Fare-2} 

}

\caption{Plot of *Embarked* distribution and survival rate.}\label{fig:Fare}
\end{figure}

The graph shows that about 70\% of the people boarded from Southampton (914/1309 = 0.698). Just over 20\% boarded from Cherbourg (270/1309 = 0.206) and the rest boarded from Queenstown, which is about 10\%.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Calculate death distribution over Embarked port with Train data}
\CommentTok{# create Embarked and Survived contingency table}
\NormalTok{SurviveOverEmbarkedTable <-}\StringTok{ }\KeywordTok{table}\NormalTok{(train}\OperatorTok{$}\NormalTok{Embarked, train}\OperatorTok{$}\NormalTok{Survived)}
\CommentTok{# Death-0/survived-1 value distribution (percentage) based on embarked ports}
\CommentTok{# prop.table(mytable, 2) give us column (Survived) percentages}
\NormalTok{Deathandsurvivepercentage <-}\StringTok{ }\KeywordTok{prop.table}\NormalTok{(SurviveOverEmbarkedTable, }\DecValTok{2}\NormalTok{)}
\CommentTok{# Plot}
\NormalTok{M <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"c-Cherbourg"}\NormalTok{, }\StringTok{"Q-Queenstown"}\NormalTok{, }\StringTok{"S-Southampton"}\NormalTok{)}
\KeywordTok{barplot}\NormalTok{(Deathandsurvivepercentage[}\DecValTok{2}\OperatorTok{:}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{]}\OperatorTok{*}\DecValTok{100}\NormalTok{, }\DataTypeTok{xlab =}\NormalTok{(}\StringTok{""}\NormalTok{), }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{100}\NormalTok{), }\DataTypeTok{ylab=}\StringTok{"Death distribution in percentage %"}\NormalTok{,  }\DataTypeTok{names.arg =}\NormalTok{ M, }\DataTypeTok{col=}\StringTok{"steelblue"}\NormalTok{, }\DataTypeTok{main=}\StringTok{"Death distribution"}\NormalTok{, }\DataTypeTok{border=}\StringTok{"black"}\NormalTok{, }\DataTypeTok{beside=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{barplot}\NormalTok{(Deathandsurvivepercentage[}\DecValTok{2}\OperatorTok{:}\DecValTok{4}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{*}\DecValTok{100}\NormalTok{, }\DataTypeTok{xlab =}\NormalTok{(}\StringTok{""}\NormalTok{), }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{100}\NormalTok{), }\DataTypeTok{ylab=}\StringTok{"Servive distribution in percentage %"}\NormalTok{,  }\DataTypeTok{names.arg =}\NormalTok{ M, }\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{, }\DataTypeTok{main=}\StringTok{"Servive distribution"}\NormalTok{, }\DataTypeTok{border=}\StringTok{"black"}\NormalTok{, }\DataTypeTok{beside=}\OtherTok{TRUE}\NormalTok{)}

\CommentTok{## Calculate survived RATE distribution based on embarked ports}
\CommentTok{# Death-0/survived-1 value distribution (percentage) based on embarked ports}
\CommentTok{# prop.table(mytable, 1) give us row (Port) percentages}
\CommentTok{# col-1 (Survived=0, perished) and col-2 (Survived =1, survived)}
\NormalTok{DeathandsurviveRateforeachport <-}\StringTok{ }\KeywordTok{prop.table}\NormalTok{(SurviveOverEmbarkedTable, }\DecValTok{1}\NormalTok{)}
\CommentTok{#plot}
\KeywordTok{barplot}\NormalTok{(Deathandsurvivepercentage[}\DecValTok{2}\OperatorTok{:}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{]}\OperatorTok{*}\DecValTok{100}\NormalTok{, }\DataTypeTok{xlab =}\NormalTok{(}\StringTok{""}\NormalTok{), }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{100}\NormalTok{), }\DataTypeTok{ylab=}\StringTok{"Death rate in percentage %"}\NormalTok{,  }\DataTypeTok{names.arg =}\NormalTok{ M, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{, }\DataTypeTok{main=}\StringTok{"Death rate comparison among mebarked ports"}\NormalTok{, }\DataTypeTok{border=}\StringTok{"black"}\NormalTok{, }\DataTypeTok{beside=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.3\linewidth]{04-understand-data_files/figure-latex/embark-1} \includegraphics[width=0.3\linewidth]{04-understand-data_files/figure-latex/embark-2} \includegraphics[width=0.3\linewidth]{04-understand-data_files/figure-latex/embark-3} 

}

\caption{Plots of death distribution, survive distribution and death rate comparision over embarked port.}\label{fig:embark}
\end{figure}

The plot shows that both death and survive number distribution are similar. Southampton takes most death and survive portion because it has the largest number of passenger get on board, then Cherbourg, and last is Queenstown. However, in terms of death rate, which is the death/total from the passenger who get on board, southampton is the is the highest and then Queenstown, teh last is Cherbourg. That is to say, people who boarded from Cherbourg had a higher chance of survival than people who boarded from Southampton or Queenstown.

In summary, we have explored all attributes through descriptive analysis, which is mainly using numbers and through exploratory analysis, which is using plot. We have examined the quality of each attributes by finding missing values and duplications. We have spotted some outliers and odd values.

We have also assessed relationship between attribute \emph{Survived} and all other attributes. The prediction power of each attributes have been assessed to some extend. More prediction power study such as combination of two or three attributes are needed.

The findings of each attributes provide tasks and goals for data preprocess to accomplish.

\hypertarget{data-recods-level-assessment}{%
\section{Data Recods Level Assessment}\label{data-recods-level-assessment}}

Although we have examined the data samples in the attributes level. We have a good knowledge about the record numbers in the given datasets. It is still necessary to check samples at the record level.

On one hand, if there are some records that have too many missing values for some attributes\footnote{The attribute level missing value check cannot guarantee the record level does not have missing values. It is possible that attribute missing values can be gathered on the same record.}, then they are not useful or invalid samples. They should be removed from the training dataset or the missing values should be filled in order to be useful.

On other hand, some records have identical values for most attributes. These could be considered duplicates. Depends on the problem to be solved, they could be problematic and need also to be dealt with.

In our Titanic problem, record level assessment is not an issue since most of the records are different. This does not mean we should completely ignore this step and not doing any record-level checking.

\hypertarget{summary-2}{%
\section*{Summary}\label{summary-2}}


All the analyses in this chapter provide demonstrations on how to access the raw data and understand their quantity and data quality. Notice that the understanding data is never a single one-off action. You never fully understood the given data. once the analytical process moving on, you may need to come back to apply some new decomposition on some attributes to explore more.

Since our raw data is not too big in terms of both the number of records and the number of attributes. So it is relatively easy to assess their quality. In a real-world project, the raw data can be huge or can be too little. To perform an effective analysis you may need to reduce the data size or in other cases to increase the size. It means you need to do sampling on the given datasets and probably attribute selection too. In other cases, you may need to create new attributes or combine a few attributes together. These are called attributes re-engineering. They are the part of important tasks in the data preprocessing, which is covered in the next chapter on \textbf{data preprocess}.

\begin{rmdinfo}
The entire R code in this chapter is avalable in the file ``TitanicDataAnalysis\_UnderstandData.R'' and it can be find in the appendix.
\end{rmdinfo}

\hypertarget{exercises-3}{%
\section*{Exercises}\label{exercises-3}}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Identify the code in this tutorial which can be conceptually categorized as Descriptive analysis and which one can be Exploratory analysis?
\item
  Find out what is ``rt'' mean in R \texttt{train\ \textless{}-\ read.csv("train.csv",\ header\ =\ TRUE)} error message. Explore how to load files other than \emph{csv} file.
\item
  Explore load data through RStudio build-in functions. Check ``File -\textgreater{} Import Dataset'', also check how to load data from a databases like MySql.
\item
  Calculate survival rate among the three Pclass.
\item
  Calculate the percentage of survival among different SibSp\texttt{and\ Parch} groups.
\item
  Plot distributions of Fare, Embarked of passengers who survived or did not survive.
\item
  Plot survival rate by Sex, Plot survival rate by Pclass, Plot survival rate by SibSp,Plot survival rate by Parch.
\item
  Plot survival rate (percentage of survived over total number) by over Embarked ports.
\end{enumerate}

\hypertarget{data-preparasion}{%
\chapter{Data Preparasion}\label{data-preparasion}}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/images/preparation} 

}

\end{figure}

\begin{quote}
\begin{verbatim}
“Before anything else, preparation is the key to success” 

                                   --- Alexander Graham Bell
\end{verbatim}
\end{quote}

In the previous chapter, we have done \textbf{Data understanding} by examining the quantity and quality of the given data by accessing individual attributes and record levels to do different assessments. After that, we had a pretty good understanding of the raw data in terms of its suitability for analyses. The results of the data assessment set up a number of objectives for the data preprocess or to accomplish, which are what we need to do in the chapter.

First of all, let us briefly review the typical tasks that need to be performed in a typical \textbf{Data preprocess}.

\hypertarget{general-data-prepartion-tasks}{%
\section{General Data Prepartion Tasks}\label{general-data-prepartion-tasks}}

\href{process.html}{Section 1.3} has listed a number of tasks that need to be performed to make data suitable for analysis. Depends on the understanding of the problem, the tasks can be different. In our previous analyses at both records and attributes levels, we have found some problems. These problems need to be solved first of all.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  There are inappropriate data types that need conversion. For example, a lot of features need to be converted into numeric ones so that the machine learning algorithms can process them.
\item
  There are errors or missing values.
\item
  There are attributes' values that need normalization. There are some features have widely different value's range, so the value needs to be converted into roughly the same scale.
\item
  There are also attribute values that need to be grouped or transformed into more manageable meaningful groups.
\end{enumerate}

In this chapter, we will carry on using the Titanic problem to demonstrate the tasks to be performed and the methods that can be used to accomplish these tasks. Remember the ultimate goal of the data preprocessing is to make the dataset suitable for analysis.

The analytical methods used in this chapter are again a mixture of \textbf{Descriptive data analysis} and \textbf{Exploratory analysis}.

\hypertarget{dealt-with-miss-values}{%
\section{Dealt with Miss Values}\label{dealt-with-miss-values}}

We had a pretty good understanding of the Titanic datasets. We knew that there are missing values and some errors. They need to be resolved first of all. The systematic way to find missing value is to write a function checking the missing values, like this one,

Before that, let us quickly recap the datasets we have,

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# assume we had imported both train and test dataset and we have combined them into on data}

\CommentTok{# If we save the file from the previous code we can load it directly}
\NormalTok{data <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"./data/data.csv"}\NormalTok{, }\DataTypeTok{header =} \OtherTok{TRUE}\NormalTok{)}

\CommentTok{# Check our combined dataset details}
\KeywordTok{glimpse}\NormalTok{(data) }\CommentTok{# compare with str(data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 1,309
## Columns: 12
## $ PassengerId <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, ...
## $ Survived    <int> 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0...
## $ Pclass      <int> 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3...
## $ Name        <fct> "Braund, Mr. Owen Harris", "Cumings, Mrs. John Bradley ...
## $ Sex         <fct> male, female, female, female, male, male, male, male, f...
## $ Age         <dbl> 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 1...
## $ SibSp       <int> 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1...
## $ Parch       <int> 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0...
## $ Ticket      <fct> A/5 21171, PC 17599, STON/O2. 3101282, 113803, 373450, ...
## $ Fare        <dbl> 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.86...
## $ Cabin       <fct> , C85, , C123, , , E46, , , , G6, C103, , , , , , , , ,...
## $ Embarked    <fct> S, C, S, S, S, Q, S, S, S, C, S, S, S, S, S, S, Q, S, S...
\end{verbatim}

We can observe that there are 1309 data records with 12 attributes. We can also see the types and values of each attribute.
We understood the goal of the Titanic problem is to predict given passengers' survival. So, except for the attributes \emph{PassengerID} and the targeted variable \emph{Survived}, there are 10 attributes present in the combined data that are potentially useful. Among them, two variables \emph{Name} and \emph{Ticket} are less useful intuitively and also confirmed from the previous chapter.

Let us focus on solving the data missing problem.

We can define a function missing\_vars, which can get a proportion of values that are missing in each attribute.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Define a function to check missing values}
\NormalTok{missing_vars <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{}
\NormalTok{  var <-}\StringTok{ }\DecValTok{0}
\NormalTok{  missing <-}\StringTok{ }\DecValTok{0}
\NormalTok{  missing_prop <-}\StringTok{ }\DecValTok{0}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(}\KeywordTok{names}\NormalTok{(x))) \{}
\NormalTok{    var[i] <-}\StringTok{ }\KeywordTok{names}\NormalTok{(x)[i]}
\NormalTok{    missing[i] <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(x[, i])}\OperatorTok{|}\NormalTok{x[, i] }\OperatorTok{==}\StringTok{""}\NormalTok{ )}
\NormalTok{    missing_prop[i] <-}\StringTok{ }\NormalTok{missing[i] }\OperatorTok{/}\StringTok{ }\KeywordTok{nrow}\NormalTok{(x)}
\NormalTok{  \}}
\CommentTok{# order   }
\NormalTok{missing_data <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{var =}\NormalTok{ var, }\DataTypeTok{missing =}\NormalTok{ missing, }\DataTypeTok{missing_prop =}\NormalTok{ missing_prop) }\OperatorTok{%>%}\StringTok{ }
\KeywordTok{arrange}\NormalTok{(}\KeywordTok{desc}\NormalTok{(missing_prop))}
\CommentTok{# print out}
\NormalTok{missing_data}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Apply our function to the combined dataset \texttt{data}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{missing_vars}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            var missing missing_prop
## 1        Cabin    1014 0.7746371276
## 2     Survived     418 0.3193277311
## 3          Age     263 0.2009167303
## 4     Embarked       2 0.0015278839
## 5         Fare       1 0.0007639419
## 6  PassengerId       0 0.0000000000
## 7       Pclass       0 0.0000000000
## 8         Name       0 0.0000000000
## 9          Sex       0 0.0000000000
## 10       SibSp       0 0.0000000000
## 11       Parch       0 0.0000000000
## 12      Ticket       0 0.0000000000
\end{verbatim}

\emph{Survived} has 418 missing values that is the \texttt{test} dataset number. Our entire \texttt{test} dataset needs to be filled with that value. It is not an issue.

\emph{Cabin} and \emph{Age} have some significant proportion of missing values, whereas Embarked \& Fare only has 2 and 1 missing values.

We will use Cabin and Age as examples to demonstrate the general methods used to deal with missing values.

\hypertarget{cabin-attribute}{%
\subsection*{Cabin Attribute}\label{cabin-attribute}}


\emph{Cabin} has a large number of missing values. A total of 1014 missing values and 687 missing values in the \texttt{train} dataset counts as 71 percent of the total value. Its prediction power is in serious doubt since it only has a very small number for each cabin. Facing an attribute that has a large percentage of missing values, in most analyses, it will be simply dropped. However, if you think carefully, the missing value may have some reasons, and that reason could be a factor that affects passengers' lives or perished. Therefore, the first thought, which is normally applied to a large number of missing values, is to replace the attribute with another attribute rather than to fill the missing value themselves. In this case, we can create a new attribute called ``\emph{HasCabinNum}'' which only records if \emph{Cabin} values are "" (empty or missing value). It has two values ``\texttt{yes}'' and ``\texttt{no}''. This method is very general. It can be used in any attribute that has a large number of missing values.

Ideally, we should replace the attribute \emph{cabin} with the newly created attribute \emph{HasCabinNum}. However, we find out that the data samples which have the cabin number, cabin number may have some useful information. So, we will keep it in the moment and for later use.

As mentioned in section 4.5, cabin has rich useful information that can be abstracted and used for analysis (or prediction models).

\begin{rmdaction}
Try your self:
Find details of passenger who share cabins and figure out some deatures like social status or relations. Create new featurs linked them with survive values.\\
\end{rmdaction}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{### Dealing with missing values in Cabin}
\CommentTok{# add newly created attribute and assign it with new values}
\NormalTok{data}\OperatorTok{$}\NormalTok{HasCabinNum <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{((data}\OperatorTok{$}\NormalTok{Cabin }\OperatorTok{!=}\StringTok{ ""}\NormalTok{), }\StringTok{"Yes"}\NormalTok{, }\StringTok{"No"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can examine the relation between our newly created cabin replacement's \emph{HasCabinNum} with the attribute \emph{Survival}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Make sure survived is in factor type }
\NormalTok{p1 <-}\StringTok{ }\NormalTok{data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(Survived)) }\OperatorTok{%>%}
\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{factor}\NormalTok{(HasCabinNum), }\DataTypeTok{fill =} \KeywordTok{factor}\NormalTok{(Survived))) }\OperatorTok{+}
\StringTok{   }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{width =} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+}
\StringTok{   }\KeywordTok{xlab}\NormalTok{(}\StringTok{"HasCabinNum"}\NormalTok{) }\OperatorTok{+}
\StringTok{   }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Total Count"}\NormalTok{) }\OperatorTok{+}
\StringTok{   }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"Survived"}\NormalTok{)}\OperatorTok{+}
\StringTok{   }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Newly created HasCabinNum attribute on Survived"}\NormalTok{)}
\CommentTok{# show survive percentage on HasCabinNum }
\NormalTok{p2 <-}\StringTok{ }\NormalTok{data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(Survived)) }\OperatorTok{%>%}
\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{factor}\NormalTok{(HasCabinNum), }\DataTypeTok{fill =} \KeywordTok{factor}\NormalTok{(Survived))) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{position =} \StringTok{"fill"}\NormalTok{, }\DataTypeTok{width =} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{labels =}\NormalTok{ scales}\OperatorTok{::}\NormalTok{percent, }\DataTypeTok{breaks =} \KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_discrete}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Survived"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"HasCabinNum"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Percentage of Survived"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Newly created HasCabinNum attribute (Proportion Survived)"}\NormalTok{)}

\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{05-data-preprocess_files/figure-latex/cabinPro-1} 

}

\caption{Distribution and survival percentage of the newly created HasCabinNum attribute}\label{fig:cabinPro}
\end{figure}

\hypertarget{age-attribute}{%
\subsection*{Age Attribute}\label{age-attribute}}


Now we can tackle the issue of missing values with the age attribute. \emph{Age} is a typical numerical value. There several options for filling the missing values:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Take the mean value to replace the missing value
\item
  Take a random list of ages that maintains the original statistical summary values.
\item
  Use a model to predict values based on the existing values.
\end{enumerate}

Let us look into them one by one, be aware of this if you have multiple options to deal with one attribute, you cannot simply manipulate the original attribute. If you do, the value of the attribute will be altered, so the second option will be never executed since the missing value has been already eliminated.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Take the mean value to replace the missing value. It is the simplest way to impurate the missing value.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# replace missing value in Age with its average}
\NormalTok{ageEverage <-}\StringTok{ }\KeywordTok{summarise}\NormalTok{(data, }\DataTypeTok{Average =} \KeywordTok{mean}\NormalTok{(Age, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))}
\CommentTok{# create a new attribute Age_RE1 and assign it with new values}
\NormalTok{data}\OperatorTok{$}\NormalTok{Age_RE1 <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(data}\OperatorTok{$}\NormalTok{Age), }\KeywordTok{as.numeric}\NormalTok{(ageEverage), }\KeywordTok{as.numeric}\NormalTok{(data}\OperatorTok{$}\NormalTok{Age))}
\CommentTok{# plot newly altered age attribute }
\CommentTok{# Make sure survived is in factor type }
\NormalTok{p1 <-}\StringTok{ }\NormalTok{data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(Survived)) }\OperatorTok{%>%}
\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{factor}\NormalTok{(Age_RE1), }\DataTypeTok{fill =} \KeywordTok{factor}\NormalTok{(Survived))) }\OperatorTok{+}
\StringTok{   }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{width =} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+}
\StringTok{   }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Age_RE1"}\NormalTok{) }\OperatorTok{+}
\StringTok{   }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Total Count"}\NormalTok{) }\OperatorTok{+}
\StringTok{   }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"Survived"}\NormalTok{)}\OperatorTok{+}
\StringTok{   }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Survived value on Age_RE1"}\NormalTok{)}
\CommentTok{# show survive percentage on HasCabinNum }
\NormalTok{p2 <-}\StringTok{ }\NormalTok{data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(Survived)) }\OperatorTok{%>%}
\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{factor}\NormalTok{(Age_RE1), }\DataTypeTok{fill =} \KeywordTok{factor}\NormalTok{(Survived))) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{position =} \StringTok{"fill"}\NormalTok{, }\DataTypeTok{width =} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{labels =}\NormalTok{ scales}\OperatorTok{::}\NormalTok{percent, }\DataTypeTok{breaks =} \KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_discrete}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Survived"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Age_RE1"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Percentage of Survived"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Survived percentage on Age_RE1"}\NormalTok{)}

\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{05-data-preprocess_files/figure-latex/agepro-1} 

}

\caption{Distribution and survival percentage on the Age with missing value filled}\label{fig:agepro}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Take a random number range between \texttt{min} and \texttt{max} age, and keep the mean and standard deviation unchanged.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# calculate the non-NA mean and std}
\NormalTok{mean <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(data[[}\StringTok{"Age"}\NormalTok{]], }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{) }\CommentTok{# take train mean}
\NormalTok{std <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(data[[}\StringTok{"Age"}\NormalTok{]], }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{) }\CommentTok{# take test std}
\CommentTok{# replace NA with a list that maintian the mean and std}
\NormalTok{temp_rnum <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(data}\OperatorTok{$}\NormalTok{Age)), }\DataTypeTok{mean=}\NormalTok{mean, }\DataTypeTok{sd=}\NormalTok{std)}
\CommentTok{# add new attribute Age_RE2}
\NormalTok{data}\OperatorTok{$}\NormalTok{Age_RE2 <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(data}\OperatorTok{$}\NormalTok{Age), }\KeywordTok{as.numeric}\NormalTok{(temp_rnum), }\KeywordTok{as.numeric}\NormalTok{(data}\OperatorTok{$}\NormalTok{Age))}
\KeywordTok{summary}\NormalTok{(data}\OperatorTok{$}\NormalTok{Age_RE2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  -5.219  21.000  28.000  29.483  38.000  80.000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# There are possible negative values too, replace them with positive values}
\NormalTok{data}\OperatorTok{$}\NormalTok{Age_RE2[(data}\OperatorTok{$}\NormalTok{Age_RE2)}\OperatorTok{<=}\DecValTok{0}\NormalTok{] <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(data}\OperatorTok{$}\NormalTok{Age[data}\OperatorTok{$}\NormalTok{Age}\OperatorTok{>}\DecValTok{0}\NormalTok{], }\KeywordTok{length}\NormalTok{(data}\OperatorTok{$}\NormalTok{Age_RE2[(data}\OperatorTok{$}\NormalTok{Age_RE2)}\OperatorTok{<=}\DecValTok{0}\NormalTok{]), }\DataTypeTok{replace=}\NormalTok{F)}
\CommentTok{# check}
\KeywordTok{summary}\NormalTok{(data}\OperatorTok{$}\NormalTok{Age_RE2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    0.17   21.00   28.00   29.59   38.00   80.00
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot newly altered age attribute }
\CommentTok{# Make sure survived is in factor type }
\NormalTok{p1 <-}\StringTok{ }\NormalTok{data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(Survived)) }\OperatorTok{%>%}
\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{factor}\NormalTok{(Age_RE2), }\DataTypeTok{fill =} \KeywordTok{factor}\NormalTok{(Survived))) }\OperatorTok{+}
\StringTok{   }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{width =} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+}
\StringTok{   }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Age_RE2"}\NormalTok{) }\OperatorTok{+}
\StringTok{   }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Total Count"}\NormalTok{) }\OperatorTok{+}
\StringTok{   }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"Survived"}\NormalTok{)}\OperatorTok{+}
\StringTok{   }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Survived value on Age_RE2 attribute"}\NormalTok{)}

\CommentTok{# show survive percentage on HasCabinNum }
\NormalTok{p2 <-}\StringTok{ }\NormalTok{data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(Survived)) }\OperatorTok{%>%}
\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{factor}\NormalTok{(Age_RE2), }\DataTypeTok{fill =} \KeywordTok{factor}\NormalTok{(Survived))) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{position =} \StringTok{"fill"}\NormalTok{, }\DataTypeTok{width =} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{labels =}\NormalTok{ scales}\OperatorTok{::}\NormalTok{percent, }\DataTypeTok{breaks =} \KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_discrete}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Survived"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Age_RE2"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Percentage of Survived"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Survived percentage on Age_RE2 attribute"}\NormalTok{)}

\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{05-data-preprocess_files/figure-latex/AgePro2-1} 

}

\caption{Distribution and survival percentage on the Age with missing value filled with distribution shape maintained}\label{fig:AgePro2}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Using machine generate model to produce new values based on other exiting values
\end{enumerate}

Among many prediction models (later chapters), the \emph{decision tree} is the simplest. It can split data samples into subsets based on many test conditions (called branches) until there are not test conditions to test or there is no sample left untested\footnote{detailed decision tree and prediction model will be covered in the next chapter}.\\
To demonstrate we can use a prediction model to fill the missing values, here we will only use a simple decision tree without any further calibration. Since \emph{Age} is a continuous variable we want to use the method=``anova''\footnote{ANOVA, stands for ``Analysis of variance'', is a statistical model used to analyse the differences among group means in a sample. Decision trees can take many different ways to partition data samples such as \emph{Entropy}, \emph{Gini Index}, \emph{Classification error}, and \emph{ANOVA}. } for our decision tree. So let us build a decision tree on the subset of the data with the age values available, and then replace those that are missing,

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# confirm Age missing values}
\NormalTok{data}\OperatorTok{$}\NormalTok{Age_RE3 <-}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{Age}
\KeywordTok{summary}\NormalTok{(data}\OperatorTok{$}\NormalTok{Age_RE3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
##    0.17   21.00   28.00   29.88   39.00   80.00     263
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Construct a decision tree with selected attributes and ANOVA method}
\NormalTok{Agefit <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(Age_RE3 }\OperatorTok{~}\StringTok{ }\NormalTok{Survived }\OperatorTok{+}\StringTok{ }\NormalTok{Pclass }\OperatorTok{+}\StringTok{ }\NormalTok{Sex }\OperatorTok{+}\StringTok{ }\NormalTok{SibSp }\OperatorTok{+}\StringTok{ }\NormalTok{Parch }\OperatorTok{+}\StringTok{ }\NormalTok{Fare }\OperatorTok{+}\StringTok{ }\NormalTok{Embarked,}
                  \DataTypeTok{data=}\NormalTok{data[}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(data}\OperatorTok{$}\NormalTok{Age_RE3),], }
                  \DataTypeTok{method=}\StringTok{"anova"}\NormalTok{)}
\CommentTok{#Fill AGE missing values with prediction made by decision tree prediction}
\NormalTok{data}\OperatorTok{$}\NormalTok{Age_RE3[}\KeywordTok{is.na}\NormalTok{(data}\OperatorTok{$}\NormalTok{Age_RE3)] <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(Agefit, data[}\KeywordTok{is.na}\NormalTok{(data}\OperatorTok{$}\NormalTok{Age_RE3),])}
\CommentTok{#confirm the missing values have been filled}
\KeywordTok{summary}\NormalTok{(data}\OperatorTok{$}\NormalTok{Age_RE3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    0.17   22.00   27.43   29.63   37.00   80.00
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 <-}\StringTok{ }\NormalTok{data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(Survived)) }\OperatorTok{%>%}
\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{factor}\NormalTok{(Age_RE3), }\DataTypeTok{fill =} \KeywordTok{factor}\NormalTok{(Survived))) }\OperatorTok{+}
\StringTok{   }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{width =} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+}
\StringTok{   }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Age_RE3"}\NormalTok{) }\OperatorTok{+}
\StringTok{   }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Total Count"}\NormalTok{) }\OperatorTok{+}
\StringTok{   }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"Survived"}\NormalTok{)}\OperatorTok{+}
\StringTok{   }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Survived value on Age_RE3 attribute"}\NormalTok{)}

\CommentTok{# show survive percentage on HasCabinNum }
\NormalTok{p2 <-}\StringTok{ }\NormalTok{data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(Survived)) }\OperatorTok{%>%}
\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{factor}\NormalTok{(Age_RE3), }\DataTypeTok{fill =} \KeywordTok{factor}\NormalTok{(Survived))) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{position =} \StringTok{"fill"}\NormalTok{, }\DataTypeTok{width =} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{labels =}\NormalTok{ scales}\OperatorTok{::}\NormalTok{percent, }\DataTypeTok{breaks =} \KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_discrete}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Survived"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Age_RE3"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Percentage of Survived"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Survived percentage on Age_RE3 attribute"}\NormalTok{)}

\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{05-data-preprocess_files/figure-latex/unnamed-chunk-3-1.pdf}
The above three methods can all fill the missing values. Each filled with different values. Depends on the applications you can choose to use any of them. For our prediction problem, I would use the machine predicted since we are doing predicting anyway. So I will tied data with replacing the original \emph{Age} with \emph{Age\_RE3} and removal of the other two extra age attributes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data}\OperatorTok{$}\NormalTok{Age <-}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{Age_RE3}
\NormalTok{data <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(data, }\DataTypeTok{select =} \OperatorTok{-}\KeywordTok{c}\NormalTok{(Age_RE1, Age_RE2, Age_RE3))}
\end{Highlighting}
\end{Shaded}

\hypertarget{fare-attribute}{%
\subsection*{Fare Attribute}\label{fare-attribute}}


Since there was one missing value in \emph{Fare}, we can also see that this person travelled alone, so I can't impurate it, The best solution is replacing it with the mean or median value, or even other values like median in the same class or median from the same embarked port, or age group, etc.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Replacing na with the median value}
\NormalTok{data[}\KeywordTok{is.na}\NormalTok{(data}\OperatorTok{$}\NormalTok{Fare), ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      PassengerId Survived Pclass               Name  Sex  Age SibSp Parch
## 1044        1044       NA      3 Storey, Mr. Thomas male 60.5     0     0
##      Ticket Fare Cabin Embarked HasCabinNum
## 1044   3701   NA              S          No
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data}\OperatorTok{$}\NormalTok{Fare[}\KeywordTok{is.na}\NormalTok{(data}\OperatorTok{$}\NormalTok{Fare)] <-}\StringTok{ }\KeywordTok{median}\NormalTok{(data}\OperatorTok{$}\NormalTok{Fare, }\DataTypeTok{na.rm =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\hypertarget{Embarked}{%
\subsection*{Embarked Attribute}\label{Embarked}}


\emph{Embark} has two missing values. There are two methods to make up these two values: take the mode value, which is the most value at present; or the most likely value. The mode value is \texttt{S} (Southampton), the fact that 70\% of passengers embarked from `S'.

The most likelihood value needs some analysis. Generally, the embarked port reflects a passenger's journey. It is associated with the fare of the ticket. So we could compare the fare of the ticket to see it most likely fit which part of the journey. However, we have noticed that the fare is the original data may provide faulty information since it can be a shared ticket. The fare is also shared with someone. If that is the case we should consider the partner's Embarked port as its most appropriate value.

So we take two steps:
1. find out the passenger has a shared ticket or not. If the ticket is shared then find the travel companion's embarked port and take that as the passenger's embarked port;
2. If the ticket is not shared or shared partner's embarked port is also missing, find out the ticket price per person and compare with other ticket's price per person to allocate the embarked port.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# list the missing records to figure out the fare and the ticket?}
\NormalTok{data[(data}\OperatorTok{$}\NormalTok{Embarked}\OperatorTok{==}\StringTok{""}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\StringTok{"Embarked"}\NormalTok{, }\StringTok{"PassengerId"}\NormalTok{,  }\StringTok{"Fare"}\NormalTok{, }\StringTok{"Ticket"}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     Embarked PassengerId Fare Ticket
## 62                    62   80 113572
## 830                  830   80 113572
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# we want find out if the fare is a single ticket or a group ticket.}
\end{Highlighting}
\end{Shaded}

We can see the two miss records share the same ticket number and the fare. The situation because extremely simple. We don't need to consider other possibilities. The two passengers must travel together. There is no possibility of any other reference can be used to figure out the missing port.

For safety, let us check if there are other passengers share the same ticket number?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# we need to find out is there other passenger share the ticket?}
\NormalTok{data[(data}\OperatorTok{$}\NormalTok{Ticket}\OperatorTok{==}\StringTok{"113572"}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\StringTok{"Ticket"}\NormalTok{, }\StringTok{"PassengerId"}\NormalTok{, }\StringTok{"Embarked"}\NormalTok{, }\StringTok{"Fare"}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     Ticket PassengerId Embarked Fare
## 62  113572          62            80
## 830 113572         830            80
\end{verbatim}

The answer is ``No''. It tells us only the two missing records share the ticket number. So we only need to find out the price (per person) to compare with other prices (per person) to allocate the missing embarked port. The logic is the same journey should bear the same ticket price. To calculate the ticket price (per person), we create an attribute \emph{Fare\_pp}. It is the ticket price divided by the number of the passenger who shares the same ticket. That is the concept of the group ticket. It can also be useful to single out the ``group travel'' vs ``travel alone''.

As matter of fact, the raw data sample already has this concept such as \emph{Sibsp} and \emph{Parch}. We don't know if the \emph{Sibsp} and \emph{Parch} are sharing the same ticket number since the attributes are only numbers. We can imagine the people who travel in the group may not be relatives and they could be simple friends or colleagues. Anyway, in the case of group travel, it is useful to know the group size. So we created another new attribute \emph{Friend\_size} to record the number of passengers who share the same ticket number ie. ``travel in-group''.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# calculate fare_PP per person}
\NormalTok{fare_pp <-}\StringTok{ }\NormalTok{data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(Ticket, Fare) }\OperatorTok{%>%}
\StringTok{  }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{Friend_size =} \KeywordTok{n}\NormalTok{()) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Fare_pp =}\NormalTok{ Fare }\OperatorTok{/}\StringTok{ }\NormalTok{Friend_size)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `summarise()` regrouping output by 'Ticket' (override with `.groups` argument)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data <-}\StringTok{ }\KeywordTok{left_join}\NormalTok{(data, fare_pp, }\DataTypeTok{by =} \KeywordTok{c}\NormalTok{(}\StringTok{"Ticket"}\NormalTok{, }\StringTok{"Fare"}\NormalTok{))}
\NormalTok{data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{((Embarked }\OperatorTok{!=}\StringTok{ ""}\NormalTok{)) }\OperatorTok{%>%}
\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Embarked, }\DataTypeTok{y =}\NormalTok{ Fare_pp)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_hline}\NormalTok{(}\DataTypeTok{yintercept =} \DecValTok{40}\NormalTok{, }\DataTypeTok{col =} \StringTok{"deepskyblue4"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{05-data-preprocess_files/figure-latex/embarkfarepp-1} 

}

\caption{Possible embarked port by value of Fare per person}\label{fig:embarkfarepp}
\end{figure}

From the above plot, we can see that price 40 (per person) is an outlier in embarked group \texttt{S} and \texttt{Q}. However, if they embarked from \texttt{C} the price only just falls into the upper quartile. So, we can reasonably the pare are embarked from \texttt{C}, so we want to assign \texttt{C} to the embarked missing value.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data}\OperatorTok{$}\NormalTok{Embarked[(data}\OperatorTok{$}\NormalTok{Embarked)}\OperatorTok{==}\StringTok{""}\NormalTok{] <-}\StringTok{ "C"}
\end{Highlighting}
\end{Shaded}

Now we have dealt with all the missing values. We could simply run the same code again to confirm the missing values have been fulfilled.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{missing_vars}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            var missing missing_prop
## 1        Cabin    1014    0.7746371
## 2     Survived     418    0.3193277
## 3  PassengerId       0    0.0000000
## 4       Pclass       0    0.0000000
## 5         Name       0    0.0000000
## 6          Sex       0    0.0000000
## 7          Age       0    0.0000000
## 8        SibSp       0    0.0000000
## 9        Parch       0    0.0000000
## 10      Ticket       0    0.0000000
## 11        Fare       0    0.0000000
## 12    Embarked       0    0.0000000
## 13 HasCabinNum       0    0.0000000
## 14 Friend_size       0    0.0000000
## 15     Fare_pp       0    0.0000000
\end{verbatim}

In summary, we have dealt with the 4 discovered missing values. Different approaches and methods are adopted. some of them are simple fulfillment like replacement with mean/median/mode values, others have more complicated processes involved deeper drill-down analysis or even predictions. Depends on the applications, appropriate methods may need multiple trials and exploration.

However, we have discovered one interesting thing that the fare could be shared among multiple passengers (not only the same fare but also the same ticket numbers) see the previous section. It appeared to be the price of a group ticket. It creates confusing information on the fare. So it may be a good idea to re-engineer it into another more useful attribute like \emph{fare\_PP} (Fare per person), see next section.

\hypertarget{attribute-re-engineering}{%
\section{Attribute Re-engineering}\label{attribute-re-engineering}}

In the previous chapter when we do data understanding. Apart from the missing values, we also find some attributes do not make sense or have no prediction power when considering the relationship with survival. for example, we have found \emph{name} has little prediction power. It is illogical to say some survived because he or she has a specific name. However there is title information buried inside the name, The title can potentially useful at least it shows the age addition to the gender.

We have also find other useful information hidden inside some variables. For example, the information about the deck is possibly hidden inside \emph{cabin}. Information about group travel is buried inside of \emph{Ticket} and \emph{Fare} that passengers share same tickets number and fare must travel in a group. It seems that the ticket is a group ticket. Furthermore, we have also found that the group that shares tickets is mostly family members. This is further confirmed by the none \texttt{0} values in the \emph{SibSp} and \emph{Parch} attributes. That hidden information can be very important. We can surface them by attributes' re-engineering.

\hypertarget{title-from-name-attribute}{%
\subsection*{Title from Name attribute}\label{title-from-name-attribute}}


The \emph{Name} is initially believed is useless for predict a passenger's fate. But we have found in it there is information about titles even maybe marriage relations. So our first task in attribute re-engineering is to create a new attribute called \emph{Title}. It is abstracted from \emph{Name}. It is the title of the passenger, which can be extracted from the \emph{Name} attribute using a regular expression.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Abstract Title out}
\NormalTok{ data}\OperatorTok{$}\NormalTok{Title <-}\StringTok{ }\KeywordTok{gsub}\NormalTok{(}\StringTok{'(.*, )|(}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{..*)'}\NormalTok{, }\StringTok{''}\NormalTok{, data}\OperatorTok{$}\NormalTok{Name)}
\NormalTok{ data }\OperatorTok{%>%}
\StringTok{   }\KeywordTok{group_by}\NormalTok{(Title) }\OperatorTok{%>%}
\StringTok{   }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{count}\NormalTok{() }\OperatorTok{%>%}
\StringTok{ }\KeywordTok{arrange}\NormalTok{(}\KeywordTok{desc}\NormalTok{(n))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 18 x 2
## # Groups:   Title [18]
##    Title            n
##    <chr>        <int>
##  1 Mr             757
##  2 Miss           260
##  3 Mrs            197
##  4 Master          61
##  5 Dr               8
##  6 Rev              8
##  7 Col              4
##  8 Major            2
##  9 Mlle             2
## 10 Ms               2
## 11 Capt             1
## 12 Don              1
## 13 Dona             1
## 14 Jonkheer         1
## 15 Lady             1
## 16 Mme              1
## 17 Sir              1
## 18 the Countess     1
\end{verbatim}

We can see that there is a total of 18 different titles. Some of them are commonly used titles and others are less common. Those less commonly used titles have a very small number, mostly, just 1.

We will group the less commonly used titles into \texttt{other} so to balance distribution.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Group those less common title’s into an ‘Other’ category.}
\NormalTok{data}\OperatorTok{$}\NormalTok{Title <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(data}\OperatorTok{$}\NormalTok{Title }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Mr"}\NormalTok{, }\StringTok{"Miss"}\NormalTok{, }\StringTok{"Mrs"}\NormalTok{, }\StringTok{"Master"}\NormalTok{), data}\OperatorTok{$}\NormalTok{Title, }\StringTok{"Other"}\NormalTok{)}

\NormalTok{L<-}\StringTok{ }\KeywordTok{table}\NormalTok{(data}\OperatorTok{$}\NormalTok{Title, data}\OperatorTok{$}\NormalTok{Sex)}
\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(L, }\DataTypeTok{digits =} \DecValTok{2}\NormalTok{, }\DataTypeTok{booktabs =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{caption =} \StringTok{"Title and sex confirmation"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-6}Title and sex confirmation}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & female & male\\
\midrule
Master & 0 & 61\\
Miss & 260 & 0\\
Mr & 0 & 757\\
Mrs & 197 & 0\\
Other & 9 & 25\\
\bottomrule
\end{tabular}
\end{table}

Checking the table of \emph{Title} vs \emph{Sex} shows nothing anomalous.

A stacked bar graph of the newly created attribute suggests it could be quite useful that the difference in survival between `Master' and `Mr' will be something that hasn't been captured by the \texttt{Sex} attribute.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(Survived)) }\OperatorTok{%>%}
\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{factor}\NormalTok{(Title), }\DataTypeTok{fill =} \KeywordTok{factor}\NormalTok{(Survived))) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{position =} \StringTok{"fill"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{labels =}\NormalTok{ scales}\OperatorTok{::}\NormalTok{percent, }\DataTypeTok{breaks =} \KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_discrete}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Survived"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Title"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Survival Percentage"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Title attribute (Proportion Survived)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{05-data-preprocess_files/figure-latex/titlePro-1} 

}

\caption{Survivial percentage onver Title}\label{fig:titlePro}
\end{figure}

\hypertarget{deck-from-cabin-attribute}{%
\subsection*{Deck from Cabin attribute}\label{deck-from-cabin-attribute}}


From our previous analysis, we have found out that the cabin numbers are all start with a letter. It could be a deck number of some sort. If we group cabin numbers with their initial letter, we can then treat the ordinal missing cabin's value records as a separate group.

So, we group all cabin numbers into groups according to their first letter. Create a new attribute with the name \emph{Deck}. and assign records with no cabin number as \emph{U} (no cabin number) for its \emph{Deck} value.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data}\OperatorTok{$}\NormalTok{Cabin <-}\StringTok{ }\KeywordTok{as.character}\NormalTok{(data}\OperatorTok{$}\NormalTok{Cabin)}
\NormalTok{data}\OperatorTok{$}\NormalTok{Deck <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{((data}\OperatorTok{$}\NormalTok{Cabin }\OperatorTok{==}\StringTok{ ""}\NormalTok{), }\StringTok{"U"}\NormalTok{, }\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{Cabin, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\CommentTok{# plot our newly created attribute relation with Survive}
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(data[}\DecValTok{1}\OperatorTok{:}\DecValTok{891}\NormalTok{,], }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Deck, }\DataTypeTok{fill =} \KeywordTok{factor}\NormalTok{(Survived))) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{width =} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Deck number"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Total account"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"Survived"}\NormalTok{)}

\CommentTok{# plot percentage of survive}
\NormalTok{p2 <-}\StringTok{ }\NormalTok{data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(Survived)) }\OperatorTok{%>%}
\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{factor}\NormalTok{(Deck), }\DataTypeTok{fill =} \KeywordTok{factor}\NormalTok{(Survived))) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{position =} \StringTok{"fill"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{labels =}\NormalTok{ scales}\OperatorTok{::}\NormalTok{percent, }\DataTypeTok{breaks =} \KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_discrete}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Survived"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Deck number"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Percentage"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Newly created Deck number (Proportion Survived)"}\NormalTok{)}

\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{05-data-preprocess_files/figure-latex/deckpro-1} 

}

\caption{Survivla vlaue and percentage over newly created Deck attribute}\label{fig:deckpro}
\end{figure}

\hypertarget{extract-ticket-class-from-ticket-number}{%
\subsection*{Extract ticket class from ticket number}\label{extract-ticket-class-from-ticket-number}}


We knew that the values of \emph{Ticket} appear has two major kinds `Letters Numbers' or just `Numbers'. This could be worth extracting. However, just two class is too rough. As suggested during understanding data, we can group tickets by their first letter or number. let us create a \emph{Ticket\_class} to replace \emph{Ticket}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data}\OperatorTok{$}\NormalTok{Ticket <-}\StringTok{ }\KeywordTok{as.character}\NormalTok{(data}\OperatorTok{$}\NormalTok{Ticket)}
\NormalTok{data}\OperatorTok{$}\NormalTok{Ticket_class <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{((data}\OperatorTok{$}\NormalTok{Ticket }\OperatorTok{!=}\StringTok{ " "}\NormalTok{), }\KeywordTok{substr}\NormalTok{(data}\OperatorTok{$}\NormalTok{Ticket, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\StringTok{""}\NormalTok{)}
\NormalTok{data}\OperatorTok{$}\NormalTok{Ticket_class <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data}\OperatorTok{$}\NormalTok{Ticket_class)}

\CommentTok{# plot our newly created attribute relation with Survive}
\NormalTok{p1 <-}\StringTok{ }\NormalTok{data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(Survived)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Ticket_class, }\DataTypeTok{fill =} \KeywordTok{factor}\NormalTok{(Survived))) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{width =} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Ticket_class"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Total account"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"Survived value over Ticket class"}\NormalTok{)}

\CommentTok{# plot percentage of survive}
\NormalTok{p2 <-}\StringTok{ }\NormalTok{data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(Survived)) }\OperatorTok{%>%}
\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{factor}\NormalTok{(Ticket_class), }\DataTypeTok{fill =} \KeywordTok{factor}\NormalTok{(Survived))) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{position =} \StringTok{"fill"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{labels =}\NormalTok{ scales}\OperatorTok{::}\NormalTok{percent, }\DataTypeTok{breaks =} \KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_discrete}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Survived"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Ticket_class"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Percentage"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Survived percentage over Newly created Ticket_class"}\NormalTok{)}

\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{05-data-preprocess_files/figure-latex/ticketclass-1} 

}

\caption{Survival value and percentage over newly created Ticket class}\label{fig:ticketclass}
\end{figure}

Although the plot appeared to have a skewed bi-model shape, its prediction is clearly improved by ticket number.

\hypertarget{travel-in-groups}{%
\subsection*{Travel in Groups}\label{travel-in-groups}}


We have seen that passengers shared ticket numbers and fares. It is a clear indication of the passenger traveling in groups. Travel in groups can be an important factor for survival in disasters. The Titanic movie impressed millions because of the love story about a couple, they want to stay together to live and to death. Generally, that is the spirit of grouping - stay together for worse or for better. Apart from two friends travel together, we have also seen the family travel together that as indicated by \emph{SibSp} and \emph{Parch} attributes.

To make it simple we can create a \emph{Group\_size}, which takes a minimum value of 1 to represent the passenger travel alone. otherwise in groups. The group size is defined as:

\begin{equation} 
Group\_size = Max(Friend\_size, Family\_size).
\label{eq:group}
\end{equation}

where,
\begin{equation} 
Friend\_size = Sum(PassengerID),
\label{eq:friend}
\end{equation}
that share the some ticket number and fare, which we have already created in the section @ref(fare\_pp) when we create new data frame \texttt{Fare\_pp}.
\begin{equation} 
Family\_size = SibSp + Parch + 1
\label{eq:family}
\end{equation}

So we do,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data}\OperatorTok{$}\NormalTok{Family_size <-}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{SibSp }\OperatorTok{+}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{Parch }\OperatorTok{+}\StringTok{ }\DecValTok{1}
\NormalTok{data}\OperatorTok{$}\NormalTok{Group_size <-}\StringTok{ }\KeywordTok{pmax}\NormalTok{(data}\OperatorTok{$}\NormalTok{Family_size, data}\OperatorTok{$}\NormalTok{Friend_size)}
\end{Highlighting}
\end{Shaded}

Now let us see our newly created attribute's prediction power,

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{05-data-preprocess_files/figure-latex/unnamed-chunk-8-1} 

}

\caption{Survival value and percentage over newly created Group Size}\label{fig:unnamed-chunk-8}
\end{figure}

The plot shows that most people traveled alone, small and large groups have the least chance of survival while Medium-sized groups (3 and 4) seemed to have the best chance of living.

\hypertarget{age-in-groups}{%
\subsection*{Age in Groups}\label{age-in-groups}}


We have seen the age has a strong correlation with survival. However, it is too fine granted, it is better to create demographical groups called \emph{Age\_group}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Age_labels <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{'0-9'}\NormalTok{, }\StringTok{'10-19'}\NormalTok{, }\StringTok{'20-29'}\NormalTok{, }\StringTok{'30-39'}\NormalTok{, }\StringTok{'40-49'}\NormalTok{, }\StringTok{'50-59'}\NormalTok{, }\StringTok{'60-69'}\NormalTok{, }\StringTok{'70-79'}\NormalTok{)}

\NormalTok{data}\OperatorTok{$}\NormalTok{Age_group <-}\StringTok{ }\KeywordTok{cut}\NormalTok{(data}\OperatorTok{$}\NormalTok{Age, }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{60}\NormalTok{, }\DecValTok{70}\NormalTok{, }\DecValTok{80}\NormalTok{), }\DataTypeTok{include.highest=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{labels=}\NormalTok{ Age_labels)}

\NormalTok{p1 <-}\StringTok{ }\NormalTok{data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(Survived)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Age_group, }\DataTypeTok{y =}\NormalTok{ ..count.., }\DataTypeTok{fill =} \KeywordTok{factor}\NormalTok{(Survived))) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Survived value ove newly created Age_group"}\NormalTok{)}

\CommentTok{# plot percentage of survive}
\NormalTok{p2 <-}\StringTok{ }\NormalTok{data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(Survived)) }\OperatorTok{%>%}
\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Age_group, }\DataTypeTok{fill =} \KeywordTok{factor}\NormalTok{(Survived))) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{position =} \StringTok{"fill"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{labels =}\NormalTok{ scales}\OperatorTok{::}\NormalTok{percent, }\DataTypeTok{breaks =} \KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_discrete}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Survived"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Age group"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Percentage"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Survived percentage ove newly created Age_group"}\NormalTok{)}

\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{05-data-preprocess_files/figure-latex/agegroup-1} 

}

\caption{Survival value and percentage over newly created Age Group}\label{fig:agegroup}
\end{figure}

We can see here only age group ``0-9'' has a better chance of survive.

\hypertarget{farepp}{%
\subsection*{Fare per passenger}\label{farepp}}


We have used this concept when we fill the missing value of \emph{Embarked} in Section 5.2. We were comparing the records' fare with other passengers' fare because we believe the fare should reflect the journey that should indicate the embarked port. It is there we find out the passenger could share the fare and the ticket number. So it is faulty information if you only considering \emph{Fare} values between two passengers. After we introduce a new attribute \emph{Fare\_pp} that stands for fare per person, its value is the true value a passenger paid for the travel.

So we have,
\begin{equation} 
Fare\_PP = Fare / Friend\_size.
\label{eq:farepp}
\end{equation}

We do this,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data}\OperatorTok{$}\NormalTok{Fare_pp <-}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{Fare}\OperatorTok{/}\NormalTok{data}\OperatorTok{$}\NormalTok{Friend_size}
\end{Highlighting}
\end{Shaded}

Let us examine our newly created attribute \emph{Fare\_PP}'s prediction power,

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot Fare_PP against Survived}
\NormalTok{p1<-}\StringTok{ }\NormalTok{data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(Survived)) }\OperatorTok{%>%}
\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Fare_pp, }\DataTypeTok{fill =} \KeywordTok{factor}\NormalTok{(Survived))) }\OperatorTok{+}\StringTok{ }
\StringTok{ }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \DecValTok{2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{breaks =} \KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{500}\NormalTok{, }\DecValTok{50}\NormalTok{)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale_fill_discrete}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Survived"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Fare (per person)"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Count"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Survived value over Fare_pp"}\NormalTok{)}
\NormalTok{p1}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{05-data-preprocess_files/figure-latex/fareperperson-1} 

}

\caption{Survival value and percentage over newly created Fare per person}\label{fig:fareperperson-1}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot percentage of survive}
\NormalTok{p2 <-}\StringTok{ }\NormalTok{data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(Survived)) }\OperatorTok{%>%}
\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{factor}\NormalTok{(Fare_pp), }\DataTypeTok{fill =} \KeywordTok{factor}\NormalTok{(Survived))) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{position =} \StringTok{"fill"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{labels =}\NormalTok{ scales}\OperatorTok{::}\NormalTok{percent, }\DataTypeTok{breaks =} \KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_discrete}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Survived"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Fare per person"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Percentage"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Survived rate over newly created Fare_PP"}\NormalTok{)}
\NormalTok{p2}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{05-data-preprocess_files/figure-latex/fareperperson-2} 

}

\caption{Survival value and percentage over newly created Fare per person}\label{fig:fareperperson-2}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot in box plot}
\NormalTok{data }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(Survived)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(Fare }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{) }\OperatorTok{%>%}
\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\KeywordTok{factor}\NormalTok{(Survived), Fare_pp)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_boxplot}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{trans =} \StringTok{"log2"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{show.legend =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_jitter}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{05-data-preprocess_files/figure-latex/fareperperson2-1} 

}

\caption{Survival value over newly created Fare per person by boxplot}\label{fig:fareperperson2}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# grid.arrange(p1, p2, ncol = 2)}
\end{Highlighting}
\end{Shaded}

The graph confirms the fare\_PP associated with the passenger's survival. We can see that the perished passenger tend to pay less (around 8 pounds) and the average survived passenger appeared paid something around 14 pounds.

\hypertarget{build-re-engineered-dataset}{%
\section{Build Re-engineered Dataset}\label{build-re-engineered-dataset}}

We have done many things:

\begin{itemize}
\tightlist
\item
  unified the \texttt{test} dataset with \texttt{train} dataset
\item
  transformed some data types
\item
  make up and filled the missing values for some attributes
\item
  re-engineered some attributes, and
\item
  created some new attributes
\end{itemize}

Let us look at our dataset attributes,

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{glimpse}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 1,309
## Columns: 21
## $ PassengerId  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,...
## $ Survived     <int> 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, ...
## $ Pclass       <int> 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, ...
## $ Name         <fct> "Braund, Mr. Owen Harris", "Cumings, Mrs. John Bradley...
## $ Sex          <fct> male, female, female, female, male, male, male, male, ...
## $ Age          <dbl> 22.00000, 38.00000, 26.00000, 35.00000, 35.00000, 27.4...
## $ SibSp        <int> 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, ...
## $ Parch        <int> 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, ...
## $ Ticket       <chr> "A/5 21171", "PC 17599", "STON/O2. 3101282", "113803",...
## $ Fare         <dbl> 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8...
## $ Cabin        <chr> "", "C85", "", "C123", "", "", "E46", "", "", "", "G6"...
## $ Embarked     <fct> S, C, S, S, S, Q, S, S, S, C, S, S, S, S, S, S, Q, S, ...
## $ HasCabinNum  <chr> "No", "Yes", "No", "Yes", "No", "No", "Yes", "No", "No...
## $ Friend_size  <int> 1, 2, 1, 2, 1, 1, 2, 5, 3, 2, 3, 1, 1, 7, 1, 1, 6, 1, ...
## $ Fare_pp      <dbl> 7.250000, 35.641650, 7.925000, 26.550000, 8.050000, 8....
## $ Title        <chr> "Mr", "Mrs", "Miss", "Mrs", "Mr", "Mr", "Mr", "Master"...
## $ Deck         <chr> "U", "C", "U", "C", "U", "U", "E", "U", "U", "U", "G",...
## $ Ticket_class <fct> A, P, S, 1, 3, 3, 1, 3, 3, 2, P, 1, A, 3, 3, 2, 3, 2, ...
## $ Family_size  <dbl> 2, 2, 1, 2, 1, 1, 1, 5, 3, 2, 3, 1, 1, 7, 1, 1, 6, 1, ...
## $ Group_size   <dbl> 2, 2, 1, 2, 1, 1, 2, 5, 3, 2, 3, 1, 1, 7, 1, 1, 6, 1, ...
## $ Age_group    <fct> 20-29, 30-39, 20-29, 30-39, 30-39, 20-29, 50-59, 0-9, ...
\end{verbatim}

We can see there are 21 attributes in total. Compare with the 12 attributes in the original raw dataset, there are 9 newly added contributes. They have enriched the original attributes but some re-engineered attributes are leftover such as \emph{Name} and \emph{Cabin} (too many missing values). \emph{Name} has been transformed into \emph{Title} and \emph{Cabin} has been transformed into \emph{HasCabinNum} and \emph{Deck}.

Clearly, we need to clean up or remove redundant attributes. For some re-engineered attributes like \emph{Deck} effectively is derived from \emph{Cabin}. With the \emph{Deck} in place, \emph{Cabin} has no need to exist. Effectively, lose \emph{Cabin} will not lose any information. \emph{Fare} provides misleading information because it only keeps the amount of money paid for a ticket but does not specify the amount is for group fare or single fare. So \emph{Fare\_PP} is the accurate replacement of the \emph{Fare}. \emph{Family\_size} is derived from \emph{Sibsp} and \emph{Parch}, they are containment relations, if you want fine grant analysis, you can keep all of them. \emph{Friend\_size} was introduced when we calculate the ticket price. That is a person who paid for the ticket. \emph{Friend\_size} is different from the \emph{Family\_size} because the Friend\_size is simply the passenger who shares the same ticket number. There is no way to know if they are a family member. At the same time, ``Family\_size'' does not ensure the sharing of the ticket. \emph{Ticket\_class} is derived from the \emph{Ticket} number. It is a kind of grouping of the ticket. Finally, the \emph{Age\_group} is a similar concept that groups the \emph{Age} attribute.

Therefore, we could keep our re-engineered dataset as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RE_data <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(data, }\DataTypeTok{select =} \OperatorTok{-}\KeywordTok{c}\NormalTok{(Name, Cabin, Fare))}
\end{Highlighting}
\end{Shaded}

Our dataset now have the following attributes:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{glimpse}\NormalTok{(RE_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 1,309
## Columns: 18
## $ PassengerId  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,...
## $ Survived     <int> 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, ...
## $ Pclass       <int> 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, ...
## $ Sex          <fct> male, female, female, female, male, male, male, male, ...
## $ Age          <dbl> 22.00000, 38.00000, 26.00000, 35.00000, 35.00000, 27.4...
## $ SibSp        <int> 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, ...
## $ Parch        <int> 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, ...
## $ Ticket       <chr> "A/5 21171", "PC 17599", "STON/O2. 3101282", "113803",...
## $ Embarked     <fct> S, C, S, S, S, Q, S, S, S, C, S, S, S, S, S, S, Q, S, ...
## $ HasCabinNum  <chr> "No", "Yes", "No", "Yes", "No", "No", "Yes", "No", "No...
## $ Friend_size  <int> 1, 2, 1, 2, 1, 1, 2, 5, 3, 2, 3, 1, 1, 7, 1, 1, 6, 1, ...
## $ Fare_pp      <dbl> 7.250000, 35.641650, 7.925000, 26.550000, 8.050000, 8....
## $ Title        <chr> "Mr", "Mrs", "Miss", "Mrs", "Mr", "Mr", "Mr", "Master"...
## $ Deck         <chr> "U", "C", "U", "C", "U", "U", "E", "U", "U", "U", "G",...
## $ Ticket_class <fct> A, P, S, 1, 3, 3, 1, 3, 3, 2, P, 1, A, 3, 3, 2, 3, 2, ...
## $ Family_size  <dbl> 2, 2, 1, 2, 1, 1, 1, 5, 3, 2, 3, 1, 1, 7, 1, 1, 6, 1, ...
## $ Group_size   <dbl> 2, 2, 1, 2, 1, 1, 2, 5, 3, 2, 3, 1, 1, 7, 1, 1, 6, 1, ...
## $ Age_group    <fct> 20-29, 30-39, 20-29, 30-39, 30-39, 20-29, 50-59, 0-9, ...
\end{verbatim}

In order to preserve our re-engineered dataset, it is a good idea to save it back to hard drive. So it can be used later in the data analysis.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{write.csv}\NormalTok{(RE_data, }\DataTypeTok{file =} \StringTok{"./data/RE_Data.CSV"}\NormalTok{, }\DataTypeTok{row.names =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{summary-3}{%
\section*{Summary}\label{summary-3}}


In this chapter, based on the previous chapter on \textbf{Data Understanding}, we have demonstrated some basic tasks needed to perform in the step of the data preprocess or data preparation. Those tasks are either resulted from the initial data quality assessment like discover the missing values or demanded by the next step of data analyses like correlation analyses to order the potential predictors based on the prediction power. Attributes re-engineering is the task to make the maximum use of the information contained in the given dataset or transform give attributes in the most appropriate form or types. The ultimate goal is to make datasets ready for analysis.

\hypertarget{exercises-4}{%
\section*{Exercises}\label{exercises-4}}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Discuss the advantage and disadvantage of fill \emph{Age} missing value with a sample that has the same \texttt{mean} and \texttt{std}.
\item
  Re-engineer \emph{Cabin} to capture social status and relations by creating new features to reflect more accurate relations with \emph{survive}. Discuss their generalisation and specification.
\item
  When we make up missing values of the \emph{Embarked} attribute we want to compare the price of the ticket the passenger paid with other tickets' price to allocate the possible embarked port. It all works well, however, one of the factors we did not consider is the variation of the price on \emph{Pclass}. We have knowledge that the higher class the more expensive the price will be. Can you analyze the price per ticket with the \emph{Pclass} to see if it can produce conflict results against the allocation of the embarked port by price comparison?
\end{enumerate}

\hypertarget{data-analysis}{%
\chapter{Data Analysis}\label{data-analysis}}

\begin{center}\includegraphics[width=0.6\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/images/data-analysis} \end{center}

\begin{quote}
``Doing data science is building models.''

\begin{verbatim}
                                   --- David Langer
\end{verbatim}
\end{quote}

Data analysis is the key task of any data science project. It can be done with many methods depending on the goal of the analysis\footnote{This chapter provides basic concepts and foundations for predictive analytics if you already know them to jump to chapter 8}. The three often used methods are \textbf{Descriptive analysis}, \textbf{Exploratory analysis}, and \textbf{Predictive analysis} as we mention in the section \ref{analyse} of chapter one. However, the dominant view of the data analysis is \textbf{model building}. Model building\indenx{Model building} is a term used often in predictive analytics\index{predictive analytics}. Predictive analytics is a data analysing method that encompasses a variety of statistical techniques from data mining, predictive modelling, and machine learning together analyse current and historical facts to make predictions about future or otherwise unknown events.

Building a model is to provide a simple summary of a given dataset to reflect the data ``signals'' or ``patterns'' that are buried inside of the observed data samples. This summary sometimes called mapping. is formulated into a model. This model, once learned or constructed, can then be used to predict a future trend or particular values of a dependent variable. model building is also a frequently used term in \textbf{``Machine Learning''} where a machine learns a model from the observed dataset, and after verification, the model can be used for prediction on a new dataset.

\hypertarget{predictive-data-analysis-pda}{%
\section{Predictive Data Analysis (PDA)}\label{predictive-data-analysis-pda}}

Human has a long history of obsessed with production. A magic crystal ball was used to predict the future.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/images/Prediction} 

}

\caption{Human is obsessed with prediction using a crystal ball.}\label{fig:prediction}
\end{figure}

In data science, \textbf{Predictive data analysis (PDA)} can not be accomplished alone. It needs to encompass both of DDA and EDA, to analyse historical and current data, and then to make predictions about future or unknown data.

A classic example of a predictive model is customer scoring as shown in Figure \ref{fig:modelexam}. The customer scoring model factors together individual customer's attributes (properties or attributes), weight them, and adds them up to produce an overall score.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/images/examplemodel} 

}

\caption{Example of predictive model for customer score}\label{fig:modelexam}
\end{figure}

The typical way to build a predictive model is through training on the training dataset. The model built is then tested with the testing dataset, so its performance can be tested and evaluated, improved to satisfactory. Finally, the model can be applied to the unknown datasets for predictions and applications.

\hypertarget{process-of-predcitive-data-analysis}{%
\section{Process of Predcitive Data Analysis}\label{process-of-predcitive-data-analysis}}

The process of predictive data analysis is called \textbf{predictive modelling}. It is generally involves three steps:

\begin{itemize}
\tightlist
\item
  \textbf{Predictor selection},
\item
  \textbf{model construction}, and
\item
  \textbf{model evaluation}.
\end{itemize}

\hypertarget{predictor-selection}{%
\subsection*{Predictor Selection}\label{predictor-selection}}


A predictor, in data science, is an attribute that a prediction model used to predict values of another attribute. The attribute to be predicted is called \textbf{consequencer} (or \textbf{dependent}, or \textbf{response}, we may use them exchangeable). Generally, a data object can have a large number of attributes, which can potentially be used as predictors by a model to produce consequencer. Most models do not use all of the data attributes, instead only a number of selected attributes are used.

The selection is based on the relationship between the predictor and the consequencer and also the relationship among the predictors. \textbf{Filter} and \textbf{wrapper} are the most common methods used in the attributes selection:

\begin{itemize}
\item
  \textbf{Filters}. The filter is a method that examines each predictor in turn. A numerical measure is calculated, representing the strength of the correlation\footnote{Correlation, in statistics, is a measurement of any statistical relationship between two attributes. It can be any association. It commonly refers to the degree to which a pair of attributes is linearly related.} between the predictor attribute and the consequencer. This correlation is conventionally called the prediction power of a predictor in the prediction modelling. The only predictor attributes where the correlation measure\footnote{The most commonly used measurement of the correlation between two attributes is the ``Pearson's correlation coefficient'', commonly called simply ``the correlation coefficient''.} exceeds a given threshold are selected or simply select the fixed number of the top attributes which has higher correlation measure.
\item
  \textbf{Wrappers}. A wrapper takes a group of predictors and considers the ``value add'' of each attribute compared to other attributes in the group. If two attributes tell you more or less the same thing (e.g.~age and date of birth) then one will be discarded because it adds no value. Step-wise linear regression\footnote{In statistics, step-wise linear regression is a method of fitting regression models in which the selection of predictors is carried out by a procedure that in each step, one attribute is considered for addition to or subtraction from the set of selected attributes based on some pre-specified criterion.} and principal component analysis\footnote{Principal component analysis (PCA) is the process of computing the principal components and using only the first few principal components and ignoring the rest in a prediction or data dimension reduction. The principal components are often computed by eigendecomposition of the data covariance matrix or singular value decomposition of the data matrix.} two popular wrapper methods.
\end{itemize}

\hypertarget{model-construction}{%
\subsection*{Model Construction}\label{model-construction}}


Model construction is the centre of the data analysing. It normally involves two phases: \textbf{Induction} and \textbf{Deduction}.

\begin{itemize}
\item
  Induction\index{Induction} is also called model learning \index{model learning}, which means learn to predict;
\item
  Deduction\index{Deduction} is called model application\index{model application}, which means model applied to predict.
\end{itemize}

The division of model learn and model application allows a predictive model to be mature while induction using \emph{training dataset} to construct a model and deduction using \emph{testing dataset} to test and adjust the model constructed.

There are many predictive models that exist for different purposes. Many different methods can be used to create a model, and more are being developed all the time. Three broad predictive models based on the model format and the way it is built are \textbf{Math model}, \textbf{Rule-based model}, and \textbf{Machine Learning model}.

\hypertarget{math-model}{%
\subsubsection*{Math model}\label{math-model}}


Mathematically formulated model\index{Math model} is the model produced by a mathematical formula that combines multiple predictors (attributes) to predict a response (we called it targeted attribute). A predictor is a single attribute in a data object that contributes to the result of the prediction, which is a consequencer (also called dependents in the same applications).

A well-known example of a math model is \emph{Regression model}. A linear regression model\index{linear regression model} is a target function \(f\) that maps each attribute set \(X\) into a continuous-valued output \(y\) with minimum error.

\begin{equation} 
  y = f(x) = f(x)= ω_1 x+ω_0,
  \label{eq:binom}
\end{equation}

where \(ω_0\) and \(ω_1\) are parameters of the model and are called the \emph{regression coefficients}. The model is to find the parameters \((ω_1, ω_0)\) that minimize the sum of the squared error (SSE),

\begin{equation} 
 SSE= \Sigma^{N}_{i=1}[y_i-f(x_i)]^2 = \Sigma^{N}_{i=1}[y_i - ω_1 x + ω_0 ]^2
  \label{eq:sse}
\end{equation}

Clearly, the linear regression is very simple, its prediction is also limited. So you can have more complicated models like \emph{Logistic Regression} and \emph{Support vector machine (SVM)}.

\hypertarget{rule-based-model}{%
\subsubsection*{Rule-based model}\label{rule-based-model}}


In a rule-based model\index{Rule-based model}, the model is a collection of rules. For example, a model for customer retention maybe something like,

\texttt{If\ the\ customer\ is\ rural,\ and\ her\ monthly\ usage\ is\ high,\ then\ the\ customer\ will\ probably\ renew.}

In a rule-based model, a model is a collection of \texttt{if\ \ldots{}\ then\ \ldots{}} rules. The list below shows an example of a classification model generated by a rule-based classifier for the vertebrate classification problem.

\begin{equation} 
r_1:  (Gives Birth = no) ∧ (Aerial Creature = yes) → Birds\\
r_2:    (Gives Birth = no) ∧ (Aquatic Creature = yes) → Fishes\\
r_3:    (Gives Birth = yes) ∧ (Body Temperature = warm-blooded) → Mammals\\
r_4:    (Gives Birth = no) ∧ (Aerial Creature = no) → Reptiles\\
r_5:    (Aquatic Creature = semi) → Amphibians
\end{equation}

The rules for the model are represented in a disjunctive normal form \(R=(r_1 \vee r_2\vee … \vee r_k)\), where \(R\) is known as the rule set and \(r_i\) are the model rules.
Each rule is expressed in a form of:

\begin{equation} 
r_i:   (Condition_i) →  y_i.
  \label{eq:rule}
\end{equation}

The left-hand side of the rule is called the \textbf{rule antecedent or precondition}. It contains a conjunction of attribute test:

\begin{equation} 
condition_i = (A_1\quad op\quad v_1 ) ∧ (A_2\quad op\quad v_2 ) ∧ … ∧(A_k\quad  op\quad v_k ),
  \label{eq:condition}
\end{equation}

Where \((A_j\quad op\quad v_j )\) is an attribute-value pair and \(op\) is a relation operator chosen from the set \(\{ =, ≠, <, >, ≤, ≥ \}\). Each attribute test \((A_j\quad op \quad v_j )\) is known as a conjunct. The right hand of the rule is called the rule consequent which contains the value of conseqencer \(y_i\).

\hypertarget{machine-learning-model}{%
\subsubsection*{Machine Learning Model}\label{machine-learning-model}}


In many applications, the relationship between the predictor and the consequencer is non-deterministic or is too difficult to either formulate a model or figure out rules by humans. In these cases, advanced technologies are used to generate prediction models automatically taking advantage of massive computer storage and fast computation power of distributed and cloud-based computing infrastructure. The models used in these situations are mostly mathematical formulas and even Neural Networks (NN). Expression \eqref{eq:ML} is a good illustration.

\begin{equation} 

Input → f(w_1,w_2, ...,w_n) → Output

\label{eq:ML}
\end{equation}

In machine learning, different predictive models are utilized and tested to produce a valid prediction such as \textbf{regression}, \textbf{decision tree}, and \textbf{decision forest}, etc.

The Machine Learning approach also takes a \emph{``black box''} approach that is ignoring the detailed transformation between predictors and the consequence, and simply simulating input and out through NN. Neural Network modelling heavily relies on features engineering that is feature extraction and feature selection. One way to overcome this problem is an approach called \emph{Deep Leaning}. Deep learning is built based on the concept of NN and adds extra layers between the input and the output layers. Figure \ref{fig:diff} shows the differences between Machine Learning and Deep Learning.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/images/Diff-ML-DL} 

}

\caption{Comparison between Machine Learning & Deep Learning}\label{fig:diff}
\end{figure}

There are three important types of neural networks that are also called pre-trained models in deep learning: \emph{Artificial Neural Networks (ANN)}, \emph{Convolution Neural Networks (CNN)}, and \emph{Recurrent Neural Networks (RNN)}. We will not use them but it is good to understand what are they.

In practice, predictive models used in a Data Science project are mostly mathematics formulated models. They are implemented in different computer programs with different languages. They are normally packed into integrated software packages. This software undertakes a mixture of training data and goes through number crunching, parameter adjustment, and error correction, which normally called trial or training, and finally produces a working prediction model. During the process of machine generating model, human involvement is much less but needed. It enables fine-tune the model and improving its performance.
\#\#\# Model Validation \{-\}

A major problem in building models is that it is relatively easy to build a prediction model but it is not easy to prove the model is useful. That is to say, finding relationships that exist and are the result of random patterns in the training and testing datasets, but this relationship may not exist in the unseen datasets. Model validation is the task of confirming that the outputs of a model have enough fidelity to the outputs of the model building process that the objectives of the model can be achieved.

In practice, a model can be \emph{over-fitted} or \emph{under-fitted}. An over-fitted model can perform extremely well in the test with the test dataset but perform significantly worse with the new unseen dataset. In other words, the over-fitted model remembers a huge number of examples from the training dataset instead of learning to notice features of the training dataset. On other hand, an under-fitted model misses some features or patterns that exist in the training dataset. Under-fitting would occur, for example, when fitting a linear model to non-linear data. Both over-fitted and under-fitted models will tend to have poor predictive performance.

To determine if over-fitting has occurred, the model needs to be tested on ``\emph{validation dataset}''. Validation datasets are a subset from the given datasets that have targeted attributes values. This subset was not used to construct the model. The validation dataset is genially taken from the training datasets with a certain percentage.

Over-fitting is quite common and this is not necessarily a problem. However, if the degree of over-fitting is large, the model may need to be reconstructed using a different set of attributes.

Apart from checking the model's over-fitting, Depends on the model being constructed, there a number of evaluation methods are available to perform the model validation such as \emph{Confusion Matrix} for nominal output like class labels, \emph{AUC (Area Under Curve)}, \emph{Accuracy} and other evaluation metrics are used for evaluating different models.

\hypertarget{classification-as-a-specific-prediction}{%
\section{Classification as A Specific Prediction}\label{classification-as-a-specific-prediction}}

Classification is the simplest form of predictive analysis. In classification, the model is called \emph{classifier}. It is built by a process called \emph{training} using a training dataset. The training dataset always has the targeted values, which is also called \emph{class label}. The training is to build a matching model that can map the input data to the class label. The use of the model, now called a classifier, is to predict class labels for new data. Before a classifier can be used, it needs to go through a process called testing or verification. In the testing, the test dataset is used, which the targeted value or labels are not there. The most commonly used classifiers are \textbf{Decision tree}, \textbf{Random Forest}, \textbf{regression models}, and \textbf{Gaussian Naive Bayes}.

We will use these classifiers to solve our Titanic problem. It is to predict a passenger's death or survival based on the dataset we have. The Titanic prediction problem is even a simpler classification problem because it only has two possibilities results. This kind of classification is also called \textbf{binary classification}\index{binary classification}. Because we only need to classify a passenger either belongs to a survived class or perished class.

\hypertarget{summary-4}{%
\section*{Summary}\label{summary-4}}


Predictive data analysis is the most advanced data analysis method. It largely overlaps with model building and machine learning. In many cases, predictive analysis is a model building. The process of building a model has three steps: predictor selection, model construction, and model validation.

Until a model is fine-tuned and validated, it can be put to use since it may have been under-fitted or over-fitted. Classification is the simplest prediction. The prediction model in classification is called classifier and the consequencer or dependent is called Class label.

Prediction can never be 100 percent accurate because of the unknown. Therefore prediction model can always be improved once a mistake is made or new data becomes available. There is a new concept that emerged in the last few years called ``continue learning'' or ``life-long learning''. It emphasizes the point of model construction is a continuous process.

\hypertarget{predictor-selection-1}{%
\chapter{Predictor Selection}\label{predictor-selection-1}}

\begin{center}\includegraphics[width=0.8\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/images/selection} \end{center}

\begin{quote}
``Transformation occurs when there has been a learning lesson and you choose to create a better choice.''

\begin{verbatim}
                          -- Andrea Reibmayr 
\end{verbatim}
\end{quote}

In the previous chapter, we have established the idea that predictive data analysis (PDA) is a kind of model building. In its three-step process, the first step is the predictor selection. This is because most prediction model does not use all the attributes of the data samples. An only a small amount of predictors are used in a model. Therefore before a model can be constructed or trained, it is necessary to select predictors\footnote{if you are familiar with association and PCA analyses, you can jump to chapter 8}.

\hypertarget{predictor-selection-pricinples}{%
\section{Predictor Selection Pricinples}\label{predictor-selection-pricinples}}

Selecting predictors needs to answer two questions: how many predictors and which one to be selected. It is a very complicated issue that not only depends on the attributes but also the model to be constructed. The latter can only be clear after the model construction. Let us focus on the first factor that is the attributes themselves.

Predictor selection when considering the attributes, the principle is:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Select as little as possible. Since more predictors may increase the computation cost of a model and may reduce the model's performance by introducing noise and outliers.
\item
  Do not select attributes that do not have prediction power. the prediction power refers to the influence or impacts a predictor on the dependent variable.
\item
  Do not select attributes that do not provide extra information. Some attributes are strongly correlated or have Collinearity\footnote{A phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.}. In this case, only one predictor from them is enough.
\item
  Always choose predictors to follow the order of the prediction power. That is select the attribute that has the most prediction power and then the second and the third, so on so forth.
\end{enumerate}

\hypertarget{attributes-analysis}{%
\section{Attributes Analysis}\label{attributes-analysis}}

To obtain the attributes' prediction power and the correlation among them, the basic analytic tasks need to be performed. These analytic tasks include \emph{Correlation Analysis}, \emph{Principal component analysis (PCA)}, and \emph{Possibly factor analysis (FA)}.

\begin{itemize}
\item
  \textbf{Correlation Analysis}\index{Correlation Analysis}. Analysis correlation among the attributes, and ordering them based on the correlation of attributes with the dependent attribute. Select an appropriate number of the attributes from the highest value towards the lowest value of correlation.
\item
  \textbf{Principal component analysis (PCA)}\index{Principal component analysis}. PCA is a dimension reduction method by projecting each data point onto only the first few principal components to obtain lower-dimensional data while preserving as much of the data's variation as possible. PCA only works on numerical variables.
\item
  \textbf{Possibly factor analysis (FA)}\index{Factor analysis}. Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors. For example, it is possible that variations in six observed variables mainly reflect the variations in two unobserved (underlying) variables.
\end{itemize}

There are other similar tasks such as \emph{MCA}, \emph{FAMD}, \emph{CA}, and \emph{MFA}. MCA\index{multiple correspondence analysis} stands for multiple correspondence analysis. It can only apply to categorical variables; FAMD\index{factor analysis of mixed data} stands for factor analysis of mixed data. It can apply to both numerical and categorical variables; CA\index{correspondence analysis} is correspondence analysis, it can only work on two variables (contingency table); MFA\index{multiple factor analysis} is multiple factor analysis, it is needed only when you have variables set by the group. These tasks are all species of the PCA.

In this chapter, we will demonstrate the basic Correlation analysis and principal component analysis to understand the relationship among attributes and between the predictor and the dependent variable. We will continue to use the Titanic example.

\hypertarget{attributes-correlation-analysis}{%
\section{Attributes Correlation Analysis}\label{attributes-correlation-analysis}}

We have re-engineered the Titanic dataset. So instead of using the original dataset, let us consider the correlation among attributes of our re-engineered dataset.

\begin{verbatim}
## Rows: 1,309
## Columns: 18
## $ PassengerId  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,...
## $ Survived     <int> 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, ...
## $ Pclass       <int> 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, ...
## $ Sex          <fct> male, female, female, female, male, male, male, male, ...
## $ Age          <dbl> 22.00000, 38.00000, 26.00000, 35.00000, 35.00000, 27.4...
## $ SibSp        <int> 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, ...
## $ Parch        <int> 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, ...
## $ Ticket       <fct> A/5 21171, PC 17599, STON/O2. 3101282, 113803, 373450,...
## $ Embarked     <fct> S, C, S, S, S, Q, S, S, S, C, S, S, S, S, S, S, Q, S, ...
## $ HasCabinNum  <fct> No, Yes, No, Yes, No, No, Yes, No, No, No, Yes, Yes, N...
## $ Friend_size  <int> 1, 2, 1, 2, 1, 1, 2, 5, 3, 2, 3, 1, 1, 7, 1, 1, 6, 1, ...
## $ Fare_pp      <dbl> 7.250000, 35.641650, 7.925000, 26.550000, 8.050000, 8....
## $ Title        <fct> Mr, Mrs, Miss, Mrs, Mr, Mr, Mr, Master, Mrs, Mrs, Miss...
## $ Deck         <fct> U, C, U, C, U, U, E, U, U, U, G, C, U, U, U, U, U, U, ...
## $ Ticket_class <fct> A, P, S, 1, 3, 3, 1, 3, 3, 2, P, 1, A, 3, 3, 2, 3, 2, ...
## $ Family_size  <int> 2, 2, 1, 2, 1, 1, 1, 5, 3, 2, 3, 1, 1, 7, 1, 1, 6, 1, ...
## $ Group_size   <int> 2, 2, 1, 2, 1, 1, 2, 5, 3, 2, 3, 1, 1, 7, 1, 1, 6, 1, ...
## $ Age_group    <fct> 20-29, 30-39, 20-29, 30-39, 30-39, 20-29, 50-59, 0-9, ...
\end{verbatim}

\begin{verbatim}
##   PassengerId      Survived          Pclass          Sex           Age       
##  Min.   :   1   Min.   :0.0000   Min.   :1.000   female:466   Min.   : 0.17  
##  1st Qu.: 328   1st Qu.:0.0000   1st Qu.:2.000   male  :843   1st Qu.:22.00  
##  Median : 655   Median :0.0000   Median :3.000                Median :27.43  
##  Mean   : 655   Mean   :0.3838   Mean   :2.295                Mean   :29.63  
##  3rd Qu.: 982   3rd Qu.:1.0000   3rd Qu.:3.000                3rd Qu.:37.00  
##  Max.   :1309   Max.   :1.0000   Max.   :3.000                Max.   :80.00  
##                 NA's   :418                                                  
##      SibSp            Parch            Ticket     Embarked HasCabinNum
##  Min.   :0.0000   Min.   :0.000   CA. 2343:  11   C:272    No :1014   
##  1st Qu.:0.0000   1st Qu.:0.000   1601    :   8   Q:123    Yes: 295   
##  Median :0.0000   Median :0.000   CA 2144 :   8   S:914               
##  Mean   :0.4989   Mean   :0.385   3101295 :   7                       
##  3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7                       
##  Max.   :8.0000   Max.   :9.000   347082  :   7                       
##                                   (Other) :1261                       
##   Friend_size      Fare_pp           Title          Deck       Ticket_class
##  Min.   : 1.0   Min.   :  0.000   Master: 61   U      :1014   3      :429  
##  1st Qu.: 1.0   1st Qu.:  7.579   Miss  :260   C      :  94   2      :278  
##  Median : 1.0   Median :  8.050   Mr    :757   B      :  65   1      :210  
##  Mean   : 2.1   Mean   : 14.765   Mrs   :197   D      :  46   P      : 98  
##  3rd Qu.: 3.0   3rd Qu.: 15.000   Other : 34   E      :  41   S      : 98  
##  Max.   :11.0   Max.   :128.082                A      :  22   C      : 77  
##                                                (Other):  27   (Other):119  
##   Family_size       Group_size       Age_group  
##  Min.   : 1.000   Min.   : 1.000   20-29  :552  
##  1st Qu.: 1.000   1st Qu.: 1.000   30-39  :229  
##  Median : 1.000   Median : 1.000   40-49  :171  
##  Mean   : 1.884   Mean   : 2.194   10-19  :162  
##  3rd Qu.: 2.000   3rd Qu.: 3.000   0-9    :100  
##  Max.   :11.000   Max.   :11.000   50-59  : 62  
##                                    (Other): 33
\end{verbatim}

A quick correlation plot of the numeric attributes to get an idea of how they might relate to one another. You can see that we have dropped two \texttt{chr} attributes: \emph{Title} and \emph{Deck}. We could include them if we convert the character value into numbers. For example, the \emph{Title} could be converted into 1-6 numbers as 1 represents \texttt{Mr}, 2 represents \texttt{Mrs}, and so on.

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{07-predictor-selection_files/figure-latex/unnamed-chunk-1-1} 

}

\caption{Correlation among numerical attributes}\label{fig:unnamed-chunk-1}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# show correlation in table}
\KeywordTok{library}\NormalTok{(kableExtra) }\CommentTok{# markdown tables }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'kableExtra' was built under R version 3.6.3
\end{verbatim}

\begin{verbatim}
## 
## Attaching package: 'kableExtra'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:dplyr':
## 
##     group_rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lower <-}\StringTok{ }\KeywordTok{round}\NormalTok{(cor,}\DecValTok{2}\NormalTok{)}
\NormalTok{lower[}\KeywordTok{lower.tri}\NormalTok{(cor, }\DataTypeTok{diag=}\OtherTok{TRUE}\NormalTok{)]<-}\StringTok{""}
\NormalTok{lower <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(lower)}
\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(lower, }\DataTypeTok{booktabs =} \OtherTok{TRUE}\NormalTok{,}
  \DataTypeTok{caption =} \StringTok{'Coorelations among attributes'}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{kable_styling}\NormalTok{(}\DataTypeTok{bootstrap_options =} \KeywordTok{c}\NormalTok{(}\StringTok{"striped"}\NormalTok{, }\StringTok{"hover"}\NormalTok{, }\StringTok{"condensed"}\NormalTok{, }\StringTok{"responsive"}\NormalTok{, }\DataTypeTok{font_size =} \DecValTok{8}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-2}Coorelations among attributes}
\centering
\begin{tabular}[t]{lllllllllllllllll}
\toprule
  & Survived & Pclass & Sex & Age & SibSp & Parch & Embarked & HasCabinNum & Friend\_size & Fare\_pp & Title & Deck & Ticket\_class & Family\_size & Group\_size & Age\_group\\
\midrule
Survived &  & -0.34 & -0.54 & -0.05 & -0.04 & 0.08 & -0.17 & 0.32 & 0.07 & 0.29 & -0.05 & -0.3 & -0.04 & 0.02 & 0.08 & -0.03\\
Pclass &  &  & 0.12 & -0.43 & 0.06 & 0.02 & 0.19 & -0.71 & -0.08 & -0.77 & -0.22 & 0.73 & -0.02 & 0.05 & -0.07 & -0.45\\
Sex &  &  &  & 0.06 & -0.11 & -0.21 & 0.1 & -0.14 & -0.17 & -0.12 & 0.01 & 0.13 & 0 & -0.19 & -0.2 & 0.07\\
Age &  &  &  &  & -0.27 & -0.15 & -0.07 & 0.3 & -0.2 & 0.38 & 0.49 & -0.32 & 0 & -0.26 & -0.2 & 0.98\\
SibSp &  &  &  &  &  & 0.37 & 0.07 & -0.01 & 0.68 & -0.05 & -0.2 & 0.01 & 0.05 & 0.86 & 0.73 & -0.27\\
\addlinespace
Parch &  &  &  &  &  &  & 0.05 & 0.04 & 0.65 & -0.03 & -0.09 & -0.03 & 0.06 & 0.79 & 0.67 & -0.14\\
Embarked &  &  &  &  &  &  &  & -0.21 & 0.01 & -0.3 & -0.03 & 0.24 & -0.04 & 0.07 & 0.02 & -0.06\\
HasCabinNum &  &  &  &  &  &  &  &  & 0.1 & 0.65 & 0.14 & -0.96 & -0.03 & 0.01 & 0.09 & 0.3\\
Friend\_size &  &  &  &  &  &  &  &  &  & 0.09 & -0.19 & -0.1 & 0.12 & 0.8 & 0.97 & -0.2\\
Fare\_pp &  &  &  &  &  &  &  &  &  &  & 0.18 & -0.7 & 0.1 & -0.05 & 0.09 & 0.38\\
\addlinespace
Title &  &  &  &  &  &  &  &  &  &  &  & -0.15 & 0.01 & -0.18 & -0.18 & 0.48\\
Deck &  &  &  &  &  &  &  &  &  &  &  &  & 0.02 & -0.01 & -0.1 & -0.32\\
Ticket\_class &  &  &  &  &  &  &  &  &  &  &  &  &  & 0.06 & 0.11 & 0.01\\
Family\_size &  &  &  &  &  &  &  &  &  &  &  &  &  &  & 0.85 & -0.26\\
Group\_size &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  & -0.2\\
\addlinespace
Age\_group &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  & \\
\bottomrule
\end{tabular}
\end{table}

The plot shows not only the correlation between other attributes, which can potentially be used as predictors, with the dependent attribute \emph{Survived}, but also the correction among potential predictors. In terms of correlation with \emph{Survived}, \emph{Sex} has the largest value but in negative -0.54, the next is \emph{Pclass} with -0.34. So if we can only have two predictors for survival, the first two we should use are \emph{Sex} and \emph{Pclass}. If I want to choose five predictors for survival, I would choose \emph{Sex}, \emph{Pclass}, \emph{HasCabinNum}, \emph{Deck}, and \emph{Fare\_PP}.

The largest correlation value is between \emph{Age} and \emph{Age\_group} with 0.98. It makes sense because \emph{Age\_group} is a grouping of \emph{Age}.

We can also observe that \emph{Pclass} has a high correlation with \emph{HasCabinNum} (71), \emph{Fare\_pp} (-77) and \emph{Deck} (73). It suggests that if we have \emph{Pclass} in our model, we may not need to use \emph{Fare\_pp}, \emph{HasCabinNum} or \emph{Deck} since they are effectively telling us the same thing that we have suspected at the beginning that is the ``social class'' of a passenger. This social class can be interpreted as the richer people, who paid more money on a ticket, has a better cabin.

A similar concept can be read between attribute \emph{Group\_size} and the other three attributes \emph{Friend\_Size}, \emph{Family\_size}, \emph{SibSp}, and \emph{Parch}. \emph{Family\_size} also has a high correlation with both \emph{SibSp} and \emph{Parch}. But \emph{Family\_size} has a very low correlation with \emph{Friend\_Size}.

The important point is that the correlation analysis is very useful. It provides the basic reasons for our predictor selection. The idea is that we should choose attributes that have a high correlation with the response variable. For example, if only choose three predictors in a model to predict \emph{Survived}, we should choose the \emph{Sex}, \emph{Pclass}, and \emph{HasCabinNum} because they have the three highest absolute correlation values with \emph{Survived}. If in a model we have chosen \emph{Pclass} we may not need to choose \emph{Fare\_pp} and \emph{Deck} because these three have large correction values.

\hypertarget{pca-analysis}{%
\section{PCA Analysis}\label{pca-analysis}}

PCA and Factor analysis are the most commonly used methods in dimension reduction. In a general data science project, it is possible that a given dataset can have tens or hundreds of features (attributes). For example in the text analysis, if we count words' appearance in a document, we could easily have hundreds even thousands of dimensions. If we want to reduce the dimension into a manageable number, PCA can be very useful. Particularly in visualization, humans are not good with anything over three dimensions.

PCA uses \emph{Eigenvalues} and \emph{Eigenvectors}\footnote{In linear algebra, an Eigenvector or characteristic vector of a linear transformation is a non-zero vector that changes by a scalar factor when that linear transformation is applied to it.} to reserve the original data information and variation as much as possible. Therefore PCA is simple to calculate the given data's Eigenvectors. The Eigenvectors show the attributes' importance.

PCA normally has the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Calculate the Covariance Matrix\footnote{Covariance matrix is a square matrix giving the covariance between each pair of elements of a given random vector.} of the given dataset.
\item
  Calculate the Eigenvalues and Eigenvectors of the resulting Covariance Matrix.
\item
  The resulting Eigenvector that corresponds to the largest Eigenvalue can then be used to reconstruct a large fraction of the variance of the original dataset.
\end{enumerate}

In R, we have a function called \texttt{prcomp()}. It takes numerical values. Let us calculate all the 18 attributes' Eigenvalue in our RE\_data dataset except \emph{passengerId} and \emph{Survived}. It is obvious that these two attributes are out of consideration.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Calculate Eigenvalues of the attributes}
\NormalTok{data.pca <-}\StringTok{ }\KeywordTok{prcomp}\NormalTok{(RE_data[}\DecValTok{1}\OperatorTok{:}\DecValTok{891}\NormalTok{,}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{1}\NormalTok{, }\DecValTok{-2}\NormalTok{)], }\DataTypeTok{center =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{scale =} \OtherTok{TRUE}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(data.pca)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Importance of components:
##                           PC1    PC2   PC3     PC4     PC5     PC6     PC7
## Standard deviation     2.1734 1.9300 1.362 1.23379 0.99671 0.91959 0.79275
## Proportion of Variance 0.2952 0.2328 0.116 0.09514 0.06209 0.05285 0.03928
## Cumulative Proportion  0.2952 0.5280 0.644 0.73913 0.80122 0.85407 0.89335
##                            PC8     PC9    PC10    PC11    PC12    PC13    PC14
## Standard deviation     0.75131 0.66502 0.55475 0.48300 0.27453 0.19723 0.15091
## Proportion of Variance 0.03528 0.02764 0.01923 0.01458 0.00471 0.00243 0.00142
## Cumulative Proportion  0.92863 0.95627 0.97550 0.99008 0.99479 0.99722 0.99865
##                           PC15      PC16
## Standard deviation     0.14717 1.534e-15
## Proportion of Variance 0.00135 0.000e+00
## Cumulative Proportion  1.00000 1.000e+00
\end{verbatim}

We have seen 16 principal components, which named as PC1 to PC16. Each of these explains a percentage of the total variation in the dataset. That is to say, PC1 explains 29\% of the total variance, PC2 explains nearly 24\% of the variance. Together with over half of the information in the dataset can be encapsulated by just these two principal components. So, by knowing the position of a sample in relation to just PC1 and PC2, you can get a very accurate view of where it stands in relation to other samples, as just PC1 and PC2 can explain 53\% of the variance.

Let's call \texttt{str()} to have a look at the PCA object.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(data.pca)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## List of 5
##  $ sdev    : num [1:16] 2.173 1.93 1.362 1.234 0.997 ...
##  $ rotation: num [1:16, 1:16] -0.1995 0.0756 0.3116 -0.3526 -0.2938 ...
##   ..- attr(*, "dimnames")=List of 2
##   .. ..$ : chr [1:16] "Pclass" "Sex" "Age" "SibSp" ...
##   .. ..$ : chr [1:16] "PC1" "PC2" "PC3" "PC4" ...
##  $ center  : Named num [1:16] 2.309 1.648 29.452 0.523 0.382 ...
##   ..- attr(*, "names")= chr [1:16] "Pclass" "Sex" "Age" "SibSp" ...
##  $ scale   : Named num [1:16] 0.836 0.478 13.432 1.103 0.806 ...
##   ..- attr(*, "names")= chr [1:16] "Pclass" "Sex" "Age" "SibSp" ...
##  $ x       : num [1:891, 1:16] -0.571 1.664 -0.24 1.708 0.818 ...
##   ..- attr(*, "dimnames")=List of 2
##   .. ..$ : chr [1:891] "1" "2" "3" "4" ...
##   .. ..$ : chr [1:16] "PC1" "PC2" "PC3" "PC4" ...
##  - attr(*, "class")= chr "prcomp"
\end{verbatim}

The above results contain a lot of details, briefly:

\begin{itemize}
\tightlist
\item
  The center point (\texttt{\$center}), scaling (\texttt{\$scale}), standard deviation(\texttt{sdev}) of each principal component
\item
  The relationship (correlation or anti-correlation, etc) between the initial variables (on the whole, It can be regarded as the data record) and the principal components (\texttt{\$rotation})
\item
  The values of each sample in terms of the principal components (\texttt{\$x})
\end{itemize}

Let us plot PCA to get a visual sense of it. To do so we need to use \textbf{biplot}. A biplot is a type of plot that will allow you to visualize how the samples relate to one another in the selected principal components (which samples are similar and which are different) and will simultaneously reveal how each variable contributes to each principal component.

\textbackslash begin\{figure\}

\{\centering \includegraphics[width=0.95\linewidth]{07-predictor-selection_files/figure-latex/PCA-1}

\}

\textbackslash caption\{The 1st and the 2nd PCs ploted with ggplot\_pca\}\label{fig:PCA-1}
\textbackslash end\{figure\}
\textbackslash begin\{figure\}

\{\centering \includegraphics[width=0.95\linewidth]{07-predictor-selection_files/figure-latex/PCA-2}

\}

\textbackslash caption\{The 1st and the 2nd PCs ploted with ggplot\_pca\}\label{fig:PCA-2}
\textbackslash end\{figure\}
The axes are seen as arrows originating from the center point. Here, you see that the variables \emph{Fare\_pp}, \emph{Age\_group}, and \emph{Survived} contribute to PC1, with higher values in those variables moving the records to the right on this plot. This lets you see how the data points relate to the axes.

We also have other principal components available although they may have fewer weights in comparison with the first two. Each of the other components maps differs from the original variables. We can also plot these other components, for example, PC3 and PC4. If you look into the PC3 and PC4, they are \emph{Sex} and \emph{Age\_group}. You may wonder what do they do with our prediction. Well, it can show at least the contribution between them with the dependent variable \emph{Survived}, in addition, it can also show the covariance of both with other variables.\\
\textbackslash begin\{figure\}

\{\centering \includegraphics[width=0.95\linewidth]{07-predictor-selection_files/figure-latex/PCA2-1}

\}

\textbackslash caption\{The 3rd and the 4th PC ploted with ggplot\_pca\}\label{fig:PCA2}
\textbackslash end\{figure\}
This Plot shows that original attributes \emph{Ticket\_class}, \emph{Sex}, \emph{Fare\_PP}, and \emph{Group\_size} contribute to PC3, which is \emph{Sex}, in a negative way. It means that with lower values in those variables, the records will move to the left on this plot. Notice the graph shows a close relationship between original attributes with the newly created Principal Components. It indicates the correlation among them.

With these correlation and PCA analyses, We can have a pretty good idea about the attributes. Depending on the models we are constructing, we can be confident to select the number of predictors and specific predictors to ensure our model has a good performance.\\
Attribute selection is a parsimonious process that aims to identify a minimal set of predictors for the maximum gain (predictive accuracy). This approach is the opposite of the data pre-process whereas as many meaningful attributes as possible are considered for potential use.
Later on, when we talk about prediction models, a lot of models have a function to analyse its predictor's importance. It is very similar to the PCA here.

It is important to recognize that attribute selection could be an iterative process that occurs throughout the model building process. It finishes after no more improvement can be achieved in terms of model accuracy.

\hypertarget{summary-5}{%
\section*{Summary}\label{summary-5}}


Predictor selection is a complex issue. It has been studied in many fields like Statistics, Data Analysis, Predictions, and Machine learning. In data science, it is addressed at the individual attribute level and at multiple attributes level. At the individual attribute level, it is called single variant analysis\index{single variant analysis}, it is many studies the relationship between individual attribute and the dependent variable like what we have done in Chapter 4 and 5. Correlation analysis between individual attributes and the dependent variable can provide the prediction power of each individual attribute so the selection can take predictors as wished. The multiple attribute analysis, called multivariant analysis\index{multivariant analysis}, focused on and covariant among the multiple attributes, so the strong correlation or collinearity can be identified, so only representative attribute can be selected as a predictor. Predictor selection is also influenced and affected by the model constructed. this will become clear in later Chapters after the model construction is introduced.

\hypertarget{prediction-with-decision-trees}{%
\chapter{Prediction with Decision Trees}\label{prediction-with-decision-trees}}

\begin{center}\includegraphics[width=0.6\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/images/tree} \end{center}

Model building is the main task of any data science project after understood data, processed some attributes, and analysed the attributes' correlations and the individual's prediction power. As described in the previous chapters. There are many ways to build a prediction model. In this chapter, we will demonstrate to build a prediction model with the most simple algorithm - \textbf{Decision tree}.

A decision tree\index{decision tree} is a commonly used classification model, which is a flowchart-like tree structure. In a decision tree, each internal node (non-leaf node) denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (or terminal node) holds a class label. The topmost node in a tree is the root node. A typical decision tree is shown in Figure \ref{fig:decisiontree}.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/images/decision tree} 

}

\caption{An example of decision tree}\label{fig:decisiontree}
\end{figure}

It represents the concept buys\_computer, that is, it predicts whether a customer is likely to buy a computer or not. `\texttt{yes}' is likely to buy, and `\texttt{no}' is unlikely to buy. Internal nodes are denoted by \emph{rectangles}, they are test conditions, and leaf nodes are denoted by \emph{ovals}, which are the final predictions. Some decision trees produce binary trees where each internal node branches to exactly two other nodes. Others can produce non-binary trees, like \texttt{age?} in the above tree has three branches.

A decision tree is built by a process called tree \emph{induction}, which is the learning or construction of decision trees from a class-labelled training dataset. Once a decision tree has been constructed, it can be used to classify a test dataset, which is also called \emph{deduction}.

The deduction process is Starting from the root node of a decision tree, we apply the test condition to a record or data sample and follow the appropriate branch based on the outcome of the test. This will lead us either to another internal node, for which a new test condition is applied or to a leaf node. The class label associated with the leaf node is then assigned to the record or the data sample. For example, to predict a new data input with \texttt{\textquotesingle{}age=senior\textquotesingle{}} and \texttt{\textquotesingle{}credit\_rating=excellent\textquotesingle{}}, traverse starting from the root goes to the most right side along the decision tree and reaches a leaf \texttt{yes}, which is indicated by the dotted line in the figure \ref{fig:decisiontree}.

Build a decision tree classifier needs to make two decisions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  which attributes to use for test conditions?
\item
  and in what order?
\end{enumerate}

Answering these two questions differently forms different decision tree algorithms. Different decision trees can have different prediction accuracy on the test dataset. Some decision trees are more accurate and cheaper to run than others. Finding the optimal tree is computationally expensive and sometimes is impossible because of the exponential size of the search space. In real practice, it is often to seek efficient algorithms, that are reasonably accurate and only compute in a reasonable amount of time. Hunt's\index{Hunt’s Algorithm}, ID3\index{ID3}, C4.5\index{C4.5} and CART algorithms\index{CART algorithms} are all of this kind of algorithms for classification. The common feature of these algorithms is that they all employ a greedy strategy as demonstrated in the \textbf{Hunt's algorithm}.

\hypertarget{decision-tree-in-hunts-algorithm}{%
\section{Decision Tree in Hunt's Algorithm}\label{decision-tree-in-hunts-algorithm}}

Hunt's algorithm builds a decision tree in a recursive fashion by partitioning the training dataset into successively \emph{purer} subsets. Hunt's algorithm takes three input values:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A training dataset, \(D\) with a number of attributes,
\item
  A subset of attributes \(Att_{list}\) and its testing criterion together to form a \texttt{\textquotesingle{}test\ condition\textquotesingle{}}, such as \texttt{\textquotesingle{}age\textgreater{}=25\textquotesingle{}} is a test condition, where, \texttt{\textquotesingle{}age\textquotesingle{}} is the attribute and \texttt{\textquotesingle{}\textgreater{}=25\textquotesingle{}} is the test criterion.
\item
  A \texttt{Attribute\_selection\_method}, it refers a procedure to determine the best splitting.
\end{enumerate}

The general recursive procedure is defined as below \citep{Tan2005}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create a node \(N\), suppose the training dataset when reach to note \(N\) is \(D_{N}\). Initially, \(D_{N}\) is the entire training set \(D\). Do the following:
\item
  If \(D_{t}\) contains records that belong the same class \(y_{t}\), then \(t\) is a leaf node labelled as \(y_{t}\);
\item
  If \(D_{t}\) is not an empty set but \(Att_{list}\) is empty, (there is no more test attributes left untested), then \(t\) is a leaf node labelled by the label of the majority records in the dataset;
\item
  If \(D_{t}\) contains records that belong to more than one class and \(Att_{list}\) is not empty, use \texttt{Attribute\_selection\_method} to choose next \textbf{best attribute} from the \(Att_{list}\) and remove that list from \(Att_{list}\). use the attribute and its condition as next test condition.
\item
  Repeat steps 2,3 and 4 until all the records in the subset belong to the same class.
\end{enumerate}

There are two fundamental problems that need to be sorted before Hunt's algorithm can work:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  How to form a `test condition'? particularly when non-binary attributes exist?
\item
  How to define the `best test conditions', so very loop the best test condition can be used in a decision tree?
\end{enumerate}

\hypertarget{test_condition}{%
\subsection*{How to Form a Test Condition?}\label{test_condition}}


Decision tree algorithms must provide a method for expressing a test condition and its corresponding outcomes for different attribute types.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Binary Attributes. The test condition for a binary attribute is simple because it only generates two potential outcomes, as shown in figure \ref{fig:sex}.
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/images/sex} 

}

\caption{Test condition for binary attributes.}\label{fig:sex}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  No-binary attributes. No-binary attributes depend on the types of nominal or ordinal, it can have different ways of the split. A nominal attribute can have many values, its test condition can be expressed in two ways, as shown in Figure \ref{fig:age2}. For a multiway split, see Figure \ref{fig:age2}(a), the number of outcomes depends on the number of distinct values for the corresponding attribute. For example, if an attribute such as age has three distinct values: youth, m\_aged, or senior. Its test condition will produce a three-way split or a binary split. Figure \ref{fig:age2}(b) illustrates three different ways of grouping the attribute values for age into two subsets.
\end{enumerate}

\begin{center}\includegraphics[width=0.5\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/images/age1} \end{center}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/images/age2} 

}

\caption{Test condition for no-binary attributes.}\label{fig:age2}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Continuous Attributes. For continuous attributes, the test condition can be constructed as a comparison test \((A<v)\) or \((A≥v)\) with binary outcomes, or a range query with outcomes of the form \(v_i≤A<v_{i+1}\), For \(i=1,…,k\). The difference between these approaches is shown in Figure \ref{fig:fare}. For the binary case, the decision tree algorithm must consider all possible split positions v, and it selects the one that produces the best partition. For the multiway split, the algorithm must consider all possible ranges of continuous values.
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/images/Fare} 

}

\caption{Test condition for continuous attributes.}\label{fig:fare}
\end{figure}

\hypertarget{best_split}{%
\subsection*{How to Determine the Best Split Condition?}\label{best_split}}


The method used to define the best split makes different decision tree algorithms. There are many measures that can be used to determine the best way to split the records. These measures are defined in terms of the class distribution of the records before and after splitting. The best splitting is the one that has more \emph{purity} after the splitting. If we were to split \(D\) into smaller partitions according to the outcomes of the splitting criterion, ideally each partition after splitting would be pure (i.e., all the records that fall into a given partition would belong to the same class). Instead of defining a split's purity, the impurity of its child node is used. There are a number of commonly used impurity measurements: \emph{Entropy}, \emph{Gini Index} and \emph{Classification Error}.

\textbf{Entropy:} measures the degree of uncertainty, impurity, or disorder. The formula for calculate entropy is as shown below:

\begin{equation} 
E(x)= ∑_{i=1}^{n}p_ilog_2(p_i),
  \label{eq:entropy}
\end{equation}

Where \(p\) represents the probability and \(E(x)\) represents the entropy.

\textbf{Gini Index:} also called Gini impurity, measures the degree of probability of a particular variable being incorrectly classified when it is chosen randomly. The degree of the Gini index varies between zero and one, where zero denotes that all elements belong to a certain class or only one class exists, and one denotes that the elements are randomly distributed across various classes. A Gini index of 0.5 denotes equally distributed elements into some classes.

The formula used to calculate \emph{Gini} index is shown below:

\begin{equation} 
GINI(x) = 1- ∑_{i=1}^{n}p_i^2,
  \label{eq:Gini}
\end{equation}

Where \(p_i\) is the probability of an object being classified to a particular class.

\textbf{Classification Error} measures the misclassified class labels. It is calculated with the formula shows below:
\begin{equation} 
Classification error(x)= 1 - max_{i}p_i.
  \label{eq:clerror}
\end{equation}

Among these three impurity measurements, \emph{Gini} is Used by the CART (classification and regression tree) algorithm for classification trees, and \emph{Entropy} is Used by the ID3, C4.5, and C5.0 tree-generation algorithms.

With the above explanation, we can now say that the aims of a decision tree algorithm are to reduce the Entropy level from the root to the leaves and the best tree is the one that takes order from the most to the least in reducing the Entropy level.

The good news is that we do not need to calculate the impurity of each test condition to build a decision tree manually. Most tools have the tree construction built-in already. We will use the R package called \emph{rpart} to build decision trees for our Titanic prediction problem.

\hypertarget{the-simplest-decision-tree-for-titanic}{%
\section{The Simplest Decision Tree for Titanic}\label{the-simplest-decision-tree-for-titanic}}

In the Titanic problem, Let's quickly review the possible attributes. Previously, we have understood that there are a few attributes that have a little prediction power or we say they have a little association with the dependent variable \emph{Survivded}. These attributes include \emph{PassengerID}, \emph{Name}, and \emph{Ticket}. That is why we re-engineered some of them like the passenger's name has been re-engineered into the \emph{Title}, etc. Other attributes can all be used to predict a passenger's death or survival since they all have some power of prediction. So, which one to use and in what order? We will use the Titanic problem to demonstrate how to build decision trees for prediction.

Let us consider a simple decision tree firstly.

The simplest decision tree perhaps is the one that only has one test condition and two possible outcomes. In terms of a tree, we called it one internal node and two branches. There is only one attribute that meets the requirements. That is \emph{Sex}, so our decision tree will be built only base on the passenger's gender.

We need a number of libraries to make our code works.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#load rpart the library which support decision tree }
\KeywordTok{library}\NormalTok{(rpart)}
\CommentTok{# Build our first model only use Sex attribute, check help on rpart, }
\CommentTok{# This model only takes Sex as predictor and Survived as the consequencer.}
\CommentTok{# load our re-engineered data set and separate train and test datasets}
\NormalTok{RE_data <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"./data/RE_data.csv"}\NormalTok{, }\DataTypeTok{header =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{train <-}\StringTok{ }\NormalTok{RE_data[}\DecValTok{1}\OperatorTok{:}\DecValTok{891}\NormalTok{, ]}
\NormalTok{test <-}\StringTok{ }\NormalTok{RE_data[}\DecValTok{892}\OperatorTok{:}\DecValTok{1309}\NormalTok{, ]}
\CommentTok{#build a decision tree model use rpart. }
\CommentTok{#set seed to make random reproducible}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{) }\CommentTok{#decision tree model has many random selections}
\NormalTok{model1 <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(Survived }\OperatorTok{~}\StringTok{ }\NormalTok{Sex, }\DataTypeTok{data =}\NormalTok{ train, }\DataTypeTok{method=}\StringTok{"class"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Simple! and quick. There are only three lines of code. you can see we have the first two lines to build two variables `train' and `test' to hold our training dataset and testing dataset. The model is simply a function invocation, the function is called `rpart'.

R function did the job for us so we do not need to go through the model construction phase manually to build our classifier. The decision tree has been already built. Now we can use our model to make predictions on the test dataset.

For the Kaggle competition, participants can produce predictions on the test dataset provided and make submissions to the Kaggle competition website. Kaggle will award a score based on the prediction's accuracy on the test dataset.

In practice, people would not build a prediction model and use it to produce a prediction on the test dataset blandly to finish the job. We want to know how our model is performing before using it to do predictions on the test dataset. One way to get to know about the model's performance is to make a prediction on the training dataset or part of it. So we can compare the model's prediction with the original value.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#library caret is a comprehensive library support all sorts of model analysis}
\KeywordTok{library}\NormalTok{(caret)}
\KeywordTok{options}\NormalTok{(}\DataTypeTok{digits=}\DecValTok{4}\NormalTok{)}
\CommentTok{# assess the model's accuracy with train dataset by make a prediction on the train data. }
\NormalTok{Predict_model1_train <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(model1, train, }\DataTypeTok{type =} \StringTok{"class"}\NormalTok{)}
\CommentTok{#build a confusion matrix to make comparison}
\NormalTok{conMat <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(Predict_model1_train), }\KeywordTok{as.factor}\NormalTok{(train}\OperatorTok{$}\NormalTok{Survived))}
\CommentTok{#show confusion matrix }
\NormalTok{conMat}\OperatorTok{$}\NormalTok{table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           Reference
## Prediction   0   1
##          0 468 109
##          1  81 233
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#show percentage of same values - accuracy}
\NormalTok{predict_train_accuracy <-}\StringTok{ }\NormalTok{conMat}\OperatorTok{$}\NormalTok{overall[}\StringTok{"Accuracy"}\NormalTok{]}
\NormalTok{predict_train_accuracy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Accuracy 
##   0.7868
\end{verbatim}

A brief assessment shows our model1's accuracy is 78.68\%. It is not bad! Let us use this model to make a prediction on test dataset.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# The firs prediction produced by the first decision tree which only used one predictor Sex}
\NormalTok{Prediction1 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(model1, test, }\DataTypeTok{type =} \StringTok{"class"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Our prediction is produced. Let us submit to Kaggle for an evaluation. We need to convert our prediction into Kaggle's required format and save it into a file and name it as ``Tree\_Model1.CSV''. Here, the importance is knowing the procedure.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# produce a submit with Kaggle required format that is only two attributes: PassengerId and Survived}
\NormalTok{submit1 <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{PassengerId =}\NormalTok{ test}\OperatorTok{$}\NormalTok{PassengerId, }\DataTypeTok{Survived =}\NormalTok{ Prediction1)}
\CommentTok{# Write it into a file "Tree_Model1.CSV"}
\KeywordTok{write.csv}\NormalTok{(submit1, }\DataTypeTok{file =} \StringTok{"./data/Tree_Model1.CSV"}\NormalTok{, }\DataTypeTok{row.names =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Once we submit this result to Kaggle. Kaggle will evaluate our results and provide a feedback score. That is a good way to know how good the model performed for unknown data. The Kaggle feedback tells us that we have scored \textbf{0.76555}. It means our prediction's accuracy is \textbf{76.555\%}. This accuracy is lower than the accuracy we have assessed with the training dataset, which was \textbf{78.68\%}.

Let us look a bit more into our prediction model's performance. We check our prediction's death and survive ratio on the test dataset and compare with the same ratio on the train dataset.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Inspect prediction}
\KeywordTok{summary}\NormalTok{(submit1}\OperatorTok{$}\NormalTok{Survived)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   0   1 
## 266 152
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{prop.table}\NormalTok{(}\KeywordTok{table}\NormalTok{(submit1}\OperatorTok{$}\NormalTok{Survived, }\DataTypeTok{dnn=}\StringTok{"Test survive percentage"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Test survive percentage
##      0      1 
## 0.6364 0.3636
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#train survive ratio}
\KeywordTok{prop.table}\NormalTok{(}\KeywordTok{table}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(train}\OperatorTok{$}\NormalTok{Survived), }\DataTypeTok{dnn=}\StringTok{"Train survive percentage"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Train survive percentage
##      0      1 
## 0.6162 0.3838
\end{verbatim}

The result shows that among a total of 418 passengers in the test dataset, 266 passengers predicted perished (with survived value 0), which counts as 64\%, and 152 passengers predicted to be survived (with survived value 1) and which count as 36\%. This is not too far from the radio on the training dataset, which was 62\% survived and 38\% perished see \ref{survive}.

We know that our model only had one test condition which is \emph{Sex}. From the training dataset, we knew that the gender ratio was very similar to this number.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Install expss}
\KeywordTok{library}\NormalTok{(expss)}
\CommentTok{# add Sex back to the submit and form a new data frame called compare}
\NormalTok{compare <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(submit1[}\DecValTok{1}\NormalTok{], }\DataTypeTok{Sex =}\NormalTok{ test}\OperatorTok{$}\NormalTok{Sex, submit1[}\DecValTok{2}\NormalTok{])}
\CommentTok{# Check train sex and Survived ratios}
\KeywordTok{prop.table}\NormalTok{(}\KeywordTok{table}\NormalTok{(train}\OperatorTok{$}\NormalTok{Sex, train}\OperatorTok{$}\NormalTok{Survived, }\DataTypeTok{dnn =} \KeywordTok{c}\NormalTok{(}\StringTok{""}\NormalTok{, }\StringTok{"Gender and Survive Ratio in Train"}\NormalTok{)),  }\DataTypeTok{margin =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         Gender and Survive Ratio in Train
##               0      1
##   female 0.2580 0.7420
##   male   0.8111 0.1889
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Check predicted sex radio}
\KeywordTok{prop.table}\NormalTok{(}\KeywordTok{table}\NormalTok{(compare}\OperatorTok{$}\NormalTok{Sex, }\DataTypeTok{dnn=}\StringTok{"Gender ratio in Test"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Gender ratio in Test
## female   male 
## 0.3636 0.6364
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#check predicted Survive and Sex radio}
\KeywordTok{prop.table}\NormalTok{(}\KeywordTok{table}\NormalTok{(compare}\OperatorTok{$}\NormalTok{Sex, compare}\OperatorTok{$}\NormalTok{Survived, }\DataTypeTok{dnn=}\KeywordTok{c}\NormalTok{(}\StringTok{""}\NormalTok{,}\StringTok{"Gender and Survived Ratio in Prediction"}\NormalTok{)), }\DataTypeTok{margin =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         Gender and Survived Ratio in Prediction
##          0 1
##   female 0 1
##   male   1 0
\end{verbatim}

It is clear that our model is too simple: it predicts any male will perish and every female will be survived! This is approved by the gender (male and female) ratio in the test dataset is identical to the death ratio in our prediction result.

Further, our predicted results' survival ratio on sex is 0\% male and 100\% female. It makes sense, doesn't it? Since our model was trained using the training dataset. The survive ratio based on gender were as: only 19\% male survived and 81\% of male perished. Similarly, the Female survival rate was 74\% survived and only 26\% perished.

Any prediction model will have to go for the majority. But, we cannot be satisfied with this simple model that only looking into the sex of a given dataset and predict a passenger's fate with sex!

This is only the starting, we can improve on it later. But before we do, let us have a close look into our model (the tree structure) and have a sense of the results it produced. R has a lot of functions to help. Plot is a visual tool we can use to visualize our model.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{08-decision-tree_files/figure-latex/tree1-1} \includegraphics[width=0.5\linewidth]{08-decision-tree_files/figure-latex/tree1-2} 

}

\caption{The decision tree only has *Sex* test condition.}\label{fig:tree1}
\end{figure}

This graph is pretty and informative. The first box top number is the voting (either 0 - dead or 1-survived). The two percentages show the value of the splitting (also called \emph{voting} or \emph{confidence}). The final number on each node shows the percent of the population which resides in this node. Also, the colour of nodes signifies the two classes here. For example, the root node, ``0'' (death) shows the way the root node is voting; ``.62'' and ``.38'' represent the proportion of those who die and those who survive; 100\% implies that the entire population resides in the root node.

\hypertarget{the-decision-tree-with-core-predictors}{%
\section{The Decision Tree with Core Predictors}\label{the-decision-tree-with-core-predictors}}

We have identified the correlation between the dependent variable \emph{Survived} and other attributes in the previous Chapter. Let us try to improve the basic \emph{Sex} model by introducing more predictors. From the previous chapter, we know that the five most predictive attributes are: \emph{Sex}, \emph{Pclass}, \emph{HasCabinNum}, \emph{Deck}, and \emph{Fare\_PP}. Let us see if they can produce a good model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# A tree model with the top five attributes }
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{model2 <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(Survived }\OperatorTok{~}\StringTok{ }\NormalTok{Sex }\OperatorTok{+}\StringTok{ }\NormalTok{Pclass }\OperatorTok{+}\StringTok{ }\NormalTok{HasCabinNum }\OperatorTok{+}\StringTok{ }\NormalTok{Deck }\OperatorTok{+}\StringTok{ }\NormalTok{Fare_pp, }\DataTypeTok{data =}\NormalTok{ train, }\DataTypeTok{method=}\StringTok{"class"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Assess model's accuracy with train data}
\NormalTok{Predict_model2_train <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(model2, train, }\DataTypeTok{type =} \StringTok{"class"}\NormalTok{)}
\NormalTok{conMat <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(Predict_model2_train), }\KeywordTok{as.factor}\NormalTok{(train}\OperatorTok{$}\NormalTok{Survived))}
\NormalTok{conMat}\OperatorTok{$}\NormalTok{table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           Reference
## Prediction   0   1
##          0 524 140
##          1  25 202
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#conMat$overall}
\NormalTok{predict2_train_accuracy <-}\StringTok{ }\NormalTok{conMat}\OperatorTok{$}\NormalTok{overall[}\StringTok{"Accuracy"}\NormalTok{]}
\NormalTok{predict2_train_accuracy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Accuracy 
##   0.8148
\end{verbatim}

Our assessment of model2's accuracy is to make a prediction on train data and compare the predicted value with the original value. It shows our model2's accuracy is 81\%. This is great! Let us use this model to make a prediction on the test dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Prediction2 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(model2, test, }\DataTypeTok{type =} \StringTok{"class"}\NormalTok{)}
\NormalTok{submit2 <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{PassengerId =}\NormalTok{ test}\OperatorTok{$}\NormalTok{PassengerId, }\DataTypeTok{Survived =}\NormalTok{ Prediction2)}
\KeywordTok{write.csv}\NormalTok{(submit2, }\DataTypeTok{file =} \StringTok{"./data/Tree_model2.CSV"}\NormalTok{, }\DataTypeTok{row.names =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We have produced our second prediction. We can also submit our results to the Kaggle website for the second evaluation.

This time, we can see the score is still \textbf{0.76555}. The accuracy of the test dataset has not been improved.

Let us examine our classifier again by plot it in a graph.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot our full house classifier }
\KeywordTok{prp}\NormalTok{(model2, }\DataTypeTok{type =} \DecValTok{0}\NormalTok{, }\DataTypeTok{extra =} \DecValTok{1}\NormalTok{, }\DataTypeTok{under =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{# plot our full house classifier }
\KeywordTok{fancyRpartPlot}\NormalTok{(model2, }\DataTypeTok{caption =} \StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{08-decision-tree_files/figure-latex/tree2-1} \includegraphics[width=0.5\linewidth]{08-decision-tree_files/figure-latex/tree2-2} 

}

\caption{Decision trees with core predictors.}\label{fig:tree2}
\end{figure}

The above decision tree appeared much complicated than the first one and it goes a lot deeper than what we saw with the decision tree the only test on sex. Note that both trees are binary trees (have two branches). For test conditions that more than two possible answers have been changed to a binary by auto add a split with them. For example, predictor \texttt{Fare\_pp} takes real numbers and has many possibilities, our model simply split it by test conditions \texttt{"Fare\_pp\ \textgreater{}=\ 8"} and ``Fare\_pp \textless{} 7.2''. Conditions have been automatically set for other attributes as well such as \texttt{"Pclass\ \textgreater{}=\ 2.5"}. These auto-generated test conditions are not ideal, they can be changed if you know how to optimize the decision tree. For the moment, we will take the default settings.

We can also look into the difference between our two prediction models,

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# build a comparison data frame to record each prediction results}
\NormalTok{Tree_compare <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(test}\OperatorTok{$}\NormalTok{PassengerId, }\DataTypeTok{predict1=}\NormalTok{Prediction1, }\DataTypeTok{predict2=}\NormalTok{Prediction2)}
\CommentTok{# Find differences}
\NormalTok{dif <-}\StringTok{ }\NormalTok{Tree_compare[Tree_compare[}\DecValTok{2}\NormalTok{]}\OperatorTok{!=}\NormalTok{Tree_compare[}\DecValTok{3}\NormalTok{], ]}
\CommentTok{#show dif}
\KeywordTok{print.data.frame}\NormalTok{(dif, }\DataTypeTok{row.names =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  test.PassengerId predict1 predict2
##               893        1        0
##               896        1        0
##               911        1        0
##               924        1        0
##               925        1        0
##               928        1        0
##               929        1        0
##               941        1        0
##               979        1        0
##               982        1        0
##               996        1        0
##              1009        1        0
##              1017        1        0
##              1024        1        0
##              1030        1        0
##              1032        1        0
##              1045        1        0
##              1051        1        0
##              1061        1        0
##              1080        1        0
##              1091        1        0
##              1117        1        0
##              1141        1        0
##              1155        1        0
##              1160        1        0
##              1172        1        0
##              1175        1        0
##              1176        1        0
##              1183        1        0
##              1201        1        0
##              1225        1        0
##              1246        1        0
##              1257        1        0
##              1259        1        0
##              1268        1        0
##              1275        1        0
##              1301        1        0
\end{verbatim}

We can see the second classifier has produced 37 different predictions in comparison with the first classifier. Interesting is that the differences are all that predicted to be survived by the model1 is now predicted to be dead by the model2.

\hypertarget{the-decision-tree-with-more-predictors}{%
\section{The Decision Tree with More Predictors}\label{the-decision-tree-with-more-predictors}}

We have seen that our 5 key predictor decision tree model has improved on the sex-only prediction model. However, we know that our re-engineered data has more dimensions that contain useful information. Let us see if we can improve the decision tree model with more predictors in addition to the correlation analysis and PCA analyses results. This time We add travel in groups, Age\_group, embarked port and the title attributes \emph{Sex}, \emph{Pclass}, \emph{HasCabinNum}, \emph{Deck}, and \emph{Fare\_pp}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# tree model3 construction using more predictors}
\NormalTok{model3 <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(Survived }\OperatorTok{~}\StringTok{ }\NormalTok{Sex }\OperatorTok{+}\StringTok{ }\NormalTok{Fare_pp }\OperatorTok{+}\StringTok{ }\NormalTok{Pclass }\OperatorTok{+}\StringTok{ }\NormalTok{Title }\OperatorTok{+}\StringTok{ }\NormalTok{Age_group }\OperatorTok{+}\StringTok{ }\NormalTok{Group_size }\OperatorTok{+}\StringTok{ }\NormalTok{Ticket_class  }\OperatorTok{+}\StringTok{ }\NormalTok{Embarked,}
              \DataTypeTok{data=}\NormalTok{train,}
              \DataTypeTok{method=}\StringTok{"class"}\NormalTok{)}
\CommentTok{# This model will be used in later chapters so save it in to a file for later to be loaded into memory}
\KeywordTok{save}\NormalTok{(model3, }\DataTypeTok{file =} \StringTok{"./data/model3.rda"}\NormalTok{)}
\CommentTok{#Assess prediction accuracy on train data}
\NormalTok{Predict_model3_train <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(model3, train, }\DataTypeTok{type =} \StringTok{"class"}\NormalTok{)}
\NormalTok{conMat <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(Predict_model3_train), }\KeywordTok{as.factor}\NormalTok{(train}\OperatorTok{$}\NormalTok{Survived))}
\NormalTok{conMat}\OperatorTok{$}\NormalTok{table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           Reference
## Prediction   0   1
##          0 517 100
##          1  32 242
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#conMat$overall}
\NormalTok{predict3_train_accuracy <-}\StringTok{ }\NormalTok{conMat}\OperatorTok{$}\NormalTok{overall[}\StringTok{"Accuracy"}\NormalTok{]}
\NormalTok{predict3_train_accuracy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Accuracy 
##   0.8519
\end{verbatim}

Our assessment about the model3's accuracy on the train data shows the accuracy has increased to 85\%. It is a big increase from 82\% of model2. Let us use this model to make another prediction on the test dataset and see if the accuracy on the test dataset is also increased.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Prediction3 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(model3, test, }\DataTypeTok{type =} \StringTok{"class"}\NormalTok{)}
\NormalTok{submit3<-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{PassengerId =}\NormalTok{ test}\OperatorTok{$}\NormalTok{PassengerId, }\DataTypeTok{Survived =}\NormalTok{ Prediction3)}
\KeywordTok{write.csv}\NormalTok{(submit3, }\DataTypeTok{file =} \StringTok{"./data/Tree_model3.CSV"}\NormalTok{, }\DataTypeTok{row.names =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

After submitting it to Kaggle the feedback was \textbf{0.77033}. This is a big improvement on the test dataset. Let us look into the difference between the last two predictions,

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot our full house classifier }
\KeywordTok{prp}\NormalTok{(model3, }\DataTypeTok{type =} \DecValTok{0}\NormalTok{, }\DataTypeTok{extra =} \DecValTok{1}\NormalTok{, }\DataTypeTok{under =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{# plot our full house classifier }
\KeywordTok{fancyRpartPlot}\NormalTok{(model3)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{08-decision-tree_files/figure-latex/multi-trees-1} \includegraphics[width=0.5\linewidth]{08-decision-tree_files/figure-latex/multi-trees-2} 

}

\caption{Decision trees with more predictors.}\label{fig:multi-trees}
\end{figure}

Again, let us look into the difference between predicted values on the test dataset.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# build a comparison data frame to record each prediction results}
\NormalTok{compare <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(test}\OperatorTok{$}\NormalTok{PassengerId, }\DataTypeTok{predict2 =}\NormalTok{ Prediction2 , }\DataTypeTok{predict3 =}\NormalTok{ Prediction3)}
\CommentTok{# Find differences}
\NormalTok{dif <-}\StringTok{ }\NormalTok{compare[compare[}\DecValTok{2}\NormalTok{] }\OperatorTok{!=}\StringTok{ }\NormalTok{compare[}\DecValTok{3}\NormalTok{], ]}
\CommentTok{#show dif}
\KeywordTok{print.data.frame}\NormalTok{(dif, }\DataTypeTok{row.names =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  test.PassengerId predict2 predict3
##               896        0        1
##               913        0        1
##               925        0        1
##               956        0        1
##               972        0        1
##               981        0        1
##               982        0        1
##               996        0        1
##              1009        0        1
##              1051        0        1
##              1053        0        1
##              1084        0        1
##              1086        0        1
##              1088        0        1
##              1093        0        1
##              1098        1        0
##              1106        1        0
##              1117        0        1
##              1136        0        1
##              1141        0        1
##              1155        0        1
##              1173        0        1
##              1175        0        1
##              1176        0        1
##              1183        0        1
##              1199        0        1
##              1205        1        0
##              1225        0        1
##              1231        0        1
##              1236        0        1
##              1239        1        0
##              1246        0        1
##              1284        0        1
##              1301        0        1
##              1309        0        1
\end{verbatim}

There are 35 differences.

\hypertarget{the-decision-tree-with-full-predictors}{%
\section{The Decision Tree with Full Predictors}\label{the-decision-tree-with-full-predictors}}

Let us use all the attributes:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# The full-house classifier apart from name and ticket}
\NormalTok{model4 <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(Survived }\OperatorTok{~}\StringTok{ }\NormalTok{Sex }\OperatorTok{+}\StringTok{ }\NormalTok{Pclass }\OperatorTok{+}\StringTok{ }\NormalTok{Age }\OperatorTok{+}\StringTok{ }\NormalTok{SibSp }\OperatorTok{+}\StringTok{ }\NormalTok{Parch }\OperatorTok{+}\StringTok{ }\NormalTok{Embarked }\OperatorTok{+}\StringTok{ }\NormalTok{HasCabinNum }\OperatorTok{+}\StringTok{ }\NormalTok{Friend_size }\OperatorTok{+}\StringTok{ }\NormalTok{Fare_pp }\OperatorTok{+}\StringTok{ }\NormalTok{Title }\OperatorTok{+}\StringTok{ }\NormalTok{Deck }\OperatorTok{+}\StringTok{ }\NormalTok{Ticket_class }\OperatorTok{+}\StringTok{ }\NormalTok{Family_size }\OperatorTok{+}\StringTok{ }\NormalTok{Group_size }\OperatorTok{+}\StringTok{ }\NormalTok{Age_group,}
\CommentTok{#model4 <- rpart(Survived ~ .,}
              \DataTypeTok{data=}\NormalTok{train,}
              \DataTypeTok{method=}\StringTok{"class"}\NormalTok{)}
\CommentTok{#assess prediction accuracy on train data}
\NormalTok{Predict_model4_train <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(model4, train, }\DataTypeTok{type =} \StringTok{"class"}\NormalTok{)}
\NormalTok{conMat <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(Predict_model4_train), }\KeywordTok{as.factor}\NormalTok{(train}\OperatorTok{$}\NormalTok{Survived))}
\NormalTok{conMat}\OperatorTok{$}\NormalTok{table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           Reference
## Prediction   0   1
##          0 513  94
##          1  36 248
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#conMat$overall}
\NormalTok{predict4_train_accuracy <-}\StringTok{ }\NormalTok{conMat}\OperatorTok{$}\NormalTok{overall[}\StringTok{"Accuracy"}\NormalTok{]}
\NormalTok{predict4_train_accuracy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Accuracy 
##   0.8541
\end{verbatim}

Our assessment on model4's accuracy on the train data shows model4's accuracy is 85\%. Let us use this model to make a prediction on the test dataset.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# make prediction on test dataset}
\NormalTok{Prediction4 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(model4, test, }\DataTypeTok{type =} \StringTok{"class"}\NormalTok{)}
\NormalTok{submit4 <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{PassengerId =}\NormalTok{ test}\OperatorTok{$}\NormalTok{PassengerId, }\DataTypeTok{Survived =}\NormalTok{ Prediction4)}
\KeywordTok{write.csv}\NormalTok{(submit4, }\DataTypeTok{file =} \StringTok{"./data/Tree_model4.CSV"}\NormalTok{, }\DataTypeTok{row.names =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We have produced a new prediction with new model. You can submit to Kaggle for an evaluation. You may find it has a pretty bed score (\textbf{0.75119}). Let us examine our classifier again by plot it in a graph.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot our full house classifier }
\KeywordTok{prp}\NormalTok{(model4, }\DataTypeTok{type =} \DecValTok{0}\NormalTok{, }\DataTypeTok{extra =} \DecValTok{1}\NormalTok{, }\DataTypeTok{under =} \OtherTok{TRUE}\NormalTok{)}
\KeywordTok{fancyRpartPlot}\NormalTok{(model4)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{08-decision-tree_files/figure-latex/tree3-1} \includegraphics[width=0.5\linewidth]{08-decision-tree_files/figure-latex/tree3-2} 

}

\caption{Decision trees.}\label{fig:tree3}
\end{figure}

We can see from the tree, the first test condition is \texttt{"title\ =\ Mr"}. We know that there is a large number of passengers are male adults and most of them perish. However, we can see from our decision tree (left branch) that there are two further test conditions are \texttt{"Ticket\_class"} and \texttt{"Deck"} numbers. Our tree end with 2 survived leaves and one dead leaf. The purity on the leaves is not very high, the highest is 39:366 (90\%:10\%) and the lowest is 7:14 (33\%:67\%).

On the right-hand side of the tree, although some leaves have higher purity the overall percentage is very low. The most powerful predictor \texttt{Sex} and \texttt{Pclass} has been used towards the leaf of the tree. That could be an explanation for the poor performance. We will demonstrate the detailed interpretation of the model in the later chapter of cross validation and report.

Let us look into the difference between the last two predictions,

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# build a comparison data frame to record each prediction results}
\NormalTok{compare <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(test}\OperatorTok{$}\NormalTok{PassengerId, }\DataTypeTok{predict3 =}\NormalTok{ Prediction3 , }\DataTypeTok{predict4 =}\NormalTok{ Prediction4)}
\CommentTok{# Find differences}
\NormalTok{dif2 <-}\StringTok{ }\NormalTok{compare[compare[}\DecValTok{2}\NormalTok{] }\OperatorTok{!=}\StringTok{ }\NormalTok{compare[}\DecValTok{3}\NormalTok{], ]}
\CommentTok{#show dif}
\KeywordTok{print.data.frame}\NormalTok{(dif2, }\DataTypeTok{row.names =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  test.PassengerId predict3 predict4
##               898        1        0
##               916        1        0
##               933        0        1
##               945        1        0
##               956        1        0
##               961        1        0
##               964        1        0
##               965        0        1
##              1038        0        1
##              1050        0        1
##              1073        0        1
##              1128        0        1
##              1134        0        1
##              1137        0        1
##              1183        1        0
##              1247        0        1
##              1251        1        0
##              1259        0        1
##              1296        0        1
##              1304        1        0
\end{verbatim}

There are 20 different predictions in comparison with the third prediction model3 that is the best model we have with the decision tree.

\hypertarget{summary-6}{%
\section*{Summary}\label{summary-6}}


So far, we have produced four prediction models with different predictors. We also used these four models to generate four submissions to the Kaggle competition for evaluation since we don't have the survival values for the test dataset.

The results obtained from the Kaggle in terms of prediction's accuracy were 76.56\%, 76.56\%, 77.03\%, and 75.12\% respectively.

We have noticed the accuracy differences between the model's assessment and model's test accuracy are:

\begin{itemize}
\item
  model1's assessed accuracy was 78.68\% but the accuracy on the test was 76.555\%;
\item
  Model2's accessed accuracy was 81.48\% and the actual accuracy on the test was still 76.555\%;
\item
  Model3's accessed accuracy was 85.19\%. and the actual accuracy on the test was 77.03\%;
\item
  Model4's accessed accuracy was 85.41\% and the actual accuracy on the test was 75.12\%.
\end{itemize}

let us plot these model's accuracy, so we can have a comparison.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyr)}
\CommentTok{# Tree models comparison}
\NormalTok{Model <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Model1"}\NormalTok{,}\StringTok{"Model2"}\NormalTok{,}\StringTok{"Model3"}\NormalTok{,}\StringTok{"Model4"}\NormalTok{)}
\NormalTok{Pre <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Sex"}\NormalTok{, }\StringTok{"Sex, Pclass, HasCabinNum, Deck, Fare_pp"}\NormalTok{, }\StringTok{"Sex, Fare_pp, Pclass, Title, Age_group, Group_size, Ticket_class, Embarked"}\NormalTok{, }\StringTok{"All"}\NormalTok{)}
\NormalTok{Train <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{78.68}\NormalTok{, }\FloatTok{81.48}\NormalTok{, }\FloatTok{85.19}\NormalTok{, }\FloatTok{85.41}\NormalTok{)}
\NormalTok{Test <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{76.56}\NormalTok{, }\FloatTok{76.56}\NormalTok{, }\FloatTok{77.03}\NormalTok{, }\FloatTok{75.12}\NormalTok{)}
\NormalTok{df1 <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(Model, Pre, Train, Test)}
\NormalTok{df2 <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(Model, Train, Test)}
\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(df1, }\DataTypeTok{longtable =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{booktabs =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{digits =} \DecValTok{2}\NormalTok{, }\DataTypeTok{col.names =}\KeywordTok{c}\NormalTok{(}\StringTok{"Models"}\NormalTok{, }\StringTok{"Predictors"}\NormalTok{, }\StringTok{"Accuracy on Train"}\NormalTok{, }\StringTok{"Accuracy on Test"}\NormalTok{), }
  \DataTypeTok{caption =} \StringTok{'The Comparision among 4 decision tree models'}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[t]{llrr}
\caption{\label{tab:unnamed-chunk-2}The Comparision among 4 decision tree models}\\
\toprule
Models & Predictors & Accuracy on Train & Accuracy on Test\\
\midrule
Model1 & Sex & 78.68 & 76.56\\
Model2 & Sex, Pclass, HasCabinNum, Deck, Fare\_pp & 81.48 & 76.56\\
Model3 & Sex, Fare\_pp, Pclass, Title, Age\_group, Group\_size, Ticket\_class, Embarked & 85.19 & 77.03\\
Model4 & All & 85.41 & 75.12\\
\bottomrule
\end{longtable}

Let us plot a bar graph,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.long <-}\StringTok{ }\KeywordTok{gather}\NormalTok{(df2, Dataset, Accuracy, }\OperatorTok{-}\NormalTok{Model, }\DataTypeTok{factor_key =}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ df.long, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Model, }\DataTypeTok{y =}\NormalTok{ Accuracy, }\DataTypeTok{fill =}\NormalTok{ Dataset)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_col}\NormalTok{(}\DataTypeTok{position =} \KeywordTok{position_dodge}\NormalTok{()) }
\end{Highlighting}
\end{Shaded}

\includegraphics{08-decision-tree_files/figure-latex/modelcompare-1.pdf}
From the plot we can see that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  All four models perform better when predicting the survival on the training dataset than they do on the test data. In other words, all models drop prediction accuracy when facing unseen new data.
\item
  The accuracy of the training and testing datasets are both affected by the number of predictors used in the model.
\item
  It is not true that the more predictors a model has the better accuracy. Model4 has the worst accuracy on the test dataset.
\item
  The biggest drop of prediction accuracy is model4, the difference is almost 11\% from 85\% on train data to 74\% on the test dataset.
\end{enumerate}

The issue demonstrated by model4 is called overfitting. That is a phenomenon that a model has a higher prediction accuracy on the training dataset and subsequently drops prediction accuracy on the unseen data. Overfitting can be a consequence of many factors. One of the factors has been illustrated with the model4. \textbf{That is the more predictors a model used the more chance of the model's overfitting}. this is because the model will be well fitted with the training dataset. This can be seen from the prediction accuracy on the training dataset. The overfit will not suit the unseen data so the model will perform badly on the test dataset.

We will investigate overfitting by a method called ``Cross Validation'' later in Chapters 10.

\hypertarget{exercises-5}{%
\section*{Exercises}\label{exercises-5}}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Prove there is overfitting with model3 and model4. The idea is to take certain samples from the training dataset and form a validate dataset. Remove the value of attribute \emph{survived} from the validate dataset. Using models to a prediction on validate dataset. Compare their results. If model3 performs better than model2 on the validate dataset but not on the test dataset (which is given by Kaggle), it shows the model has an overfitting problem. Prove this exists on both model3 and model4.
\item
  Build a decision tree model using different predictors.
\item
  Investigate the predictor's number and the same number but different predictors to find the highest prediction accuracy on validation dataset.
\end{enumerate}

\hypertarget{titiannic-prediction-with-random-forest}{%
\chapter{Titiannic Prediction with Random Forest}\label{titiannic-prediction-with-random-forest}}

\begin{center}\includegraphics[width=0.8\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/images/forest-tree} \end{center}

\begin{quote}
Can't see forest for the trees.

\begin{verbatim}
                                -- English Proverb
\end{verbatim}
\end{quote}

As we can see from the previous chapter, the decision tree models do not perform well in our prediction. In this chapter, we will try different models to see if we can improve the model's accuracy by using \textbf{Random Forest} model\index{Random Forest}.

Random Forest is one of the powerful ensembling machine learning algorithms which works by creating multiple decision trees and combining the output generated by each of the decision trees through a voting mechanism to produce an output based on the majority of decision trees' votes \citep{Trevor2013}. Figure \ref{fig:forest} is an example of a random forest.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/images/Random_forest_diagram_complete} 

}

\caption{Example of the Random Forest.}\label{fig:forest}
\end{figure}

In the random forest, its decision tree classifier does not select all the data samples and attributes in each tree. Instead, every individual tree randomly selects data samples and attributes and combines the output at the end. The forest then removes the bias that a single decision tree might introduce in the model. The combination of the individual tree's result is done in a vote carried out to find the result with the highest frequency. A test dataset is evaluated based on these outputs to get the final predicted results. Because of the evaluation process, the random forest model will have an estimated prediction accuracy once constructed. This models' estimated accuracy can be used to compare between random forest models \citep{Aleksandra2021}.

\hypertarget{steps-to-build-a-random-forest}{%
\section{Steps to Build a Random Forest}\label{steps-to-build-a-random-forest}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Randomly select \(k\) attributes from total \(m\) attributes where \(k < m\), the default value of \(k\) is generally \(\sqrt{m}\).
\item
  Among the \(k\) attributes, calculate the node \(d\) using the \textbf{best split point}
\item
  Split the node into a number of nodes using the \textbf{best split method}. See Section @ref(best\_split), by default R random Forest, uses Gini impurity values
\item
  Repeat the previous steps build an individual decision tree
\item
  Build a forest by repeating all steps for \(n\) number times to create \(n\) number of trees
\end{enumerate}

After the random forest trees and classifiers are created, predictions can be made using the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Run the test data through the rules of each decision tree to predict the outcome and then
\item
  Store that predicted target outcome
\item
  Calculate the votes for each of the predicted targets
\item
  Output the most highly voted predicted target as the final prediction
\end{enumerate}

Similar to the decision tree model, the random forest also has many implementations already built. You do not need to write code to do the actual model construction. In R, you can use a package called `randomForest'. There are a number of terminologies that are used in random forest algorithms that need to be understood, such as:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Variance}. When there is a change in the training data algorithm, this is the measure of that change. The most commonly used parameters to reflect changes are \emph{ntree} and \emph{mtry}.
\item
  \textbf{Bagging}. This is a variance-reducing method that trains the model based on random sub-samples of training data.
\item
  \textbf{Out-of-bag (OOB)} error estimate - The random forest classifier is trained using bootstrap aggregation, where each new tree is fit from a bootstrap sample of the training dataset. The out-of-bag (OOB) error is the average error for each calculation using predictions from the trees that do not contain their respective bootstrap sample. This enables the random forest classifier to be adjusted and validated during training.
\end{enumerate}

Let's now look at how we can implement the random forest algorithm for our Titanic prediction.
R provides \texttt{\textquotesingle{}randomForest\textquotesingle{}} package. You can check the details of the package for full usage. We will start with a direct function call with its default settings and we may change settings later. We will also use the original attributes first and then use re-engineered attributes to see if we can improve on the model.

\hypertarget{random-forest-with-key-predictors}{%
\section{Random Forest with Key Predictors}\label{random-forest-with-key-predictors}}

The process of using \texttt{randomForest} package to build an RF model is the same as the decision tree package \texttt{rpart}. Note also if a dependent (response) variable is a factor, classification is assumed, otherwise, regression is assumed. So to uses \texttt{randomForest}, we need to convert the dependent variable into a factor.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# convert variables into factor}
\CommentTok{# convert other attributes which really are categorical data but in form of numbers}
\NormalTok{train}\OperatorTok{$}\NormalTok{Group_size <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(train}\OperatorTok{$}\NormalTok{Group_size)}
\CommentTok{#confirm types}
\KeywordTok{sapply}\NormalTok{(train, class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  PassengerId     Survived       Pclass          Sex          Age        SibSp 
##    "integer"     "factor"     "factor"     "factor"    "numeric"    "integer" 
##        Parch       Ticket     Embarked  HasCabinNum  Friend_size      Fare_pp 
##    "integer"     "factor"     "factor"     "factor"    "integer"    "numeric" 
##        Title         Deck Ticket_class  Family_size   Group_size    Age_group 
##     "factor"     "factor"     "factor"    "integer"     "factor"     "factor"
\end{verbatim}

Let us use the same five most related attributes: \emph{Pclass}, \emph{Sex}, \emph{HasCabinNum}, \emph{Deck} and \emph{Fare\_pp} in the decision tree model2. We use all default parameters of the \texttt{randomForest}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Build the random forest model uses pclass, sex, HasCabinNum, Deck and Fare_pp}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{) }\CommentTok{#for reproduction }
\NormalTok{ RF_model1 <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(Survived) }\OperatorTok{~}\StringTok{ }\NormalTok{Sex }\OperatorTok{+}\StringTok{ }\NormalTok{Pclass }\OperatorTok{+}\StringTok{ }\NormalTok{HasCabinNum }\OperatorTok{+}\StringTok{ }\NormalTok{Deck }\OperatorTok{+}\StringTok{ }\NormalTok{Fare_pp, }\DataTypeTok{data=}\NormalTok{train, }\DataTypeTok{importance=}\OtherTok{TRUE}\NormalTok{)}
 \KeywordTok{save}\NormalTok{(RF_model1, }\DataTypeTok{file =} \StringTok{"./data/RF_model1.rda"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let us check model's prediction accuracy.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{load}\NormalTok{(}\StringTok{"./data/RF_model1.rda"}\NormalTok{)}
\NormalTok{RF_model1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
##  randomForest(formula = as.factor(Survived) ~ Sex + Pclass + HasCabinNum +      Deck + Fare_pp, data = train, importance = TRUE) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 2
## 
##         OOB estimate of  error rate: 19.3%
## Confusion matrix:
##     0   1 class.error
## 0 505  44  0.08014572
## 1 128 214  0.37426901
\end{verbatim}

We can see that the model uses default parameters: \texttt{ntree\ =\ 500} and \texttt{mtry\ =\ 1}. The model's estimated accuracy is \textbf{80.7\%}. It is 1 - 19.3\% (\texttt{OOB\ error}).

Let us make a prediction on the training dataset and check the accuracy.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Make your prediction using the validate dataset}
\NormalTok{RF_prediction1 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(RF_model1, train)}
\CommentTok{#check up}
\NormalTok{conMat<-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(RF_prediction1, train}\OperatorTok{$}\NormalTok{Survived)}
\NormalTok{conMat}\OperatorTok{$}\NormalTok{table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           Reference
## Prediction   0   1
##          0 521 112
##          1  28 230
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Misclassification error}
\KeywordTok{paste}\NormalTok{(}\StringTok{'Accuracy ='}\NormalTok{, }\KeywordTok{round}\NormalTok{(conMat}\OperatorTok{$}\NormalTok{overall[}\StringTok{"Accuracy"}\NormalTok{],}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Accuracy = 0.84"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{paste}\NormalTok{(}\StringTok{'Error ='}\NormalTok{, }\KeywordTok{round}\NormalTok{(}\KeywordTok{mean}\NormalTok{(train}\OperatorTok{$}\NormalTok{Survived }\OperatorTok{!=}\StringTok{ }\NormalTok{RF_prediction1), }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Error = 0.16"
\end{verbatim}

We can see that prediction on the training dataset has achieved \textbf{84\%} accuracy.
It has made 107 wrong predictions and 516 correct predictions on death. The prediction on survived is 33 wrong predictions out of 235 correct predictions.

The model has an accuracy of 80\% after learning, but our evaluation of the training dataset achieves 84\%. It has been increased. Compare with the decision tree model2, in which the same attributes were used and the prediction accuracy on the train data was 81\%, the accuracy is also increased. Let us make a prediction on the test dataset and submit it to Kaggle to obtain an accuracy score.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# produce a submit with Kaggle required format that is only two attributes: PassengerId and Survived}
\NormalTok{test}\OperatorTok{$}\NormalTok{Pclass <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(test}\OperatorTok{$}\NormalTok{Pclass)}
\NormalTok{test}\OperatorTok{$}\NormalTok{Group_size <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(test}\OperatorTok{$}\NormalTok{Group_size)}

\CommentTok{#make prediction}
\NormalTok{RF_prediction <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(RF_model1, test)}
\NormalTok{submit <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{PassengerId =}\NormalTok{ test}\OperatorTok{$}\NormalTok{PassengerId, }\DataTypeTok{Survived =}\NormalTok{ RF_prediction)}
\CommentTok{# Write it into a file "RF_Result.CSV"}
\KeywordTok{write.csv}\NormalTok{(submit, }\DataTypeTok{file =} \StringTok{"./data/RF_Result1.CSV"}\NormalTok{, }\DataTypeTok{row.names =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can see our random forest model has scored \textbf{0.76555} by the Kaggle competition. It is interesting to know that the random forest model has not improved on the test dataset compare with the decision tree model with the same predictors. The accuracy was also 0.76555.

Let us record these accuracies,

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Record the results}
\NormalTok{RF_model1_accuracy <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{80}\NormalTok{, }\DecValTok{84}\NormalTok{, }\FloatTok{76.555}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{random-forest-with-more-variables}{%
\section{Random Forest with More Variables}\label{random-forest-with-more-variables}}

Now let us see if We can build a better model if we use more predictors. The predictor we are using is identical to the decision tree model3.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{### RE_model2 with more predictors}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2222}\NormalTok{)}
\NormalTok{ RF_model2 <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(Survived) }\OperatorTok{~}\StringTok{ }\NormalTok{Sex }\OperatorTok{+}\StringTok{ }\NormalTok{Fare_pp }\OperatorTok{+}\StringTok{ }\NormalTok{Pclass }\OperatorTok{+}\StringTok{ }\NormalTok{Title }\OperatorTok{+}\StringTok{ }\NormalTok{Age_group }\OperatorTok{+}\StringTok{ }\NormalTok{Group_size }\OperatorTok{+}\StringTok{ }\NormalTok{Ticket_class  }\OperatorTok{+}\StringTok{ }\NormalTok{Embarked, }
                          \DataTypeTok{data =}\NormalTok{ train,}
                          \DataTypeTok{importance=}\OtherTok{TRUE}\NormalTok{)}
\CommentTok{# # This model will be used in later chapters, so save it in a file and it can be loaded later.}
 \KeywordTok{save}\NormalTok{(RF_model2, }\DataTypeTok{file =} \StringTok{"./data/RF_model2.rda"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can assess the new model,

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{load}\NormalTok{(}\StringTok{"./data/RF_model2.rda"}\NormalTok{)}
\NormalTok{RF_model2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
##  randomForest(formula = as.factor(Survived) ~ Sex + Fare_pp +      Pclass + Title + Age_group + Group_size + Ticket_class +      Embarked, data = train, importance = TRUE) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 2
## 
##         OOB estimate of  error rate: 16.84%
## Confusion matrix:
##     0   1 class.error
## 0 499  50  0.09107468
## 1 100 242  0.29239766
\end{verbatim}

Notice that the default parameter `mtry = 2' and \texttt{ntree\ =\ 500}. It means the number of variables tried at each split is now 2 and the number of trees that can be built is 500. The model's \texttt{estimated\ OOB\ error\ rate} is 16.5\%. It has an increase in comparison with the first model which was 20\%. So the overall accuracy of the model has reached \textbf{83.5\%}.

Let us make a prediction on train Data to verify the model's training accuracy.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# RF_model2 Prediction on train}
\NormalTok{RF_prediction2 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(RF_model2, train)}
\CommentTok{#check up}
\NormalTok{conMat<-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(RF_prediction2, train}\OperatorTok{$}\NormalTok{Survived)}
\NormalTok{conMat}\OperatorTok{$}\NormalTok{table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           Reference
## Prediction   0   1
##          0 529  55
##          1  20 287
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Misclassification error}
\KeywordTok{paste}\NormalTok{(}\StringTok{'Accuracy ='}\NormalTok{, }\KeywordTok{round}\NormalTok{(conMat}\OperatorTok{$}\NormalTok{overall[}\StringTok{"Accuracy"}\NormalTok{],}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Accuracy = 0.92"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{paste}\NormalTok{(}\StringTok{'Error ='}\NormalTok{, }\KeywordTok{round}\NormalTok{(}\KeywordTok{mean}\NormalTok{(train}\OperatorTok{$}\NormalTok{Survived }\OperatorTok{!=}\StringTok{ }\NormalTok{RF_prediction2), }\DecValTok{2}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Error = 0.08"
\end{verbatim}

We can see the model's accuracy of the training dataset has reached 91\%. The result shows that the prediction on survival has 55 wrong predictions out of 527 correct predictions; The prediction on death has 287 correct predictions and 22 wrong predictions. The overall accuracy reaches \textbf{91\%}. It is again higher than the model learning accuracy \textbf{83.5\%}.

It has also increased a bit comparing with the accuracy on the estimated accuracy \textbf{80\%} and the accuracy on train dataset \textbf{84\%} of the random forest RF\_model1. Compare with the decision tree model3, which has identical predictors, the accuracy was \textbf{85\%} on the training dataset.

Let us make another submit to Kaggle to see if the prediction on unseen data has been improved.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# produce a submission and submit to Kaggle }
\NormalTok{test}\OperatorTok{$}\NormalTok{Pclass <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(test}\OperatorTok{$}\NormalTok{Pclass)}
\NormalTok{test}\OperatorTok{$}\NormalTok{Group_size <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(test}\OperatorTok{$}\NormalTok{Group_size)}

\CommentTok{#make prediction}
\NormalTok{RF_prediction <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(RF_model2, test)}
\NormalTok{submit <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{PassengerId =}\NormalTok{ test}\OperatorTok{$}\NormalTok{PassengerId, }\DataTypeTok{Survived =}\NormalTok{ RF_prediction)}
\CommentTok{# Write it into a file "RF_Result.CSV"}
\KeywordTok{write.csv}\NormalTok{(submit, }\DataTypeTok{file =} \StringTok{"./data/RF_Result2.CSV"}\NormalTok{, }\DataTypeTok{row.names =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The feedback shows the prediction only increased a lot with a scored \textbf{0.78947}! It has improved on the RF\_model1 (0.76555) and decision tree model3 (0.77033).

Let us record these various accuracy.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Record RF_model2's results}
\NormalTok{RF_model2_accuracy <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{83.16}\NormalTok{, }\DecValTok{92}\NormalTok{, }\FloatTok{78.95}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{random-forest-with-all-variables}{%
\section{Random Forest with All Variables}\label{random-forest-with-all-variables}}

Now let use the random forest to build a model with the maximum predictors that can be used from attributes. We may not be able to use all the attributes since the \texttt{randomForest} function cannot handle an attribute that is not a factor and has over 53 levels. So, we will not use the attribute \emph{Ticket}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# RF_model3 construction with the maximum predictors}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2233}\NormalTok{)}
\CommentTok{# RF_model3 <- randomForest(Survived ~ Sex + Pclass + Age }
\CommentTok{#                           + SibSp + Parch + Embarked +}
\CommentTok{#                             HasCabinNum + Friend_size +}
\CommentTok{#                             Fare_pp + Title + Deck +}
\CommentTok{#                             Ticket_class + Family_size +}
\CommentTok{#                             Group_size + Age_group, }
\CommentTok{#                           data = train, importance=TRUE)}
\CommentTok{# save(RF_model3, file = "./data/RF_model3.rda")}
\end{Highlighting}
\end{Shaded}

We can assess the new model,

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Display RE_model3's details}
\KeywordTok{load}\NormalTok{(}\StringTok{"./data/RF_model3.rda"}\NormalTok{)}
\NormalTok{RF_model3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
##  randomForest(formula = Survived ~ Sex + Pclass + Age + SibSp +      Parch + Embarked + HasCabinNum + Friend_size + Fare_pp +      Title + Deck + Ticket_class + Family_size + Group_size +      Age_group, data = train, importance = TRUE) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 3
## 
##         OOB estimate of  error rate: 16.95%
## Confusion matrix:
##     0   1 class.error
## 0 485  64   0.1165756
## 1  87 255   0.2543860
\end{verbatim}

Notice that the default parameter \texttt{mtry\ =\ 3} and \texttt{ntree\ =\ 500}. It means the number of variables tried at each split is now 3 and number of trees can be built is 500. The model's estimated OOB error rate is 17\%. It has an increase in comparison with the model2 which was 18\%. So the overall accuracy of the model has reached \textbf{83\%}.

Let us make a prediction on train Data to verify the model's training accuracy.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Make a prediction on Train}
\NormalTok{RF_prediction3 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(RF_model3, train)}
\CommentTok{#check up}
\NormalTok{conMat<-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(RF_prediction3, train}\OperatorTok{$}\NormalTok{Survived)}
\NormalTok{conMat}\OperatorTok{$}\NormalTok{table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           Reference
## Prediction   0   1
##          0 536  38
##          1  13 304
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Misclassification error}
\KeywordTok{paste}\NormalTok{(}\StringTok{'Accuracy ='}\NormalTok{, }\KeywordTok{round}\NormalTok{(conMat}\OperatorTok{$}\NormalTok{overall[}\StringTok{"Accuracy"}\NormalTok{],}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Accuracy = 0.94"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{paste}\NormalTok{(}\StringTok{'Error ='}\NormalTok{, }\KeywordTok{round}\NormalTok{(}\KeywordTok{mean}\NormalTok{(train}\OperatorTok{$}\NormalTok{Survived }\OperatorTok{!=}\StringTok{ }\NormalTok{RF_prediction3), }\DecValTok{2}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Error = 0.06"
\end{verbatim}

We can see the accuracy on train dataset has reached 95\%. The result shows that the prediction on survive has 38 wrong predictions out of 536 correct predictions; The prediction on death has 304 correct predictions and 13 wrong predictions. The overall accuracy reaches \textbf{95\%}. It is again higher than the model learning accuracy \textbf{83\%}.

Let us make another submit to Kaggle to see if the prediction on unseen data has been improved.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# produce a submit with Kaggle }
\NormalTok{test}\OperatorTok{$}\NormalTok{Pclass <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(test}\OperatorTok{$}\NormalTok{Pclass)}
\NormalTok{test}\OperatorTok{$}\NormalTok{Group_size <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(test}\OperatorTok{$}\NormalTok{Group_size)}

\CommentTok{#make prediction}
\NormalTok{RF_prediction <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(RF_model3, test)}
\NormalTok{submit <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{PassengerId =}\NormalTok{ test}\OperatorTok{$}\NormalTok{PassengerId, }\DataTypeTok{Survived =}\NormalTok{ RF_prediction)}
\CommentTok{# Write it into a file "RF_Result.CSV"}
\KeywordTok{write.csv}\NormalTok{(submit, }\DataTypeTok{file =} \StringTok{"./data/RF_Result3.CSV"}\NormalTok{, }\DataTypeTok{row.names =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The score is \textbf{0.77033}. It shows the decrease of the accuracy.

Let us record these various accuracy.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Record RF_model3's results}
\NormalTok{RF_model3_accuracy <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{83}\NormalTok{, }\DecValTok{94}\NormalTok{, }\DecValTok{77}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{comparision-the-three-random-forest-models}{%
\section{Comparision the Three Random Forest Models}\label{comparision-the-three-random-forest-models}}

We have produced three random forest models, each has different performance in terms of prediction accuracy on the test dataset. Let us make a quick comparison among them.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyr)}
\NormalTok{Model <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"RF_Model1"}\NormalTok{,}\StringTok{"RF_Model2"}\NormalTok{,}\StringTok{"RF_Model3"}\NormalTok{)}
\NormalTok{Pre <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Sex, Pclass, HasCabinNum, Deck, Fare_pp"}\NormalTok{, }\StringTok{"Sex, Fare_pp, Pclass, Title, Age_group, Group_size, Ticket_class, Embarked"}\NormalTok{, }\StringTok{"Sex, Pclass, Age, SibSp, Parch, Embarked, HasCabinNum, Friend_size, Fare_pp, Title, Deck, Ticket_class, Family_size, Group_size, Age_group"}\NormalTok{)}

\NormalTok{Learn <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{80.0}\NormalTok{, }\FloatTok{83.16}\NormalTok{, }\FloatTok{83.0}\NormalTok{)}
\NormalTok{Train <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{84}\NormalTok{, }\DecValTok{92}\NormalTok{, }\DecValTok{78}\NormalTok{)}
\NormalTok{Test <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{76.555}\NormalTok{, }\FloatTok{78.95}\NormalTok{, }\FloatTok{77.03}\NormalTok{)}
\NormalTok{df1 <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(Model, Pre, Learn, Train, Test)}
\NormalTok{df2 <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(Model, Learn, Train, Test)}
\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(df1, }\DataTypeTok{longtable =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{booktabs =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{digits =} \DecValTok{2}\NormalTok{, }\DataTypeTok{col.names =}\KeywordTok{c}\NormalTok{(}\StringTok{"Models"}\NormalTok{, }\StringTok{"Predictors"}\NormalTok{, }\StringTok{"Accuracy on Learn"}\NormalTok{, }\StringTok{"Accuracy on Train"}\NormalTok{, }\StringTok{"Accuracy on Test"}\NormalTok{), }
  \DataTypeTok{caption =} \StringTok{'The Comparision among 3 Random Forest models'}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[t]{llrrr}
\caption{\label{tab:RFmodelscomp}The Comparision among 3 Random Forest models}\\
\toprule
Models & Predictors & Accuracy on Learn & Accuracy on Train & Accuracy on Test\\
\midrule
RF\_Model1 & Sex, Pclass, HasCabinNum, Deck, Fare\_pp & 80.00 & 84 & 76.56\\
RF\_Model2 & Sex, Fare\_pp, Pclass, Title, Age\_group, Group\_size, Ticket\_class, Embarked & 83.16 & 92 & 78.95\\
RF\_Model3 & Sex, Pclass, Age, SibSp, Parch, Embarked, HasCabinNum, Friend\_size, Fare\_pp, Title, Deck, Ticket\_class, Family\_size, Group\_size, Age\_group & 83.00 & 78 & 77.03\\
\bottomrule
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.long <-}\StringTok{ }\KeywordTok{gather}\NormalTok{(df2, Dataset, Accuracy, }\OperatorTok{-}\NormalTok{Model, }\DataTypeTok{factor_key =}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ df.long, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Model, }\DataTypeTok{y =}\NormalTok{ Accuracy, }\DataTypeTok{fill =}\NormalTok{ Dataset)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_col}\NormalTok{(}\DataTypeTok{position =} \KeywordTok{position_dodge}\NormalTok{()) }
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{09-random-forest_files/figure-latex/RFmodelcompare-1.pdf}
\caption{\label{fig:RFmodelcompare}Random Froest models' accuracy on model learning, Train dataset and Test dataset.}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  It is not true that the more predictors the better performance with Random Forest models.
\item
  The result of the model validation on the training dataset is not reliable. The higher accuracy on the training dataset does not mean a higher accuracy on the test dataset.
\item
  All the model has a degree of overfitting. That is the accuracy on the test data is lower than the training dataset and even lower than the model its own estimated accuracy while learn or construct it.
\item
  the cause of the overfitting is a complicated issue. It may be related to all the factors: the number of predictors used to build the model, the dataset used to build the model, and the model default parameters.
\end{enumerate}

In comparison with the decision tree models, we have built in the previous Chapter. The random forest models over-perform all the four models on the test dataset. The lowest accuracy is the same as the highest accuracy with the decision tree models (76.55\%).

\hypertarget{summary-7}{%
\section*{Summary}\label{summary-7}}


In this chapter, we have demonstrated the use of random forest models for the Titanic problem. We have tried using different numbers of predictors. Their accuracy on the test dataset has been illustrated in the figure \ref{fig:RFmodelcompare}.

Despite the efforts in features' engineering, the careful selection of the predictors, the random forest models have higher accuracy on the train dataset but fall dramatically with the test dataset. It demonstrated the practical problem in a data science project that is overfitting. Overfitting is a serious problem because it is the test dataset (unseen data) matters in real practice. Overfitting can be discovered and eliminated with Cross-validation that is what we are going to discuss in the next Chapter.

\hypertarget{exercises-6}{%
\section*{Exercises}\label{exercises-6}}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Find out what is ``OOB estimate of error rate''? How to reduce its value?
\item
  In a random forest model, its Confusion matrix shows misclassified samples and their error rate. Explain the concept of the ``Positive error'' and ``Negative error'' how to balance them?
\item
  Try different sampling methods by using the different fold and repeat numbers in the Cross-Validation to see the effect of the tune parameters and model's accuracy.
\item
  Explore train method in caret with different models and methods.
\end{enumerate}

\hypertarget{model-cross-validation}{%
\chapter{Model Cross Validation}\label{model-cross-validation}}

\begin{center}\includegraphics[width=0.8\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/images/knobs} \end{center}

\begin{quote}
``We cannot solve our problems with the same thinking we used
when we created them.''

\begin{verbatim}
                                   -- Albert Einstein
\end{verbatim}
\end{quote}

In the previous two chapters, we have demonstrated how to build prediction models using both \emph{Decision Tree} and \emph{Random Forest}, the two popular prediction models. The models we built have different prediction accuracy. The big problem with the models is their reduced prediction accuracy with the test dataset. An even bigger problem is that the reduction of the prediction accuracy with each model is different and unpredictable. Together they created great difficulty to choose which model should be used for the real applications.

We are lucky because we have a Kaggle competition that provides us with a test dataset and feedback on our model's performance. In real applications, as the titanic competition simulated, the test dataset has no response variable's (survival status) value. We will have no means to compare to evaluate the model's accuracy.

Although we may use the methods as we have used in Chapter 8, where we used our model to predict on the training dataset and made a comparison with the original value to estimate the model's prediction accuracy. A similar method (\texttt{OOB}) is also used in the random forest models (in Chapter 9) to estimate the model's accuracy. We know that our estimated accuracy is not reliable.

There is a systematic method in data science to evaluate a prediction model's performance. It is called ``Cross-Validation (CV)''\index{Cross Validation (CV)}. In this chapter, we will demonstrate how to use a CV to evaluate a model's performance.

\hypertarget{models-underfitting-and-overfitting}{%
\section{Model's Underfitting and Overfitting}\label{models-underfitting-and-overfitting}}

We have experienced problems with both of our decision tree and random forest models. The models have higher estimated accuracy (from the model construction) and much lower accuracy on the test dataset. This Would only mean two things either the prediction model is \textbf{overfitting}\index{overfitting} or is \textbf{underfitting}\index{underfitting}.

let us quickly look at a very graphic example of underfitting and overfitting.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/images/underfit} 

}

\caption{Model's Fitting, Overfitting and Underfitting}\label{fig:modelfit}
\end{figure}

We can see in Figure \ref{fig:modelfit} where the first model is a straight line (a low variance model: \(y\) = \(m\) * \(x\) + \(c\)) fails to capture the underlying parabolic curve in the dataset, this is underfitting. At the other extreme, the high degree polynomial (a low bias model) captures too much of the noise at the same time as the underlying parabola and is overfitting. Although it is following the data points provided (i.e.~the training dataset), this curve is not transferable to new data (i.e.~the test dataset).

Among the models we have produced, the decision tree \texttt{model1} with only attribute \emph{Sex} as its predictor is an example of the underfitting model. It has 78.68\% estimated accuracy on the training dataset but only has 76.56\% accuracy on the test dataset. On the contrary, all our random forest Models have an issue of overfitting.

\hypertarget{general-cross-validation-methods}{%
\section{General Cross Validation Methods}\label{general-cross-validation-methods}}

There are two general CV methods that can be used to validate a prediction model:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Single model CV.
\item
  Multiple models comparison.
\end{enumerate}

\hypertarget{single-model-cross-validation}{%
\subsection*{Single Model Cross Validation}\label{single-model-cross-validation}}


The goal of a single model CV is to test the model's ability to predict new data that was not seen and not used in model construction. So, the problem can be spotted like overfitting or selection bias, in addition, it can also give an insight on how the model will generalize to an independent dataset or an unknown dataset.

One round of CV involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the \emph{training set}\index{training set}), and validating the analysis on the other subset (called the \emph{validation set}\index{validation set}). To reduce variability, in most methods multiple rounds of CV are performed using different partitions, and the validation results are combined (e.g.~averaged) over the rounds to give an estimate of the model's predictive performance.

There are two major cross-validation methods: exhaustive CV and non-exhaustive CV.

\begin{itemize}
\item
  \textbf{Exhaustive CV} learn and test on all possible ways to divide the original sample into a training and a validation set. \textbf{Leave-p-out CV (LpO CV)} is an exhaustive cross-validation method. It involves using \(p\) data samples as the validation dataset and the remaining data samples as the training dataset. This is repeated over and over until all possible ways to divide the original data sample into a training and a validation dataset \(p\).
\item
  \textbf{Non-exhaustive cross-validation}, in the contrary, does not compute all the possible ways of splitting the original data sample but still has a certain coverage. \textbf{\(k\)-fold CV} is typical non-exhaustive cross-validation.
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/images/k-fold} 

}

\caption{K-Folds Cross Validation}\label{fig:k-fold}
\end{figure}

In \(k\)-fold CV, the original data sample is randomly partitioned into \(k\) equal-sized sub-samples. Of the k sub-samples, a single subsample is retained as the validation dataset for testing the model, and the remaining \(k\) − 1 sub-samples are used as training data. The CV process is then repeated \(k\) times, with each of the \(k\) sub-samples used exactly once as the validation data. The \(k\) results can then be averaged to produce a single estimation. The advantage of this method over repeated random sub-sampling is that all observations are used for both training and validation, and each observation is used for validation exactly once. \texttt{10-fold} CV is commonly used in practice.

\hypertarget{general-procedure-of-cv}{%
\subsection*{General Procedure of CV}\label{general-procedure-of-cv}}


The general process of Cross-Validation is as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Split the entire data randomly into \(K\) folds (value of \(K\) shouldn't be too small or too high, ideally we choose 5 to 10 depending on the data size). The higher value of \(K\) leads to a less biased model (but large variance might lead to overfitting), whereas the lower value of \(K\) is similar to the train-test split approach we saw before.
\item
  Then fit the model using the \(K - 1\) folds and validate the model using the remaining \(K\)th fold. Note down the scores/errors.
\item
  Repeat this process until every \(K\) fold serves as the test set. Then take the average of your recorded scores. That will be the performance metric for the model.
\end{enumerate}

We will use examples to demonstrate this procedure.

\hypertarget{cross-validation-on-decision-tree-models}{%
\subsection*{Cross Validation on Decision Tree Models}\label{cross-validation-on-decision-tree-models}}


We have produced four decision tree models in Chapter 8. Let us do cross-validation on \texttt{model2} and \texttt{model3} since they have identical predictors with the random forest \texttt{RF\_model1} and \texttt{RF\_model2} which we will do cross-validation later.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caret)}
\KeywordTok{library}\NormalTok{(rpart)}
\KeywordTok{library}\NormalTok{(rpart.plot)}

\CommentTok{#read Re-engineered dataset}
\NormalTok{RE_data <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"./data/RE_data.csv"}\NormalTok{, }\DataTypeTok{header =} \OtherTok{TRUE}\NormalTok{)}

\CommentTok{#Factorize response variable}
\NormalTok{RE_data}\OperatorTok{$}\NormalTok{Survived <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(RE_data}\OperatorTok{$}\NormalTok{Survived)}

\CommentTok{#Separate Train and test data.}
\NormalTok{train <-}\StringTok{ }\NormalTok{RE_data[}\DecValTok{1}\OperatorTok{:}\DecValTok{891}\NormalTok{, ]}
\NormalTok{test <-}\StringTok{ }\NormalTok{RE_data[}\DecValTok{892}\OperatorTok{:}\DecValTok{1309}\NormalTok{, ]}

\CommentTok{#setup model's train and valid dataset}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1000}\NormalTok{)}
\NormalTok{samp <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(train), }\FloatTok{0.8} \OperatorTok{*}\StringTok{ }\KeywordTok{nrow}\NormalTok{(train))}
\NormalTok{trainData <-}\StringTok{ }\NormalTok{train[samp, ]}
\NormalTok{validData <-}\StringTok{ }\NormalTok{train[}\OperatorTok{-}\NormalTok{samp, ]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random for reproduction}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{3214}\NormalTok{)}
\CommentTok{# specify parameters for cross validation}
\NormalTok{control <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"repeatedcv"}\NormalTok{, }
                        \DataTypeTok{number =} \DecValTok{10}\NormalTok{, }\CommentTok{# number of folds}
                        \DataTypeTok{repeats =} \DecValTok{5}\NormalTok{, }\CommentTok{# repeat times}
                        \DataTypeTok{search =} \StringTok{"grid"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Our cross validation settings are: \texttt{10\ folds}, and \texttt{repeat\ 5} times, with \texttt{Grid} search the optimal parameter. The detailed meaning of each settings refers to \url{http://topepo.github.io/caret/data-splitting.html}.

Let us do cross validation (CV) for Tree \emph{model2},

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1010}\NormalTok{)}
\CommentTok{# Create model from cross validation train data}
\CommentTok{#  Tree_model2_cv <- train(Survived ~ Sex + Pclass + HasCabinNum + Deck + Fare_pp,}
\CommentTok{#                       data = trainData,}
\CommentTok{#                       method = "rpart",}
\CommentTok{#                       trControl = control)}
\CommentTok{# # Due to the computation cost once a model is trained. }
\CommentTok{# # it is better to save it and load later rather than compute a gain}
\CommentTok{# save(Tree_model2_cv, file = "./data/Tree_model2_cv.rda")}
\KeywordTok{load}\NormalTok{(}\StringTok{"./data/Tree_model2_cv.rda"}\NormalTok{)}

\KeywordTok{print.train}\NormalTok{(Tree_model2_cv)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## CART 
## 
## 712 samples
##   5 predictor
##   2 classes: '0', '1' 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 641, 640, 640, 641, 641, 642, ... 
## Resampling results across tuning parameters:
## 
##   cp          Accuracy   Kappa    
##   0.00887199  0.8253404  0.6003541
##   0.03041825  0.7955233  0.5418574
##   0.43726236  0.6908918  0.2281378
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was cp = 0.00887199.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model_accuracy <-}\StringTok{ }\KeywordTok{format}\NormalTok{(Tree_model2_cv}\OperatorTok{$}\NormalTok{results}\OperatorTok{$}\NormalTok{Accuracy[}\DecValTok{1}\NormalTok{], }\DataTypeTok{digits =} \DecValTok{4}\NormalTok{)}
\KeywordTok{paste}\NormalTok{(}\StringTok{"Estimated accuracy:"}\NormalTok{, model_accuracy)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Estimated accuracy: 0.8253"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Model estimated accuracy is 82.53.}
\end{Highlighting}
\end{Shaded}

Display details of the cross validation model,

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Visualize cross validation tree}
\KeywordTok{rpart.plot}\NormalTok{(Tree_model2_cv}\OperatorTok{$}\NormalTok{finalModel, }\DataTypeTok{extra=}\DecValTok{4}\NormalTok{)}
\KeywordTok{plot.train}\NormalTok{(Tree_model2_cv)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{10-model-cross-validation_files/figure-latex/model2CV-1} \includegraphics[width=0.5\linewidth]{10-model-cross-validation_files/figure-latex/model2CV-2} 

}

\caption{Decision Tree CV model2.}\label{fig:model2CV}
\end{figure}

Let us record the model's accuracy on \emph{trainData}, \emph{validData}, and \emph{test} dataset. Remember \emph{trainData} and \emph{validData} are randomly partitioned from the \emph{train} dataset.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{### Access accuracy on different datasets}
\CommentTok{# prediction's Confusion Matrix on the trainData }
\NormalTok{predict_train <-}\KeywordTok{predict}\NormalTok{(Tree_model2_cv, trainData)}
\NormalTok{conMat <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(predict_train, trainData}\OperatorTok{$}\NormalTok{Survived)}
\NormalTok{conMat}\OperatorTok{$}\NormalTok{table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           Reference
## Prediction   0   1
##          0 431 104
##          1  18 159
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# prediction's Accuracy on the trainData }
\NormalTok{predict_train_accuracy <-}\StringTok{ }\KeywordTok{format}\NormalTok{(conMat}\OperatorTok{$}\NormalTok{overall[}\StringTok{"Accuracy"}\NormalTok{], }\DataTypeTok{digits=}\DecValTok{4}\NormalTok{)}
\KeywordTok{paste}\NormalTok{(}\StringTok{"trainData Accuracy:"}\NormalTok{, predict_train_accuracy)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "trainData Accuracy: 0.8287"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# prediction's Confusion Matrix on the validData}
\NormalTok{predict_valid <-}\KeywordTok{predict}\NormalTok{(Tree_model2_cv, validData)}
\NormalTok{conMat <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(predict_valid, validData}\OperatorTok{$}\NormalTok{Survived)}
\NormalTok{conMat}\OperatorTok{$}\NormalTok{table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           Reference
## Prediction  0  1
##          0 93 36
##          1  7 43
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# prediction's Accuracy on the validData}
\NormalTok{predict_valid_accuracy <-}\StringTok{ }\KeywordTok{format}\NormalTok{(conMat}\OperatorTok{$}\NormalTok{overall[}\StringTok{"Accuracy"}\NormalTok{], }\DataTypeTok{digits=}\DecValTok{4}\NormalTok{)}
\KeywordTok{paste}\NormalTok{(}\StringTok{"viladData Accuracy:"}\NormalTok{, predict_valid_accuracy)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "viladData Accuracy: 0.7598"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# predict on test}
\NormalTok{predict_test <-}\KeywordTok{predict}\NormalTok{(Tree_model2_cv, test)}
\NormalTok{submit <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{PassengerId =}\NormalTok{ test}\OperatorTok{$}\NormalTok{PassengerId, }\DataTypeTok{Survived =} \KeywordTok{as.factor}\NormalTok{(predict_test))}
\KeywordTok{write.csv}\NormalTok{(submit, }\DataTypeTok{file =} \StringTok{"./data/Tree_model2_CV.CSV"}\NormalTok{, }\DataTypeTok{row.names =} \OtherTok{FALSE}\NormalTok{)}
\CommentTok{# test accuracy 0.75837}
\KeywordTok{paste}\NormalTok{(}\StringTok{"Test Accuracy:"}\NormalTok{, }\FloatTok{0.7584}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Test Accuracy: 0.7584"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# accumulate model's accuracy}
\NormalTok{name <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Esti Accu"}\NormalTok{, }\StringTok{"Train Accu"}\NormalTok{, }\StringTok{"Valid Accu"}\NormalTok{, }\StringTok{"Test Accu"}\NormalTok{)}
\NormalTok{Tree_model2_CV_accuracy <-}\StringTok{ }\KeywordTok{c}\NormalTok{(model_accuracy, predict_train_accuracy, predict_valid_accuracy, }\FloatTok{0.7584}\NormalTok{)}
\KeywordTok{names}\NormalTok{(Tree_model2_CV_accuracy) <-}\StringTok{ }\NormalTok{name}
\NormalTok{Tree_model2_CV_accuracy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Esti Accu Train Accu Valid Accu  Test Accu 
##   "0.8253"   "0.8287"   "0.7598"   "0.7584"
\end{verbatim}

We can see the tree differences from Figure @ref(fig:tree\_model2\_CV) and Figure \ref{fig:tree2}. We can also see that despite the model tried the best parameters, the prediction accuracy on the test dataset is dropped from 0.76555 (default decision tree) to 0.75837. It shows that the model construction has reached the best since the change of the tree structure does not increase the accuracy.

The drop of the accuracy may caused by the reduction of the size of the training dataset. It reflects the second possible cause of the overfitting, that is the size of the training sample. Recall that decision tree \texttt{model2} was trained on the \texttt{train} dataset and now it is trained on the \texttt{trainData}. the later is a random subset of the \texttt{train} dataset and only has 80 percent of the data samples. That is to say, the smaller of the training dataset the more chance of the inaccurate prediction accuracy on the test dataset (overfitting or underfitting).

Let us do cross validation on tree \emph{model3},

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# CV on model3}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\CommentTok{# Tree_model3_cv <- train(Survived ~ Sex + Fare_pp + Pclass + Title + Age_group + Group_size + Ticket_class  + Embarked,}
\CommentTok{# }
\CommentTok{#                        data = trainData,}
\CommentTok{#                        method = "rpart",}
\CommentTok{#                        trControl = control)}
\CommentTok{# }
\CommentTok{# save(Tree_model3_cv, file = "./data/Tree_model3_cv.rda")}
\KeywordTok{load}\NormalTok{(}\StringTok{"./data/Tree_model3_cv.rda"}\NormalTok{)}
\CommentTok{# show model details}
\KeywordTok{print.train}\NormalTok{(Tree_model3_cv)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## CART 
## 
## 712 samples
##   8 predictor
##   2 classes: '0', '1' 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 641, 641, 641, 641, 641, 641, ... 
## Resampling results across tuning parameters:
## 
##   cp          Accuracy   Kappa    
##   0.03802281  0.8209755  0.6124019
##   0.05323194  0.7934831  0.5627884
##   0.42585551  0.6979987  0.2637528
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was cp = 0.03802281.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# record accuracy}
\NormalTok{model_accuracy <-}\StringTok{ }\KeywordTok{format}\NormalTok{(Tree_model3_cv}\OperatorTok{$}\NormalTok{results}\OperatorTok{$}\NormalTok{Accuracy[}\DecValTok{1}\NormalTok{], }\DataTypeTok{digits =} \DecValTok{4}\NormalTok{)}
\KeywordTok{paste}\NormalTok{(}\StringTok{"Estimated accuracy:"}\NormalTok{, model_accuracy)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Estimated accuracy: 0.821"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# accuracy is 0.82.}
\end{Highlighting}
\end{Shaded}

Visualize model,

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Visualize cross validation tree}
\KeywordTok{rpart.plot}\NormalTok{(Tree_model3_cv}\OperatorTok{$}\NormalTok{finalModel, }\DataTypeTok{extra=}\DecValTok{4}\NormalTok{)}
\KeywordTok{plot.train}\NormalTok{(Tree_model3_cv)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{10-model-cross-validation_files/figure-latex/treemodel3CV-1} \includegraphics[width=0.5\linewidth]{10-model-cross-validation_files/figure-latex/treemodel3CV-2} 

}

\caption{Decision Tree CV model3.}\label{fig:treemodel3CV}
\end{figure}

Record model's accuracy,

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{### Access accuracy on different datasets}
\CommentTok{# prediction's Confusion Matrix on the trainData }
\NormalTok{predict_train <-}\KeywordTok{predict}\NormalTok{(Tree_model3_cv, trainData)}
\NormalTok{conMat <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(predict_train, trainData}\OperatorTok{$}\NormalTok{Survived)}
\NormalTok{conMat}\OperatorTok{$}\NormalTok{table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           Reference
## Prediction   0   1
##          0 387  61
##          1  62 202
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# prediction's Accuracy on the trainData }
\NormalTok{predict_train_accuracy <-}\StringTok{ }\KeywordTok{format}\NormalTok{(conMat}\OperatorTok{$}\NormalTok{overall[}\StringTok{"Accuracy"}\NormalTok{], }\DataTypeTok{digits=}\DecValTok{4}\NormalTok{)}
\KeywordTok{paste}\NormalTok{(}\StringTok{"trainData Accuracy:"}\NormalTok{, predict_train_accuracy)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "trainData Accuracy: 0.8272"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# prediction's Confusion Matrix on the validData}
\NormalTok{predict_valid <-}\KeywordTok{predict}\NormalTok{(Tree_model3_cv, validData)}
\NormalTok{conMat <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(predict_valid, validData}\OperatorTok{$}\NormalTok{Survived)}
\NormalTok{conMat}\OperatorTok{$}\NormalTok{table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           Reference
## Prediction  0  1
##          0 90 24
##          1 10 55
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# prediction's Accuracy on the validData }
\NormalTok{predict_valid_accuracy <-}\StringTok{ }\KeywordTok{format}\NormalTok{(conMat}\OperatorTok{$}\NormalTok{overall[}\StringTok{"Accuracy"}\NormalTok{], }\DataTypeTok{digits=}\DecValTok{4}\NormalTok{)}
\KeywordTok{paste}\NormalTok{(}\StringTok{"validData Accuracy:"}\NormalTok{, predict_valid_accuracy)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "validData Accuracy: 0.8101"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#predict on test}
\NormalTok{predict_test <-}\KeywordTok{predict}\NormalTok{(Tree_model3_cv, test)}
\NormalTok{submit <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{PassengerId =}\NormalTok{ test}\OperatorTok{$}\NormalTok{PassengerId, }\DataTypeTok{Survived =} \KeywordTok{as.factor}\NormalTok{(predict_test))}
\KeywordTok{write.csv}\NormalTok{(submit, }\DataTypeTok{file =} \StringTok{"./data/Tree_model3_CV.CSV"}\NormalTok{, }\DataTypeTok{row.names =} \OtherTok{FALSE}\NormalTok{)}

\CommentTok{## test accuracy is 0.77751}
\KeywordTok{paste}\NormalTok{(}\StringTok{"Test Accuracy:"}\NormalTok{, }\FloatTok{0.7775}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Test Accuracy: 0.7775"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# accumulate model's accuracy}
\NormalTok{Tree_model3_CV_accuracy <-}\StringTok{ }\KeywordTok{c}\NormalTok{(model_accuracy, predict_train_accuracy, predict_valid_accuracy, }\FloatTok{0.7775}\NormalTok{)}
\KeywordTok{names}\NormalTok{(Tree_model3_CV_accuracy) <-}\StringTok{ }\NormalTok{name}
\NormalTok{Tree_model3_CV_accuracy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Esti Accu Train Accu Valid Accu  Test Accu 
##    "0.821"   "0.8272"   "0.8101"   "0.7775"
\end{verbatim}

The results show a consistent prediction accuracy. The accuracy of the test dataset has been increased from 0.77033 (Tree model3) to 0.7775. The point perhaps is that the increase of predictors does improve the accuracy (so far).

Based on the two cross-validations we have done to the two decision tree models: \texttt{model2} and \texttt{model3}, we can conclude that the decision tree model's default settings are nearly their best. This is because after the cross-validations with \texttt{10\ folds} and \texttt{repeat\ 5\ times} and \texttt{Grid} search have been carried, we did not manage to improve the models' accuracy.

\hypertarget{cross-validation-on-random-forest-models}{%
\subsection*{Cross Validation on Random Forest Models}\label{cross-validation-on-random-forest-models}}


Now, Let us try the same cross validation on the two random forest models constructed in Chapter 9.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set seed for reproduction}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2307}\NormalTok{)}
\NormalTok{RF_model1_cv <-}\StringTok{ }\KeywordTok{train}\NormalTok{(Survived }\OperatorTok{~}\StringTok{ }\NormalTok{Sex }\OperatorTok{+}\StringTok{ }\NormalTok{Pclass }\OperatorTok{+}\StringTok{ }\NormalTok{HasCabinNum }\OperatorTok{+}\StringTok{      }\NormalTok{Deck }\OperatorTok{+}\StringTok{ }\NormalTok{Fare_pp,}
                       \DataTypeTok{data =}\NormalTok{ trainData,}
                       \DataTypeTok{method =} \StringTok{"rf"}\NormalTok{,}
                       \DataTypeTok{trControl =}\NormalTok{ control)}
\KeywordTok{save}\NormalTok{(RF_model1_cv, }\DataTypeTok{file =} \StringTok{"./data/RF_model1_cv.rda"}\NormalTok{)}
\KeywordTok{load}\NormalTok{(}\StringTok{"./data/RF_model1_cv.rda"}\NormalTok{)}

\CommentTok{# Show CV mdoel's details}
\KeywordTok{print}\NormalTok{(RF_model1_cv)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Random Forest 
## 
## 712 samples
##   5 predictor
##   2 classes: '0', '1' 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 641, 642, 640, 641, 641, 641, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##    2    0.7615079  0.4382518
##    7    0.8401691  0.6412818
##   12    0.8284042  0.6263444
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 7.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(RF_model1_cv}\OperatorTok{$}\NormalTok{results)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   mtry  Accuracy     Kappa AccuracySD    KappaSD
## 1    2 0.7615079 0.4382518 0.05265318 0.13042865
## 2    7 0.8401691 0.6412818 0.04046572 0.09332728
## 3   12 0.8284042 0.6263444 0.04617175 0.10032573
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Record model's accuracy}
\NormalTok{model_accuracy <-}\StringTok{ }\KeywordTok{format}\NormalTok{(RF_model1_cv}\OperatorTok{$}\NormalTok{results}\OperatorTok{$}\NormalTok{Accuracy[}\DecValTok{2}\NormalTok{], }\DataTypeTok{digits =} \DecValTok{4}\NormalTok{)}
\KeywordTok{paste}\NormalTok{(}\StringTok{"Estimated accuracy:"}\NormalTok{, model_accuracy)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Estimated accuracy: 0.8402"
\end{verbatim}

We can see that the best model parameters are \texttt{mtry\ =\ 7} and \texttt{ntree\ =\ 500}, The trained model's best accuracy is 83.88\%.

Let us verify on validate dataset and make prediction on the test dataset.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{### Access accuracy on different datasets}
\CommentTok{# prediction's Confusion Matrix on the trainData }
\NormalTok{predict_train <-}\KeywordTok{predict}\NormalTok{(RF_model1_cv, trainData)}
\NormalTok{conMat <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(predict_train, trainData}\OperatorTok{$}\NormalTok{Survived)}
\NormalTok{conMat}\OperatorTok{$}\NormalTok{table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           Reference
## Prediction   0   1
##          0 432  69
##          1  17 194
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# prediction's Accuracy on the trainData }
\NormalTok{predict_train_accuracy <-}\StringTok{ }\KeywordTok{format}\NormalTok{(conMat}\OperatorTok{$}\NormalTok{overall[}\StringTok{"Accuracy"}\NormalTok{], }\DataTypeTok{digits=}\DecValTok{4}\NormalTok{)}
\KeywordTok{paste}\NormalTok{(}\StringTok{"trainData Accuracy:"}\NormalTok{, predict_train_accuracy)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "trainData Accuracy: 0.8792"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# prediction's Confusion Matrix on the validData}
\NormalTok{predict_valid <-}\KeywordTok{predict}\NormalTok{(RF_model1_cv, validData)}
\NormalTok{conMat <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(predict_valid, validData}\OperatorTok{$}\NormalTok{Survived)}
\NormalTok{conMat}\OperatorTok{$}\NormalTok{table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           Reference
## Prediction  0  1
##          0 88 34
##          1 12 45
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# prediction's Accuracy on the validData }
\NormalTok{predict_valid_accuracy <-}\StringTok{ }\KeywordTok{format}\NormalTok{(conMat}\OperatorTok{$}\NormalTok{overall[}\StringTok{"Accuracy"}\NormalTok{], }\DataTypeTok{digits=}\DecValTok{4}\NormalTok{)}
\KeywordTok{paste}\NormalTok{(}\StringTok{"validData Accuracy:"}\NormalTok{, predict_valid_accuracy)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "validData Accuracy: 0.743"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# predict on test}
\NormalTok{predict_test <-}\KeywordTok{predict}\NormalTok{(RF_model1_cv, test)}
\NormalTok{submit <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{PassengerId =}\NormalTok{ test}\OperatorTok{$}\NormalTok{PassengerId, }\DataTypeTok{Survived =} \KeywordTok{as.factor}\NormalTok{(predict_test))}
\KeywordTok{write.csv}\NormalTok{(submit, }\DataTypeTok{file =} \StringTok{"./data/RF_model1_CV.CSV"}\NormalTok{, }\DataTypeTok{row.names =} \OtherTok{FALSE}\NormalTok{)}

\CommentTok{## test accuracy 0.74641}
\KeywordTok{paste}\NormalTok{(}\StringTok{"Test Accuracy:"}\NormalTok{, }\FloatTok{0.7464}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Test Accuracy: 0.7464"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# accumulate model's accuracy}
\NormalTok{RF_model1_cv_accuracy <-}\StringTok{ }\KeywordTok{c}\NormalTok{(model_accuracy, predict_train_accuracy, predict_valid_accuracy, }\FloatTok{0.7464}\NormalTok{)}
\KeywordTok{names}\NormalTok{(RF_model1_cv_accuracy) <-}\StringTok{ }\NormalTok{name}
\NormalTok{RF_model1_cv_accuracy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Esti Accu Train Accu Valid Accu  Test Accu 
##   "0.8402"   "0.8792"    "0.743"   "0.7464"
\end{verbatim}

The \texttt{trainData} set was randomly selected 80 percent of train dataset, the random forest parameter was set to \texttt{mtry\ =\ 7}, \texttt{ntree\ =\ 500} and the cross-validation settings were \texttt{fold\ =\ 10} and \texttt{repeats=\ 5}. They all combined together trained a CV model. The model's prediction accuracy is pretty bad with 74.6\% on the test dataset. The random forest model \texttt{RF\_model1} using the same predictors and the default random forest settings (\texttt{mtry\ =\ 1}, \texttt{ntree\ =\ 500}), trained on no split train dataset has a prediction accuracy of 0.76555.

Let us try on random forest \texttt{model2},

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set seed for reproduction}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2300}\NormalTok{)}

\CommentTok{# RF_model2_cv <- train(Survived ~ Sex + Fare_pp + Pclass + Title + Age_group + Group_size + Ticket_class + Embarked,}
\CommentTok{#                        data = trainData, }
\CommentTok{#                        method = "rf", }
\CommentTok{#                        trControl = control)}
\CommentTok{# # This model will be used in chapter 12. so it is saved into a file for late to be loaded}
\CommentTok{# save(RF_model2_cv, file = "./data/RF_model2_cv.rda")}

\KeywordTok{load}\NormalTok{(}\StringTok{"./data/RF_model2_cv.rda"}\NormalTok{)}

\CommentTok{# Show CV mdoel's details}
\KeywordTok{print}\NormalTok{(RF_model2_cv)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Random Forest 
## 
## 712 samples
##   8 predictor
##   2 classes: '0', '1' 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 641, 640, 642, 641, 640, 641, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##    2    0.8105369  0.5725714
##   17    0.8404408  0.6518406
##   32    0.8312040  0.6320845
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 17.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(RF_model2_cv}\OperatorTok{$}\NormalTok{results)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   mtry  Accuracy     Kappa AccuracySD   KappaSD
## 1    2 0.8105369 0.5725714 0.04319425 0.1018195
## 2   17 0.8404408 0.6518406 0.04065381 0.0900002
## 3   32 0.8312040 0.6320845 0.04514800 0.1001118
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Record model's accuracy}
\NormalTok{mode2_accuracy <-}\StringTok{ }\KeywordTok{format}\NormalTok{(RF_model2_cv}\OperatorTok{$}\NormalTok{results}\OperatorTok{$}\NormalTok{Accuracy[}\DecValTok{2}\NormalTok{], }\DataTypeTok{digits =} \DecValTok{4}\NormalTok{)}
\KeywordTok{paste}\NormalTok{(}\StringTok{"Estimated accuracy:"}\NormalTok{, mode2_accuracy)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Estimated accuracy: 0.8404"
\end{verbatim}

Let us calculate model's accuracy,

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{### Access accuracy on different datasets}
\CommentTok{# prediction's Confusion Matrix on the trainData }
\NormalTok{predict_train <-}\KeywordTok{predict}\NormalTok{(RF_model2_cv, trainData)}
\NormalTok{conMat <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(predict_train, trainData}\OperatorTok{$}\NormalTok{Survived)}
\NormalTok{conMat}\OperatorTok{$}\NormalTok{table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           Reference
## Prediction   0   1
##          0 441  19
##          1   8 244
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# prediction's Accuracy on the trainData }
\NormalTok{predict_train_accuracy <-}\StringTok{ }\KeywordTok{format}\NormalTok{(conMat}\OperatorTok{$}\NormalTok{overall[}\StringTok{"Accuracy"}\NormalTok{], }\DataTypeTok{digits=}\DecValTok{4}\NormalTok{)}
\KeywordTok{paste}\NormalTok{(}\StringTok{"trainData Accuracy:"}\NormalTok{, predict_train_accuracy)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "trainData Accuracy: 0.9621"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# prediction's Confusion Matrix on the validData}
\NormalTok{predict_valid <-}\KeywordTok{predict}\NormalTok{(RF_model2_cv, validData)}
\NormalTok{conMat <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(predict_valid, validData}\OperatorTok{$}\NormalTok{Survived)}
\NormalTok{conMat}\OperatorTok{$}\NormalTok{table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           Reference
## Prediction  0  1
##          0 86 30
##          1 14 49
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# prediction's Accuracy on the validData }
\NormalTok{predict_valid_accuracy <-}\StringTok{ }\KeywordTok{format}\NormalTok{(conMat}\OperatorTok{$}\NormalTok{overall[}\StringTok{"Accuracy"}\NormalTok{], }\DataTypeTok{digits=}\DecValTok{4}\NormalTok{)}
\KeywordTok{paste}\NormalTok{(}\StringTok{"validData Accuracy:"}\NormalTok{, predict_valid_accuracy)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "validData Accuracy: 0.7542"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#predict on test}
\NormalTok{predict_test <-}\KeywordTok{predict}\NormalTok{(RF_model2_cv, test)}
\NormalTok{submit <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{PassengerId =}\NormalTok{ test}\OperatorTok{$}\NormalTok{PassengerId, }\DataTypeTok{Survived =} \KeywordTok{as.factor}\NormalTok{(predict_test))}
\KeywordTok{write.csv}\NormalTok{(submit, }\DataTypeTok{file =} \StringTok{"./data/RF_model2_CV.CSV"}\NormalTok{, }\DataTypeTok{row.names =} \OtherTok{FALSE}\NormalTok{)}

\CommentTok{## test accuracy 0.75119}
\KeywordTok{paste}\NormalTok{(}\StringTok{"Test Accuracy:"}\NormalTok{, }\FloatTok{0.7512}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Test Accuracy: 0.7512"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# accumulate model's accuracy}
\NormalTok{RF_model2_cv_accuracy <-}\StringTok{ }\KeywordTok{c}\NormalTok{(mode2_accuracy, predict_train_accuracy, predict_valid_accuracy, }\FloatTok{0.7512}\NormalTok{)}
\KeywordTok{names}\NormalTok{(RF_model2_cv_accuracy) <-}\StringTok{ }\NormalTok{name}
\NormalTok{RF_model2_cv_accuracy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Esti Accu Train Accu Valid Accu  Test Accu 
##   "0.8404"   "0.9621"   "0.7542"   "0.7512"
\end{verbatim}

We have used \texttt{10\ folds} and \texttt{repeated\ 5\ times} cross-validation with 80\% of the train dataset to build and validate 4 models: two from \emph{decision tree} and two from \emph{random forest}. The different accuracy measurements with different datasets have been collected. Let us put them into one table and plot them in the graph, so we can make a comparison.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyr)}
\NormalTok{Model <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Tree_Modlel2"}\NormalTok{,}\StringTok{"Tree_Model3"}\NormalTok{,}\StringTok{"RF_model1"}\NormalTok{,}\StringTok{"RF_model2"}\NormalTok{)}

\CommentTok{# Show individual models' accuracy}
\NormalTok{Tree_model2_CV_accuracy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Esti Accu Train Accu Valid Accu  Test Accu 
##   "0.8253"   "0.8287"   "0.7598"   "0.7584"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Tree_model3_CV_accuracy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Esti Accu Train Accu Valid Accu  Test Accu 
##    "0.821"   "0.8272"   "0.8101"   "0.7775"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RF_model1_cv_accuracy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Esti Accu Train Accu Valid Accu  Test Accu 
##   "0.8402"   "0.8792"    "0.743"   "0.7464"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RF_model2_cv_accuracy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Esti Accu Train Accu Valid Accu  Test Accu 
##   "0.8404"   "0.9621"   "0.7542"   "0.7512"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#preparee for table construction}
\NormalTok{Pre <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Sex, Pclass, HasCabinNum, Deck, Fare_pp"}\NormalTok{, }\StringTok{"Sex, Fare_pp, Pclass, Title, Age_group, Group_size, Ticket_class, Embarked"}\NormalTok{, }\StringTok{"Sex, Pclass, HasCabinNum, Deck, Fare_pp"}\NormalTok{, }\StringTok{"Sex, Fare_pp, Pclass, Title, Age_group, Group_size, Ticket_class, Embarked"}\NormalTok{)}
\CommentTok{#}
\NormalTok{Learn <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(Tree_model2_CV_accuracy[}\DecValTok{1}\NormalTok{])}\OperatorTok{*}\DecValTok{100}\NormalTok{, }\KeywordTok{as.numeric}\NormalTok{(Tree_model3_CV_accuracy[}\DecValTok{1}\NormalTok{])}\OperatorTok{*}\DecValTok{100}\NormalTok{, }\KeywordTok{as.numeric}\NormalTok{(RF_model1_cv_accuracy[}\DecValTok{1}\NormalTok{])}\OperatorTok{*}\DecValTok{100}\NormalTok{, }\KeywordTok{as.numeric}\NormalTok{(RF_model2_cv_accuracy[}\DecValTok{1}\NormalTok{])}\OperatorTok{*}\DecValTok{100}\NormalTok{)}
\CommentTok{#}
\NormalTok{Train <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(Tree_model2_CV_accuracy[}\DecValTok{2}\NormalTok{])}\OperatorTok{*}\DecValTok{100}\NormalTok{, }\KeywordTok{as.numeric}\NormalTok{(Tree_model3_CV_accuracy[}\DecValTok{2}\NormalTok{])}\OperatorTok{*}\DecValTok{100}\NormalTok{, }\KeywordTok{as.numeric}\NormalTok{(RF_model1_cv_accuracy[}\DecValTok{2}\NormalTok{])}\OperatorTok{*}\DecValTok{100}\NormalTok{, }\KeywordTok{as.numeric}\NormalTok{(RF_model2_cv_accuracy[}\DecValTok{2}\NormalTok{])}\OperatorTok{*}\DecValTok{100}\NormalTok{)}
\CommentTok{#}
\NormalTok{Valid <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(Tree_model2_CV_accuracy[}\DecValTok{3}\NormalTok{])}\OperatorTok{*}\DecValTok{100}\NormalTok{, }\KeywordTok{as.numeric}\NormalTok{(Tree_model3_CV_accuracy[}\DecValTok{3}\NormalTok{])}\OperatorTok{*}\DecValTok{100}\NormalTok{, }\KeywordTok{as.numeric}\NormalTok{(RF_model1_cv_accuracy[}\DecValTok{3}\NormalTok{])}\OperatorTok{*}\DecValTok{100}\NormalTok{, }\KeywordTok{as.numeric}\NormalTok{(RF_model2_cv_accuracy[}\DecValTok{3}\NormalTok{])}\OperatorTok{*}\DecValTok{100}\NormalTok{)}
\CommentTok{#}
\NormalTok{Test <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(Tree_model2_CV_accuracy[}\DecValTok{4}\NormalTok{])}\OperatorTok{*}\DecValTok{100}\NormalTok{, }\KeywordTok{as.numeric}\NormalTok{(Tree_model3_CV_accuracy[}\DecValTok{4}\NormalTok{])}\OperatorTok{*}\DecValTok{100}\NormalTok{, }\KeywordTok{as.numeric}\NormalTok{(RF_model1_cv_accuracy[}\DecValTok{4}\NormalTok{])}\OperatorTok{*}\DecValTok{100}\NormalTok{, }\KeywordTok{as.numeric}\NormalTok{(RF_model2_cv_accuracy[}\DecValTok{4}\NormalTok{])}\OperatorTok{*}\DecValTok{100}\NormalTok{)}

\CommentTok{# Construct Dataframe for table and plot}
\NormalTok{df1 <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(Model, Pre, Learn, Train, Valid, Test)}
\NormalTok{df2 <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(Model, Learn, Train, Valid, Test)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# show in table}
\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(df1, }\DataTypeTok{longtable =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{booktabs =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{digits =} \DecValTok{2}\NormalTok{, }\DataTypeTok{col.names =}\KeywordTok{c}\NormalTok{(}\StringTok{"Models"}\NormalTok{, }\StringTok{"Predictors"}\NormalTok{, }\StringTok{"Accuracy on Learn"}\NormalTok{, }\StringTok{"Accuracy on Train"}\NormalTok{, }\StringTok{"Accuracy on Valid"}\NormalTok{,  }\StringTok{"Accuracy on Test"}\NormalTok{), }
  \DataTypeTok{caption =} \StringTok{'The Comparision among 4 CV models'}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[t]{llrrrr}
\caption{\label{tab:CVmodelcom}The Comparision among 4 CV models}\\
\toprule
Models & Predictors & Accuracy on Learn & Accuracy on Train & Accuracy on Valid & Accuracy on Test\\
\midrule
Tree\_Modlel2 & Sex, Pclass, HasCabinNum, Deck, Fare\_pp & 82.53 & 82.87 & 75.98 & 75.84\\
Tree\_Model3 & Sex, Fare\_pp, Pclass, Title, Age\_group, Group\_size, Ticket\_class, Embarked & 82.10 & 82.72 & 81.01 & 77.75\\
RF\_model1 & Sex, Pclass, HasCabinNum, Deck, Fare\_pp & 84.02 & 87.92 & 74.30 & 74.64\\
RF\_model2 & Sex, Fare\_pp, Pclass, Title, Age\_group, Group\_size, Ticket\_class, Embarked & 84.04 & 96.21 & 75.42 & 75.12\\
\bottomrule
\end{longtable}

Plot results in bar chat.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.long <-}\StringTok{ }\KeywordTok{gather}\NormalTok{(df2, Dataset, Accuracy, }\OperatorTok{-}\NormalTok{Model, }\DataTypeTok{factor_key =}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ df.long, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Model, }\DataTypeTok{y =}\NormalTok{ Accuracy, }\DataTypeTok{fill =}\NormalTok{ Dataset)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_col}\NormalTok{(}\DataTypeTok{position =} \KeywordTok{position_dodge}\NormalTok{()) }
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{10-model-cross-validation_files/figure-latex/CVmodelcompare-1.pdf}
\caption{\label{fig:CVmodelcompare}Cross valid models' accuracy on model learning, Traindata dataset. Validdata and Test dataset.}
\end{figure}

From the cross-validation results, we can conclude that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Both decision tree and random forest models default settings are good settings. Despite the dynamic search for the best parameters, the changes in the parameter settings do not affect the prediction accuracy much. So both default settings for the prediction model are acceptable.
\item
  Change of training dataset for model building from \emph{train dataset} to its subset \emph{trainData}, in \texttt{10-fold} and \texttt{5-repeat} cross-validation settings, does not change the order of models' performance in terms of decision tree and random forest. It, however, when considering a single model, does suggest that the number of samples used for a model construction has an impact on the model's prediction results.
\item
  It is clear that the random forest models have overfitting.
\item
  It does not provide a conclusive result that a decision tree is better than a random forest or vice versa.
\end{enumerate}

A general rule seems to suggest that, the more predictors and the more samples used in a model's construction (training), the more likely the model will suffer from overfitting and vice versa.

Therefore to choose a model for real prediction (production), we should choose the model that has the smallest accuracy decrease from the model's training to its verification by the cross-validation.

\hypertarget{multiple-models-comparison}{%
\section{Multiple Models Comparison}\label{multiple-models-comparison}}

Multiple model comparison is also called \emph{Cross Model Validation}\index{Cross Model Validation}. Here the model refers to completely different algorithms. The idea is to use multiple models constructed from the same training dataset and validated using the same verification dataset to find out the performance of the different models.

We have already used the technique to compare our decision tree models and random forest models. Cross-model verification has a broader meaning that refers to the comparison between different models produced by the different algorithms or completely different approaches such as decision tree against random forest or decision tree against Support Vector Machine (SVM).

To demonstrate cross model validation, let us produce a few more models with completely different algorithms with the same predictors as much as possible. Let us use \emph{Sex}, \emph{Fare\_pp}, \emph{Pclass}, \emph{Title}, \emph{Age\_group}, \emph{Group\_size}, \emph{Ticket\_class}, \emph{Embarked} as predictors.

\hypertarget{regression-model-for-titanic}{%
\subsection*{Regression Model for Titanic}\label{regression-model-for-titanic}}


Logistic regression is a classification, not a regression algorithm. It predicts the probability of occurrence of an event by fitting data to a logit function. Hence, it is also known as logit regression \citep{analyticsvidhya2015}. Since it predicts the probability, its output values lie between 0 and 1, we can simply separate (or normalise) them by setting a threshold like (\textgreater{} 0.5).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{LR_Model <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ Survived }\OperatorTok{~}\StringTok{ }\NormalTok{Pclass }\OperatorTok{+}\StringTok{ }\NormalTok{Title }\OperatorTok{+}\StringTok{ }\NormalTok{Sex }\OperatorTok{+}\StringTok{ }\NormalTok{Age_group }\OperatorTok{+}\StringTok{ }\NormalTok{Group_size }\OperatorTok{+}\StringTok{ }\NormalTok{Ticket_class  }\OperatorTok{+}\StringTok{ }\NormalTok{Fare_pp }\OperatorTok{+}\StringTok{ }\NormalTok{Embarked, }\DataTypeTok{family =}\NormalTok{ binomial, }\DataTypeTok{data =}\NormalTok{ trainData)}

\CommentTok{#summary(LR_Model_CV)}
\CommentTok{### Validate on trainData}
\NormalTok{Valid_trainData <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(LR_Model, }\DataTypeTok{newdata =}\NormalTok{ trainData, }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{) }\CommentTok{#prediction threshold}
\NormalTok{Valid_trainData <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(Valid_trainData }\OperatorTok{>}\StringTok{ }\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)  }\CommentTok{# set binary }
\CommentTok{#produce confusion matrix}
\NormalTok{confusion_Mat<-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(trainData}\OperatorTok{$}\NormalTok{Survived),}\KeywordTok{as.factor}\NormalTok{(Valid_trainData))}

\CommentTok{# accuracy on traindata}
\NormalTok{Regression_Acc_Train <-}\StringTok{ }\KeywordTok{round}\NormalTok{(confusion_Mat}\OperatorTok{$}\NormalTok{overall[}\StringTok{"Accuracy"}\NormalTok{]}\OperatorTok{*}\DecValTok{100}\NormalTok{,}\DecValTok{2}\NormalTok{)}
\KeywordTok{paste}\NormalTok{(}\StringTok{'Model Train Accuracy ='}\NormalTok{, Regression_Acc_Train)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Model Train Accuracy = 84.97"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{### Validate on validData}
\NormalTok{validData_Survived_predicted <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(LR_Model, }\DataTypeTok{newdata =}\NormalTok{ validData, }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)  }
\NormalTok{validData_Survived_predicted  <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(validData_Survived_predicted  }\OperatorTok{>}\StringTok{ }\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)  }\CommentTok{# set binary prediction threshold}
\NormalTok{conMat<-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(validData}\OperatorTok{$}\NormalTok{Survived),}\KeywordTok{as.factor}\NormalTok{(validData_Survived_predicted))}

\NormalTok{Regression_Acc_Valid <-}\KeywordTok{round}\NormalTok{(conMat}\OperatorTok{$}\NormalTok{overall[}\StringTok{"Accuracy"}\NormalTok{]}\OperatorTok{*}\DecValTok{100}\NormalTok{,}\DecValTok{2}\NormalTok{)}
\KeywordTok{paste}\NormalTok{(}\StringTok{'Model Valid Accuracy ='}\NormalTok{, Regression_Acc_Valid) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Model Valid Accuracy = 79.89"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{### produce a prediction on test data}
\KeywordTok{library}\NormalTok{(pROC)}
\KeywordTok{auc}\NormalTok{(}\KeywordTok{roc}\NormalTok{(trainData}\OperatorTok{$}\NormalTok{Survived,Valid_trainData))  }\CommentTok{# calculate AUROC curve}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Area under the curve: 0.832
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#predict on test}
\NormalTok{test}\OperatorTok{$}\NormalTok{Survived <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(LR_Model, }\DataTypeTok{newdata =}\NormalTok{ test, }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)  }
\NormalTok{test}\OperatorTok{$}\NormalTok{Survived <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(test}\OperatorTok{$}\NormalTok{Survived }\OperatorTok{>}\StringTok{ }\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)  }\CommentTok{# set binary prediction threshold}
\NormalTok{submit <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{PassengerId =}\NormalTok{ test}\OperatorTok{$}\NormalTok{PassengerId, }\DataTypeTok{Survived =} \KeywordTok{as.factor}\NormalTok{(test}\OperatorTok{$}\NormalTok{Survived))}

\KeywordTok{write.csv}\NormalTok{(submit, }\DataTypeTok{file =} \StringTok{"./data/LG_model1_CV.CSV"}\NormalTok{, }\DataTypeTok{row.names =} \OtherTok{FALSE}\NormalTok{)}
\CommentTok{# Kaggle test accuracy score:0.76555}

\CommentTok{# record accuracy}
\NormalTok{Regr_Acc <-}\StringTok{ }\KeywordTok{c}\NormalTok{(Regression_Acc_Train, Regression_Acc_Valid, }\FloatTok{0.76555}\NormalTok{)}

\NormalTok{acc_names <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Train Accu"}\NormalTok{, }\StringTok{"Valid Accu"}\NormalTok{, }\StringTok{"Test Accu"}\NormalTok{)}
\KeywordTok{names}\NormalTok{(Regr_Acc) <-}\StringTok{ }\NormalTok{acc_names}
\NormalTok{Regr_Acc}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Train Accu Valid Accu  Test Accu 
##   84.97000   79.89000    0.76555
\end{verbatim}

\hypertarget{support-vector-machine-model-for-titanic}{%
\subsection*{Support Vector Machine Model for Titanic}\label{support-vector-machine-model-for-titanic}}


Let us also consider a support vector machine (SVM)\index{support vector machine} model \citep{Cortes1995}. We use the \emph{C-classification} mode. Again, we fit a model with the same set of attributes as in the \emph{logistic regression model}. We use function \texttt{svm()} from the \texttt{e1071} package (\url{https://cran.r-project.org/web/packages/e1071/e1071.pdf}).

We could try to tune the two parameters of the SVM model \texttt{gamma} \& \texttt{cost}, find and select the best parameters (see exercise).

We then use the best model to make predictions. The results of the model are collected for comparison.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#load library}
\KeywordTok{library}\NormalTok{(e1071)}

\CommentTok{# fit the model using default parameters}
\NormalTok{SVM_model<-}\StringTok{ }\KeywordTok{svm}\NormalTok{(Survived }\OperatorTok{~}\StringTok{ }\NormalTok{Pclass }\OperatorTok{+}\StringTok{ }\NormalTok{Title }\OperatorTok{+}\StringTok{ }\NormalTok{Sex }\OperatorTok{+}\StringTok{ }\NormalTok{Age_group }\OperatorTok{+}\StringTok{ }\NormalTok{Group_size }\OperatorTok{+}\StringTok{ }\NormalTok{Ticket_class }\OperatorTok{+}\StringTok{ }\NormalTok{Fare_pp }\OperatorTok{+}\StringTok{ }\NormalTok{Deck }\OperatorTok{+}\StringTok{ }\NormalTok{HasCabinNum }\OperatorTok{+}\StringTok{ }\NormalTok{Embarked, }\DataTypeTok{data=}\NormalTok{trainData, }\DataTypeTok{kernel =} \StringTok{'radial'}\NormalTok{, }\DataTypeTok{type=}\StringTok{"C-classification"}\NormalTok{)}

\CommentTok{#summary(SVM_model)}
\CommentTok{### Validate on trainData}
\NormalTok{Valid_trainData <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(SVM_model, trainData) }
\CommentTok{#produce confusion matrix}
\NormalTok{confusion_Mat<-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(trainData}\OperatorTok{$}\NormalTok{Survived), }\KeywordTok{as.factor}\NormalTok{(Valid_trainData))}

\CommentTok{# output accuracy}
\NormalTok{AVM_Acc_Train <-}\StringTok{ }\KeywordTok{round}\NormalTok{(confusion_Mat}\OperatorTok{$}\NormalTok{overall[}\StringTok{"Accuracy"}\NormalTok{]}\OperatorTok{*}\DecValTok{100}\NormalTok{,}\DecValTok{4}\NormalTok{)}
\KeywordTok{paste}\NormalTok{(}\StringTok{'Model Train Accuracy ='}\NormalTok{, AVM_Acc_Train)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Model Train Accuracy = 84.4101"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{### Validate on validData}
\NormalTok{validData_Survived_predicted <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(SVM_model, validData) }\CommentTok{#produce confusion matrix }
\NormalTok{conMat<-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(validData}\OperatorTok{$}\NormalTok{Survived), }\KeywordTok{as.factor}\NormalTok{(validData_Survived_predicted))}
\CommentTok{# output accuracy}
\NormalTok{AVM_Acc_Valid <-}\StringTok{ }\KeywordTok{round}\NormalTok{(conMat}\OperatorTok{$}\NormalTok{overall[}\StringTok{"Accuracy"}\NormalTok{]}\OperatorTok{*}\DecValTok{100}\NormalTok{,}\DecValTok{4}\NormalTok{)}
\KeywordTok{paste}\NormalTok{(}\StringTok{'Model Valid Accuracy ='}\NormalTok{, AVM_Acc_Valid) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Model Valid Accuracy = 78.2123"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{### make prediction on test}
\CommentTok{# SVM failed to produce a prediction on test because test has Survived col and it has value NA. A work around is assign it with a num like 1.}
\NormalTok{test}\OperatorTok{$}\NormalTok{Survived <-}\DecValTok{1}

\CommentTok{# predict results on test}
\NormalTok{Survived <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(SVM_model, test)}
\NormalTok{solution <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{PassengerId=}\NormalTok{test}\OperatorTok{$}\NormalTok{PassengerId, }\DataTypeTok{Survived =}\NormalTok{Survived)}
\KeywordTok{write.csv}\NormalTok{(solution, }\DataTypeTok{file =} \StringTok{'./data/svm_predicton.csv'}\NormalTok{, }\DataTypeTok{row.names =}\NormalTok{ F)}

\CommentTok{# prediction accuracy on test }
\NormalTok{SVM_Acc <-}\StringTok{ }\KeywordTok{c}\NormalTok{(AVM_Acc_Train, AVM_Acc_Valid, }\FloatTok{0.78947}\NormalTok{)}
\KeywordTok{names}\NormalTok{(SVM_Acc) <-}\StringTok{ }\NormalTok{acc_names}

\CommentTok{# print out}
\NormalTok{SVM_Acc}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Train Accu Valid Accu  Test Accu 
##   84.41010   78.21230    0.78947
\end{verbatim}

\hypertarget{neural-network-models}{%
\subsection*{Neural Network Models}\label{neural-network-models}}


Neural networks\index{Neural networks} are a rapidly developing paradigm for information processing based loosely on how neurons in the brain process information. A neural network consists of multiple layers of nodes, where each node performs a unit of computation and passes the result onto the next node. Multiple nodes can pass inputs to a single node and vice versa.

The neural network also contains a set of weights, which can be refined over time as the network learns from sample data. The weights are used to describe and refine the connection strengths between nodes.

Neural Network with one hidden layer utilizing all features.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load library}
\KeywordTok{library}\NormalTok{(nnet)}

\CommentTok{# train the model}
\NormalTok{xTrain =}\StringTok{ }\NormalTok{train[ , }\KeywordTok{c}\NormalTok{(}\StringTok{"Survived"}\NormalTok{, }\StringTok{"Pclass"}\NormalTok{,}\StringTok{"Title"}\NormalTok{, }\StringTok{"Sex"}\NormalTok{,}\StringTok{"Age_group"}\NormalTok{,}\StringTok{"Group_size"}\NormalTok{, }\StringTok{"Ticket_class"}\NormalTok{, }\StringTok{"Fare_pp"}\NormalTok{, }\StringTok{"Deck"}\NormalTok{, }\StringTok{"HasCabinNum"}\NormalTok{, }\StringTok{"Embarked"}\NormalTok{)]}

\NormalTok{NN_model1 <-}\StringTok{ }\KeywordTok{nnet}\NormalTok{(Survived }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ xTrain, }\DataTypeTok{size=}\DecValTok{10}\NormalTok{, }\DataTypeTok{maxit=}\DecValTok{500}\NormalTok{, }\DataTypeTok{trace=}\OtherTok{FALSE}\NormalTok{)}

\CommentTok{#How do we do on the training data?}
\NormalTok{nn_pred_train_class =}\StringTok{ }\KeywordTok{predict}\NormalTok{(NN_model1, xTrain, }\DataTypeTok{type=}\StringTok{"class"}\NormalTok{ )  }\CommentTok{# yields "0", "1"}
\NormalTok{nn_train_pred =}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(nn_pred_train_class ) }\CommentTok{#transform to 0, 1}
\NormalTok{confusion_Mat<-}\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(nn_train_pred), train}\OperatorTok{$}\NormalTok{Survived)}
\CommentTok{# output accuracy}
\NormalTok{NN_Acc_Train <-}\StringTok{ }\KeywordTok{round}\NormalTok{(confusion_Mat}\OperatorTok{$}\NormalTok{overall[}\StringTok{"Accuracy"}\NormalTok{]}\OperatorTok{*}\DecValTok{100}\NormalTok{,}\DecValTok{4}\NormalTok{)}
\KeywordTok{paste}\NormalTok{(}\StringTok{'Model Train Accuracy ='}\NormalTok{, NN_Acc_Train)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Model Train Accuracy = 89.2256"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#How do we do on the valid data?}
\NormalTok{nn_pred_valid_class =}\StringTok{ }\KeywordTok{predict}\NormalTok{(NN_model1, validData, }\DataTypeTok{type=}\StringTok{"class"}\NormalTok{ )  }\CommentTok{# yields "0", "1"}
\NormalTok{nn_valid_pred =}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(nn_pred_valid_class ) }\CommentTok{#transform to 0, 1}
\NormalTok{confusion_Mat<-}\KeywordTok{confusionMatrix}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(nn_valid_pred), validData}\OperatorTok{$}\NormalTok{Survived)}
\CommentTok{# output accuracy}
\NormalTok{NN_Acc_Valid <-}\StringTok{ }\KeywordTok{round}\NormalTok{(confusion_Mat}\OperatorTok{$}\NormalTok{overall[}\StringTok{"Accuracy"}\NormalTok{]}\OperatorTok{*}\DecValTok{100}\NormalTok{,}\DecValTok{4}\NormalTok{)}
\KeywordTok{paste}\NormalTok{(}\StringTok{'Model valid Accuracy ='}\NormalTok{, NN_Acc_Valid)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Model valid Accuracy = 87.7095"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#make a prediction on test data}
\NormalTok{nn_pred_test_class =}\StringTok{ }\KeywordTok{predict}\NormalTok{(NN_model1, test, }\DataTypeTok{type=}\StringTok{"class"}\NormalTok{ )  }\CommentTok{# yields "0", "1"}
\NormalTok{nn_pred_test =}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(nn_pred_test_class ) }\CommentTok{#transform to 0, 1}
\NormalTok{solution <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{PassengerId=}\NormalTok{test}\OperatorTok{$}\NormalTok{PassengerId, }\DataTypeTok{Survived =}\NormalTok{ nn_pred_test)}
\KeywordTok{write.csv}\NormalTok{(solution, }\DataTypeTok{file =} \StringTok{'./data/NN_predicton.csv'}\NormalTok{, }\DataTypeTok{row.names =}\NormalTok{ F)}

\CommentTok{###}
\CommentTok{# 0.8934,0.8547, 0.71052}
\NormalTok{NN_Acc <-}\StringTok{ }\KeywordTok{c}\NormalTok{(NN_Acc_Train, NN_Acc_Valid, }\FloatTok{0.71052}\NormalTok{)}
\KeywordTok{names}\NormalTok{(NN_Acc) <-}\StringTok{ }\NormalTok{acc_names}
\NormalTok{NN_Acc}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Train Accu Valid Accu  Test Accu 
##   89.22560   87.70950    0.71052
\end{verbatim}

\hypertarget{comparision-among-different-models}{%
\subsection*{Comparision among Different Models}\label{comparision-among-different-models}}


Let us compare the different models we have produced and see which one has a better prediction accuracy on the test dataset. We will use our best prediction accuracy on the test dataset for decision tree and random forest models.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyr)}
\NormalTok{Model <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Regression"}\NormalTok{,}\StringTok{"SVM"}\NormalTok{,}\StringTok{"NN"}\NormalTok{, }\StringTok{"Decision tree"}\NormalTok{, }\StringTok{"Random Forest"}\NormalTok{)}
\NormalTok{Train <-}\StringTok{ }\KeywordTok{c}\NormalTok{(Regression_Acc_Train, AVM_Acc_Train, NN_Acc_Train, }\FloatTok{82.72}\NormalTok{, }\FloatTok{83.16}\NormalTok{)}
\NormalTok{Valid <-}\StringTok{ }\KeywordTok{c}\NormalTok{(Regression_Acc_Valid, AVM_Acc_Valid, NN_Acc_Valid, }\FloatTok{81.01}\NormalTok{, }\DecValTok{92}\NormalTok{)}
\NormalTok{Test <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{76.56}\NormalTok{, }\FloatTok{78.95}\NormalTok{, }\FloatTok{71.05}\NormalTok{, }\FloatTok{77.75}\NormalTok{, }\FloatTok{78.95}\NormalTok{)}
\NormalTok{df1 <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(Model, Train, Valid, Test)}

\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(df1, }\DataTypeTok{longtable =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{booktabs =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{digits =} \DecValTok{2}\NormalTok{, }\DataTypeTok{col.names =}\KeywordTok{c}\NormalTok{(}\StringTok{"Models"}\NormalTok{, }\StringTok{"Accuracy on Train"}\NormalTok{, }\StringTok{"Accuracy on Valid"}\NormalTok{,}\StringTok{"Accuracy on Test"}\NormalTok{), }
  \DataTypeTok{caption =} \StringTok{'The Comparision among 3 Machine Learning Models'}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[t]{lrrr}
\caption{\label{tab:Tabmodelcompare}The Comparision among 3 Machine Learning Models}\\
\toprule
Models & Accuracy on Train & Accuracy on Valid & Accuracy on Test\\
\midrule
Regression & 84.97 & 79.89 & 76.56\\
SVM & 84.41 & 78.21 & 78.95\\
NN & 89.23 & 87.71 & 71.05\\
Decision tree & 82.72 & 81.01 & 77.75\\
Random Forest & 83.16 & 92.00 & 78.95\\
\bottomrule
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.long <-}\StringTok{ }\KeywordTok{gather}\NormalTok{(df1, Dataset, Accuracy, }\OperatorTok{-}\NormalTok{Model, }\DataTypeTok{factor_key =}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ df.long, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Model, }\DataTypeTok{y =}\NormalTok{ Accuracy, }\DataTypeTok{fill =}\NormalTok{ Dataset)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_col}\NormalTok{(}\DataTypeTok{position =} \KeywordTok{position_dodge}\NormalTok{()) }
\end{Highlighting}
\end{Shaded}

\includegraphics{10-model-cross-validation_files/figure-latex/RFmodelcompare-1.pdf}
From the above table and plot, we can see that multiple models cross-validation does not provide a conclusive answer on which model to use for real applications or production. Ideally, we would choose the model that has higher accuracy on \emph{trainData} and \emph{validData}. From the table \ref{tab:Tabmodelcompare}, we should choose model \emph{NN} since it has the highest train accuracy (91.13\%) and the second highest validation accuracy (87.71\%), however, it has the lowest test accuracy (71.05\%). Another possible logic would be to choose the highest validation accuracy and ignore the train accuracy, in this case, we would choose \emph{Random Forest} model since it has the highest validation accuracy (92.00\%), and the highest test accuracy among the models (78.95\%). However, the \emph{SVM} model also has 78.95\% test accuracy but its validation accuracy is the lowest.

It reveals an unpleasant fact - there is no model which be a certainty that its performance on unseen data can be ensured by the cross validation. The CV can only be used to spot and discover problems but not solutions.

\hypertarget{summary-8}{%
\section*{Summary}\label{summary-8}}


Remember the initial motivation of introducing \emph{Cross Validation} was to identify the overfitting of a prediction model so it should not be used for the real application. To identify overfiting, a single model CV is sufficient. After split the training dataset into \texttt{trainData} and \texttt{validData}, the model's overfitting problem can be clearly seen with the \texttt{validData}. For example, Random forest \texttt{model4}, in table \ref{tab:CVmodelcom}, has 83.99\% estimated accuracy on the model's construction and 96.63\% of accuracy on the train dataset, but it only has 77.09\% accuracy on the validation dataset which the model does not see before. This big drop on the accuracy indicates that the overfitting of the model. It is confirmed by the worsen prediction accuracy on the test dataset, which was 75.12\%.

Cross-validation with multiple models was expected to provide a conclusive judgment of a model whether it is the best or not. It seems that it can provide some evidence but it does not provide a definite conclusion. Perhaps it approved the point that each model has its own conditions and cases of usage. In case of multiple models can all be used for a problem, the individual model's performance can be affected by the data samples and the model's parameters, even the combination of the two together. In data science, there is a way to further dill down on this issue is called \textbf{model's fine tune}. That is what we are going to discuss in the next Chapter.

\hypertarget{exercises-7}{%
\section*{Exercises}\label{exercises-7}}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Tune SVM models using \texttt{e1071} package. There are two parameters gamma \& cost. Using tune.svm() to tune the model and find the best values for gamma \& cost.
\item
  Try different models with Neural Networks. It should have a better prediction accuracy than the one we have produced.
\end{enumerate}

\hypertarget{fine-tune-models}{%
\chapter{Fine Tune Models}\label{fine-tune-models}}

\begin{center}\includegraphics[width=0.7\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/images/fine_tune} \end{center}

In the previous chapters, we have obtained the knowledge that many prediction models need fine-tune, so they could:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  better fit the training dataset,
\item
  produce better performance for the test dataset (less overfit).
\end{enumerate}

In most data science projects, fine-tune a model is not only necessary but also desirable since it can increase the final outcomes of the projects. In this chapter, we will demonstrate techniques that are used to fine-tune a prediction model.

Fine-tune a model is specific to the model. Different models may have different parameters and different measurements of the performance. In general, A model's overall performance is affected by the three factors:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{The predictors.} The numbers of predictors used in a model and the specific predictors used in the model.
\item
  \textbf{The training sample.} The larger the data sample the better change of model fit. However, there are many methods for dealing with a small data sample. Mostly to enlarge the data sample or make efficient use of available data samples.
\item
  \textbf{The parameter of the model.} The adjustable parameters in a model. Most of the model tuning refers to adjust the parameters of the model.
\end{enumerate}

We can see that the search space defined by the three dimensions is fairly large. Sometimes it is computationally infeasible to search them in one go. In practice, we generally fix one or two dimensions and search for another dimension. In other words, we can fine-tune them one by one.

That is what we are going to do.

\hypertarget{tuning-a-models-predictor}{%
\section{Tuning a model's Predictor}\label{tuning-a-models-predictor}}

From our \emph{Decision tree} and \emph{Random forest} model constructions, we've learned that the predictors can affect a model's performance. The techniques used to select the best predictors is \emph{Correlation analysis} and \emph{Association analysis}, to select predictors that have no correlation among the predictors and strong association with the dependent variable, in other words, the attributes have the prediction power. Some models can provide some measurements of contributions of each predictor to the response variable. Such as in \emph{Random forest} model, the user can specify the \texttt{importance} parameter to \texttt{true} (See Chapter 9), and let the model record the predictors' \texttt{importance} and this importance can be used for post-model-construction analysis. If we have the predictors' importance, the tuning of the number of the predictors will become simple. We can simply take ``bottom-up'' or ``top-down'' approaches to tune the number of predictors until the best model accuracy has been achieved.

Let us use \textbf{Random forest} model to demonstrate this process. Recall that we have built three random forest models (in Chapter 9), each has different predictors and model's accuracy (see Table 9.1). Let us ignore the overfitting issue at the moment and focus on the predictor's impact on the model's accuracy. Among the three models, both \texttt{model1} and \texttt{model3} have lower accuracy than \texttt{model2}. \texttt{Model2} has more predictors than \texttt{model1} and fewer predictors than \texttt{model3}. It reveals a principle which we have mentioned earlier, that is more predictors not necessarily mean higher accuracy.

To fine-tune the predictors, let us use the ``top-down'' approach. We start from the use of all attributes and gradually reduce the predictors by removing the least important attribute until the last attribute. We can compare the models' accuracy and select the highest model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load necessary library}
\KeywordTok{library}\NormalTok{(randomForest)}
\KeywordTok{library}\NormalTok{(plyr)}
\KeywordTok{library}\NormalTok{(caret)}

\CommentTok{# load our re-engineered data set and separate train and test datasets}
\NormalTok{RE_data <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"./data/RE_data.csv"}\NormalTok{, }\DataTypeTok{header =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{train <-}\StringTok{ }\NormalTok{RE_data[}\DecValTok{1}\OperatorTok{:}\DecValTok{891}\NormalTok{, ]}
\NormalTok{test <-}\StringTok{ }\NormalTok{RE_data[}\DecValTok{892}\OperatorTok{:}\DecValTok{1309}\NormalTok{, ]}

\CommentTok{# Train a Random Forest with the default parameters using full attributes}
\CommentTok{# Survived is our response variable and the rest can be predictors except pasengerID. }
\NormalTok{rf.train <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(train, }\DataTypeTok{select =} \OperatorTok{-}\KeywordTok{c}\NormalTok{(PassengerId, Survived))}
\NormalTok{rf.label <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(train}\OperatorTok{$}\NormalTok{Survived)}

\CommentTok{#RandomForest cannot handle factors with over 53 levels}
\NormalTok{rf.train}\OperatorTok{$}\NormalTok{Ticket <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(train}\OperatorTok{$}\NormalTok{Ticket)}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{) }\CommentTok{# for reproduction }
\CommentTok{# FT_rf.1 <- randomForest(x = rf.train, y = rf.label, importance = TRUE)}
\CommentTok{# save(FT_rf.1, file = "./data/FT_rf.1.rda")}
\KeywordTok{load}\NormalTok{(}\StringTok{"./data/FT_rf.1.rda"}\NormalTok{)}
\NormalTok{FT_rf}\FloatTok{.1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
##  randomForest(x = rf.train, y = rf.label, importance = TRUE) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 4
## 
##         OOB estimate of  error rate: 14.93%
## Confusion matrix:
##     0   1 class.error
## 0 491  58   0.1056466
## 1  75 267   0.2192982
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#rf.1 model with full house predictors has error rate: 15.49% }
\CommentTok{# Check the order of the predictors prediction power.}
\NormalTok{pre.or <-}\StringTok{ }\KeywordTok{sort}\NormalTok{(FT_rf}\FloatTok{.1}\OperatorTok{$}\NormalTok{importance[,}\DecValTok{3}\NormalTok{], }\DataTypeTok{decreasing =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{pre.or}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        Title          Sex      Fare_pp       Pclass Ticket_class       Ticket 
##  0.084673918  0.083622938  0.034991513  0.031993819  0.031729670  0.026882064 
##          Age  Friend_size         Deck    Age_group   Group_size  Family_size 
##  0.022106781  0.016718555  0.013445769  0.012993600  0.011544400  0.010971526 
##  HasCabinNum     Embarked        SibSp        Parch 
##  0.008285454  0.005295491  0.004567226  0.002776599
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{varImpPlot}\NormalTok{(FT_rf}\FloatTok{.1}\NormalTok{, }\DataTypeTok{main=}\StringTok{"Ordered predictors measurements"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{11-fine-tune-models_files/figure-latex/ImportancePlot-1} 

}

\caption{The importance of the predictors}\label{fig:ImportancePlot}
\end{figure}

We have obtained the ``full-house'' model's accuracy 85.07\%, that is \texttt{1\ -\ prediction\ error} (14.93\%).

We have also obtained the order of the predictor's prediction power, which is from the most to the least in the following order: ``\emph{Sex}, \emph{Title}, \emph{Fare\_pp}, \emph{Ticket\_class}, \emph{Pclass}, \emph{Ticket}, \emph{Age}, \emph{Friend\_size}, \emph{Deck}, \emph{Age\_group}, \emph{Group\_size}, \emph{Family\_size}, \emph{HasCabinNum}, \emph{SibSp}, \emph{Embarked}, \emph{Parch}''.

We now can repeat the process by removing one attribute from the end of the list above and train a new \emph{Random forest model} such as \texttt{FT\_rf.2}, \texttt{FT\_rf.1.3}, \ldots{} until \texttt{FT\_rf.1.16}. We can compare the models' \texttt{OOB\ error} or \texttt{Accuracy} to find out which model has the highest accuracy.

We only shows \texttt{FT\_rf.2} as an example in here,

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# rf.2 as an example}
\NormalTok{rf.train}\FloatTok{.2}\NormalTok{ <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(rf.train, }\DataTypeTok{select =} \OperatorTok{-}\KeywordTok{c}\NormalTok{(Parch))}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\CommentTok{# FT_rf.2 <- randomForest(x = rf.train.2, y = rf.label, importance = TRUE)}
\CommentTok{# save(FT_rf.2, file = "./data/FT_rf.2.rda")}
\KeywordTok{load}\NormalTok{(}\StringTok{"./data/FT_rf.2.rda"}\NormalTok{)}
\NormalTok{FT_rf}\FloatTok{.2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
##  randomForest(x = rf.train.2, y = rf.label, importance = TRUE) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 3
## 
##         OOB estimate of  error rate: 15.38%
## Confusion matrix:
##     0   1 class.error
## 0 492  57   0.1038251
## 1  80 262   0.2339181
\end{verbatim}

We have obtained the model \texttt{FT\_rf.2}, in which the attribute \emph{``Parch''} has been removed since it is the last attributes in the attributes' prediction power list, which means it has the least prediction power.

The \emph{FT\_rf.2} model's accuracy 84.62\%, that is \texttt{1\ -\ prediction\ error} (15.38\%).

We can carry on the process until the last attribute \emph{Sex}. We will then have a list of models and each has its estimated accuracy (We will not repeat the process in here and leave it to the exercise).

Once we have completed the process, the results can be listed and compared as in the Table \ref{tab:Comppredictor}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyr)}

\NormalTok{Model <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"rf.1"}\NormalTok{,}\StringTok{"rf.2"}\NormalTok{,}\StringTok{"rf.3"}\NormalTok{,}\StringTok{"rf.4"}\NormalTok{,}\StringTok{"rf.5"}\NormalTok{,}\StringTok{"rf.6"}\NormalTok{,}\StringTok{"rf.7"}\NormalTok{,}\StringTok{"rf.8"}\NormalTok{,}\StringTok{"rf.9"}\NormalTok{,}\StringTok{"rf.10"}\NormalTok{,}\StringTok{"rf.11"}\NormalTok{,}\StringTok{"rf.12"}\NormalTok{,}\StringTok{"rf.13"}\NormalTok{,}\StringTok{"rf.14"}\NormalTok{,}\StringTok{"rf.15"}\NormalTok{,}\StringTok{"rf.16"}\NormalTok{)}
\NormalTok{Pre <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Sex"}\NormalTok{, }\StringTok{"Title"}\NormalTok{, }\StringTok{"Fare_pp"}\NormalTok{, }\StringTok{"Ticket_class"}\NormalTok{, }\StringTok{"Pclass"}\NormalTok{, }\StringTok{"Ticket"}\NormalTok{, }\StringTok{"Age"}\NormalTok{, }\StringTok{"Friend_size"}\NormalTok{, }\StringTok{"Deck"}\NormalTok{, }\StringTok{"Age_group"}\NormalTok{, }\StringTok{"Group_size"}\NormalTok{, }\StringTok{"Family_size"}\NormalTok{, }\StringTok{"HasCabinNum"}\NormalTok{, }\StringTok{"SibSp"}\NormalTok{, }\StringTok{"Embarked"}\NormalTok{, }\StringTok{"Parch"}\NormalTok{)}
\CommentTok{#Produce models predictor list}
\NormalTok{Pred <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{16}\NormalTok{)}
\NormalTok{tem <-}\StringTok{ }\OtherTok{NULL}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(Pre)) \{}
\NormalTok{  tem  <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(tem, Pre[i], }\DataTypeTok{sep =} \StringTok{" "}\NormalTok{)}
\CommentTok{#Using environment variable setting    }
\NormalTok{  ls  <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(}\StringTok{"Pred["}\NormalTok{,i,}\StringTok{"]"}\NormalTok{, }\DataTypeTok{sep=}\StringTok{""}\NormalTok{)}
\NormalTok{  eq  <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(}\KeywordTok{paste}\NormalTok{(ls, }\StringTok{"tem"}\NormalTok{, }\DataTypeTok{sep=}\StringTok{"<-"}\NormalTok{), }\DataTypeTok{collapse=}\StringTok{";"}\NormalTok{)  }
  \KeywordTok{eval}\NormalTok{(}\KeywordTok{parse}\NormalTok{(}\DataTypeTok{text=}\NormalTok{eq)) }
\NormalTok{  \}}
\NormalTok{Pred <-}\StringTok{ }\KeywordTok{sort}\NormalTok{(Pred, }\DataTypeTok{decreasing =} \OtherTok{TRUE}\NormalTok{)}

\NormalTok{Error <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{15.49}\NormalTok{, }\FloatTok{15.15}\NormalTok{, }\FloatTok{14.93}\NormalTok{, }\FloatTok{15.26}\NormalTok{, }\FloatTok{14.7}\NormalTok{, }\FloatTok{14.7}\NormalTok{, }\FloatTok{14.03}\NormalTok{, }\FloatTok{13.58}\NormalTok{, }\FloatTok{14.48}\NormalTok{, }\FloatTok{15.6}\NormalTok{, }\FloatTok{16.27}\NormalTok{, }\FloatTok{16.95}\NormalTok{, }\FloatTok{17.51}\NormalTok{, }\FloatTok{20.31}\NormalTok{, }\FloatTok{20.76}\NormalTok{, }\FloatTok{21.32}\NormalTok{)}
\NormalTok{Accuracy <-}\StringTok{ }\DecValTok{100} \OperatorTok{-}\StringTok{ }\NormalTok{Error}
\NormalTok{df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(Model, Pred, Accuracy)}

\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(df, }\DataTypeTok{longtable =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{booktabs =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{digits =} \DecValTok{2}\NormalTok{, }\DataTypeTok{col.names =}\KeywordTok{c}\NormalTok{(}\StringTok{"Models"}\NormalTok{, }\StringTok{"Predictors"}\NormalTok{, }\StringTok{"Accuracy"}\NormalTok{), }
  \DataTypeTok{caption =} \StringTok{'Model Predictors Comparision'}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[t]{llr}
\caption{\label{tab:Comppredictor}Model Predictors Comparision}\\
\toprule
Models & Predictors & Accuracy\\
\midrule
rf.1 & Sex Title Fare\_pp Ticket\_class Pclass Ticket Age Friend\_size Deck Age\_group Group\_size Family\_size HasCabinNum SibSp Embarked Parch & 84.51\\
rf.2 & Sex Title Fare\_pp Ticket\_class Pclass Ticket Age Friend\_size Deck Age\_group Group\_size Family\_size HasCabinNum SibSp Embarked & 84.85\\
rf.3 & Sex Title Fare\_pp Ticket\_class Pclass Ticket Age Friend\_size Deck Age\_group Group\_size Family\_size HasCabinNum SibSp & 85.07\\
rf.4 & Sex Title Fare\_pp Ticket\_class Pclass Ticket Age Friend\_size Deck Age\_group Group\_size Family\_size HasCabinNum & 84.74\\
rf.5 & Sex Title Fare\_pp Ticket\_class Pclass Ticket Age Friend\_size Deck Age\_group Group\_size Family\_size & 85.30\\
\addlinespace
rf.6 & Sex Title Fare\_pp Ticket\_class Pclass Ticket Age Friend\_size Deck Age\_group Group\_size & 85.30\\
rf.7 & Sex Title Fare\_pp Ticket\_class Pclass Ticket Age Friend\_size Deck Age\_group & 85.97\\
rf.8 & Sex Title Fare\_pp Ticket\_class Pclass Ticket Age Friend\_size Deck & 86.42\\
rf.9 & Sex Title Fare\_pp Ticket\_class Pclass Ticket Age Friend\_size & 85.52\\
rf.10 & Sex Title Fare\_pp Ticket\_class Pclass Ticket Age & 84.40\\
\addlinespace
rf.11 & Sex Title Fare\_pp Ticket\_class Pclass Ticket & 83.73\\
rf.12 & Sex Title Fare\_pp Ticket\_class Pclass & 83.05\\
rf.13 & Sex Title Fare\_pp Ticket\_class & 82.49\\
rf.14 & Sex Title Fare\_pp & 79.69\\
rf.15 & Sex Title & 79.24\\
\addlinespace
rf.16 & Sex & 78.68\\
\bottomrule
\end{longtable}

From the table we can see that the best model is \textbf{FT\_rf.8}. Its accuracy reaches 86.42 and the predictors are:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load the best model and record its predictors}
\CommentTok{# save(FT_rf.8, file = "./data/FT_rf.8.rda")}

\KeywordTok{load}\NormalTok{(}\StringTok{"./data/FT_rf.8.rda"}\NormalTok{)}
\NormalTok{Predictor <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Sex, Title, Fare_pp, Ticket_class, Pclass, Ticket, Age, Friend_size, Deck"}\NormalTok{)}
\NormalTok{Predictor}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Sex, Title, Fare_pp, Ticket_class, Pclass, Ticket, Age, Friend_size, Deck"
\end{verbatim}

Of course, you can try different combinations of the predictors. The idea will be the same.

Some other models support predictor fine-tune. For example, Logistic Regression model \texttt{glm} provides \emph{Stepwise} attributes prediction power analysing. One can use \emph{``backward''} Step-wise search to compare models' AIC\footnote{The Akaike information criterion (AIC) is an estimator of out-of-sample prediction error and thereby the relative quality of models for a given dataset. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models. Thus, AIC provides a means for model selection.} to find the best model and its predictors.

\hypertarget{tuning-training-data-samples}{%
\section{Tuning Training Data Samples}\label{tuning-training-data-samples}}

Once we have selected predictors from available attributes of data samples. The second factor that affects the model's performance is the data samples. We know that we should try to make the most use of data samples for training a model. In practice, a lot of times, we only have a small-sized data sample and we have to use other techniques to enlarge the data sample. However, there is a drawback of over-used the data sample. That is the possibility of the noise and outliers may have been fitted into a model and therefore decreasing the model's prediction accuracy when used in production. In this section, we use CV to demonstrate the process of setting the right portion of the data sample.

We will use CV to illustrate the technique. So it means to set the proper portion (ratio) between the split of the train dataset and the sample usage in the model's fitting process.

We will continue to use RF model as our example. We know that the RF model has an overfitting issue. One possibility is we have used an improper portion of the training dataset. Following our predictor selection in the above section, we know that the best RF model is \texttt{FT\_rf.8}. We will use \texttt{FT\_rf.8} to demonstrate how to find the best CV settings.

\hypertarget{set-prediction-accuracy-benchmark}{%
\subsection*{Set Prediction Accuracy Benchmark}\label{set-prediction-accuracy-benchmark}}


The benchmark is the model's prediction accuracy on the test dataset (unseen data).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Let's start with a submission of FT_rf.8 to Kaggle }
\CommentTok{# to find the difference between model's OOB and the accuracy}

\CommentTok{# Subset our test records and features}
\NormalTok{test.submit.df <-}\StringTok{ }\NormalTok{test[, }\KeywordTok{c}\NormalTok{(}\StringTok{"Sex"}\NormalTok{, }\StringTok{"Title"}\NormalTok{, }\StringTok{"Fare_pp"}\NormalTok{, }\StringTok{"Ticket_class"}\NormalTok{, }\StringTok{"Pclass"}\NormalTok{, }\StringTok{"Ticket"}\NormalTok{, }\StringTok{"Age"}\NormalTok{, }\StringTok{"Friend_size"}\NormalTok{, }\StringTok{"Deck"}\NormalTok{)]}
\NormalTok{test.submit.df}\OperatorTok{$}\NormalTok{Ticket <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(test.submit.df}\OperatorTok{$}\NormalTok{Ticket)}

\CommentTok{# Make predictions}
\NormalTok{FT_rf.}\FloatTok{8.}\NormalTok{preds <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(FT_rf}\FloatTok{.8}\NormalTok{, test.submit.df)}
\KeywordTok{table}\NormalTok{(FT_rf.}\FloatTok{8.}\NormalTok{preds)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## FT_rf.8.preds
##   0   1 
## 260 158
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Write out a CSV file for submission to Kaggle}
\NormalTok{submit.df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{PassengerId =}\NormalTok{ test}\OperatorTok{$}\NormalTok{PassengerId, }\DataTypeTok{Survived =}\NormalTok{ FT_rf.}\FloatTok{8.}\NormalTok{preds)}

\KeywordTok{write.csv}\NormalTok{(submit.df, }\DataTypeTok{file =} \StringTok{"./data/FT_rf.8.csv"}\NormalTok{, }\DataTypeTok{row.names =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

After our submission, we have scores of 0.75598 from Kaggle, but the OOB predicts that we should score 0.8642. We can see there is a big gap in between. The idea is to reduce this gap by adjusting the CV sampling controls.

\hypertarget{folds-cv-repeat-10-times}{%
\subsection*{10 Folds CV Repeat 10 Times}\label{folds-cv-repeat-10-times}}


Let's look into CV using the \texttt{caret} package to see if we can get more accurate estimates by adjusting CV's sampling parameters. Research has shown that \texttt{10-fold} CV \texttt{repeated\ 10\ times} is the best place to start. One of the important ideas is that to ensure that the ratio of those values of the response variable (\emph{Survived}) in each fold matches the overall training set. This is known as \textbf{stratified CV}\index{stratified CV}.

Firstly, We randomly create \texttt{10\ folds} and \texttt{repeat\ 10} times with our train dataset by a caret function \texttt{createMultiFolds}. So it has effectively enlarged our train data sample 100 times. This can be seen from the length of the list `\texttt{cv.10.folds}'. We will use these settings to train our RF model \texttt{rf.8}. Before we do that, we can also verify the survived ratio in the samples created to see if they are the same or close to the same ratio.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caret)}
\KeywordTok{library}\NormalTok{(doSNOW)}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2348}\NormalTok{)}
\CommentTok{# rf.label is the Survived in the train dataset.}
\CommentTok{# ? createMultiFolds to find out more. train (891)}
\NormalTok{cv.}\FloatTok{10.}\NormalTok{folds <-}\StringTok{ }\KeywordTok{createMultiFolds}\NormalTok{(rf.label, }\DataTypeTok{k =} \DecValTok{10}\NormalTok{, }\DataTypeTok{times =} \DecValTok{10}\NormalTok{)}

\CommentTok{# Check stratification}
\KeywordTok{table}\NormalTok{(rf.label)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## rf.label
##   0   1 
## 549 342
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{342} \OperatorTok{/}\StringTok{ }\DecValTok{549}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.6229508
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(rf.label[cv.}\FloatTok{10.}\NormalTok{folds[[}\DecValTok{34}\NormalTok{]]])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##   0   1 
## 494 308
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{308} \OperatorTok{/}\StringTok{ }\DecValTok{494}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.6234818
\end{verbatim}

We can see that we have produced 100 sample sets and they are in a size of \(length(train)*9/10\) and kept the stratification (both has a similar radio around 62.3\%). Let us use ``\texttt{repeatedcv}'' to train our \texttt{rf.8} model and see the impact of the sampling on the model's prediction accuracy.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Set up caret's trainControl object using 10-folds repeated CV}
\NormalTok{ctrl}\FloatTok{.1}\NormalTok{ <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"repeatedcv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{, }\DataTypeTok{repeats =} \DecValTok{10}\NormalTok{, }\DataTypeTok{index =}\NormalTok{ cv.}\FloatTok{10.}\NormalTok{folds)}
\end{Highlighting}
\end{Shaded}

Model construction with ``\texttt{10-folds\ repeated\ CV}'' is a very expensive computation. Thanks, R has a package called \textbf{``doSNOW''}, that facilities the use of a multi-core processor and permits parallel computing in a pseudo cluster mode\index{pseudo cluster mode}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Set up doSNOW package for multi-core training. }
\CommentTok{# cl <- makeCluster(6, type = "SOCK")}
\CommentTok{# registerDoSNOW(cl)}
\CommentTok{# # Set seed for reproducibility and train}
\CommentTok{# set.seed(34324)}
\CommentTok{# }
\CommentTok{# FT_rf.8.cv.1 <- train(x = rf.train.8, y = rf.label, method = "rf", tuneLength = 3, ntree = 500, trControl = ctrl.1)}
\CommentTok{# }
\CommentTok{# #Shutdown cluster}
\CommentTok{# stopCluster(cl)}
\CommentTok{# save(FT_rf.8.cv.1, file = "./data/FT_rf.8.cv.1.rda")}
\CommentTok{# Check out results}
\KeywordTok{load}\NormalTok{(}\StringTok{"./data/FT_rf.8.cv.1.rda"}\NormalTok{)}
\CommentTok{# Check out results}
\NormalTok{FT_rf.}\FloatTok{8.}\NormalTok{cv}\FloatTok{.1} 
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Random Forest 
## 
## 891 samples
##   9 predictor
##   2 classes: '0', '1' 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 802, 802, 802, 802, 802, 802, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##   2     0.8495937  0.6773587
##   5     0.8511480  0.6818755
##   9     0.8467582  0.6727850
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 5.
\end{verbatim}

The RF model \texttt{FT\_rf.8.cv.1} trained using new data samples (10 sets with each has 802 data records) above is only slightly more pessimistic than the \texttt{rf.8} OOB\index{OOB} prediction since the accuracy reduced from 0.8642 to 0.8511, but not pessimistic enough to the test accuracy, it is 0.75598. However, it clearly demonstrated the impact of the data samples on the model's performance.

\hypertarget{folds-cv-repeat-10-times-1}{%
\subsection*{5 Folds CV Repeat 10 Times}\label{folds-cv-repeat-10-times-1}}


Let's try new data samples with 5-fold CV repeated 10 times.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{5983}\NormalTok{)}
\CommentTok{# cv.5.folds <- createMultiFolds(rf.label, k = 5, times = 10)}
\CommentTok{# }
\CommentTok{# ctrl.2 <- trainControl(method = "repeatedcv", number = 5, repeats = 10, index = cv.5.folds)}
\CommentTok{# }
\CommentTok{# cl <- makeCluster(6, type = "SOCK")}
\CommentTok{# registerDoSNOW(cl)}
\CommentTok{# }
\CommentTok{# set.seed(89472)}
\CommentTok{# FT_rf.8.cv.2 <- train(x = rf.train.8, y = rf.label, method = "rf", tuneLength = 3, ntree = 500, trControl = ctrl.2)}
\CommentTok{# }
\CommentTok{# #Shutdown cluster}
\CommentTok{# stopCluster(cl)}
\CommentTok{# save(FT_rf.8.cv.2, file = "./data/FT_rf.8.cv.2.rda")}
\CommentTok{# # Check out results}
\KeywordTok{load}\NormalTok{(}\StringTok{"./data/FT_rf.8.cv.2.rda"}\NormalTok{)}
\CommentTok{# Check out results}
\NormalTok{FT_rf.}\FloatTok{8.}\NormalTok{cv}\FloatTok{.2} 
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Random Forest 
## 
## 891 samples
##   9 predictor
##   2 classes: '0', '1' 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold, repeated 10 times) 
## Summary of sample sizes: 714, 713, 713, 712, 712, 712, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##   2     0.8491649  0.6768403
##   5     0.8444515  0.6675058
##   9     0.8415270  0.6618163
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 2.
\end{verbatim}

We can see that \texttt{5-fold\ CV} is a little better. The accuracy is now moved under 85\% (0.8491649). The model's training data set is moved from 9/10 to 4/5, which is around 713 records per fold now.

\hypertarget{folds-cv-repeat-10-times-2}{%
\subsection*{3 Folds CV Repeat 10 Times}\label{folds-cv-repeat-10-times-2}}


Let us move further to try \texttt{3-fold\ CV} repeated 10 times.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{37596}\NormalTok{)}
\CommentTok{# cv.3.folds <- createMultiFolds(rf.label, k = 3, times = 10)}
\CommentTok{# }
\CommentTok{# ctrl.3 <- trainControl(method = "repeatedcv", number = 3, repeats = 10, index = cv.3.folds)}
\CommentTok{# }
\CommentTok{# cl <- makeCluster(6, type = "SOCK")}
\CommentTok{# registerDoSNOW(cl)}
\CommentTok{# }
\CommentTok{# set.seed(94622)}
\CommentTok{# FT_rf.8.cv.3 <- train(x = rf.train.8, y = rf.label, method = "rf", tuneLength = 3, ntree = 500, trControl = ctrl.3)}
\CommentTok{# }
\CommentTok{# #Shutdown cluster}
\CommentTok{# stopCluster(cl)}
\CommentTok{# }
\CommentTok{# save(FT_rf.8.cv.3, file = "./data/FT_rf.8.cv.3.rda")}
\CommentTok{# # # Check out results}
\KeywordTok{load}\NormalTok{(}\StringTok{"./data/FT_rf.8.cv.3.rda"}\NormalTok{)}
\CommentTok{# Check out results}
\NormalTok{FT_rf.}\FloatTok{8.}\NormalTok{cv}\FloatTok{.3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Random Forest 
## 
## 891 samples
##   9 predictor
##   2 classes: '0', '1' 
## 
## No pre-processing
## Resampling: Cross-Validated (3 fold, repeated 3 times) 
## Summary of sample sizes: 594, 594, 594, 594, 594, 594, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##   2     0.8387579  0.6529203
##   5     0.8376356  0.6522826
##   9     0.8357651  0.6503612
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 2.
\end{verbatim}

We can see the accuracy has further decreased (0.8387579). Let us also reduced the number of times that the samples are repeated used in the training (repeat times). Let us see if the sample repeat times reduce to 3, if the accuracy can be further reduced.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set.seed(396)}
\CommentTok{# cv.3.folds <- createMultiFolds(rf.label, k = 3, times = 3)}
\CommentTok{# }
\CommentTok{# ctrl.4 <- trainControl(method = "repeatedcv", number = 3, repeats = 3, index = cv.3.folds)}
\CommentTok{# }
\CommentTok{# cl <- makeCluster(6, type = "SOCK")}
\CommentTok{# registerDoSNOW(cl)}
\CommentTok{# }
\CommentTok{# set.seed(9622)}
\CommentTok{# FT_rf.8.cv.4 <- train(x = rf.train.8, y = rf.label, method = "rf", tuneLength = 3, ntree = 50, trControl = ctrl.4)}
\CommentTok{# }
\CommentTok{# #Shutdown cluster}
\CommentTok{# stopCluster(cl)}
\CommentTok{#save(FT_rf.8.cv.4, file = "./data/FT_rf.8.cv.4.rda")}
\CommentTok{# # # Check out results}
\KeywordTok{load}\NormalTok{(}\StringTok{"./data/FT_rf.8.cv.4.rda"}\NormalTok{)}
\CommentTok{# Check out results}
\NormalTok{FT_rf.}\FloatTok{8.}\NormalTok{cv}\FloatTok{.4}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Random Forest 
## 
## 891 samples
##   9 predictor
##   2 classes: '0', '1' 
## 
## No pre-processing
## Resampling: Cross-Validated (3 fold, repeated 3 times) 
## Summary of sample sizes: 594, 594, 594, 594, 594, 594, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##   2     0.8443696  0.6649206
##   5     0.8436214  0.6659881
##   9     0.8368874  0.6531629
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 2.
\end{verbatim}

We can see the impact of the training samples (\texttt{numbers} and \texttt{repeated\ times}) used on the RF model's accuracy. Among of our 4 trials, it appeared that the settings \texttt{ctrl.3}, which is \texttt{3-folds\ and\ repeated\ 10\ times} has the best accuracy. We could continue the trails until we are satisfied. But we will stop here for computation reasons.

One of the conclusion we may draw form the exercises we did is that the best sampling should imic the proportion of the training dataset with the testing dataset. Our Titanic datasets have a proportion of (891:418), which is roughly 2:1. so our sampling should match with this proportions. The 3-folds CV partition, using 2 folds to train and 1 fold to test, matches this ratio. So it is reasonable to believe that the best data sampling is the \texttt{3-folds\ repeated\ 10\ times} for our Titanic problem.

Some of you may notice that we have set different \texttt{mtree} values. That is one of the parameters \emph{RF model} used for generating the prediction. We are going to discuss about them next.

\hypertarget{tuning-models-parameters}{%
\section{Tuning Model's Parameters}\label{tuning-models-parameters}}

Tuning model parameters is a parameter optimization problem \citep{Brownlee2021}. Depending on the models, the adjustable parameters can be different completely. For example, the decision tree has two adjustable parameters: \texttt{complexity\ parameter\ (CP)} and \texttt{tune\ length\ (TL)}. \texttt{CP} tells the algorithm to stop when the measure (generally is accuracy) does not improve by this factor. \texttt{TL} tells how many instances to use for training. SVM models, as another example, also have two adjustable parameters \texttt{cost} and \texttt{gamma}. The \texttt{cost}, is a parameter that controls the trade-off between the classification of training points and a smooth decision boundary. It suggests the model chooses data points as a support vector. If the value of \texttt{cost} is large, then the model choose more data points as a support vector and we get a higher variance and lower bias, which may lead to the problem of overfitting; If the value of \texttt{cost} is small, then the model will choose fewer data points as a support vector and get a lower variance and high bias. \texttt{Gamma} defines how far the influence of a single training example reaches. If the value of Gamma is high, then the decision boundary will depend on the points close to the decision boundary and the nearer points carry more weights than far away points due to which the decision boundary becomes more wiggly. If the value of \texttt{Gamma} is low, then the far-away points carry more weights than the nearer points and thus the decision boundary becomes more like a straight line.

We will continue use \emph{RF model} as an example to demonstrate the parameter tuning process. RF has many parameters that can be adjusted but the two main tuning parameters are \textbf{\texttt{mtry}} and \textbf{\texttt{ntree}}.

\begin{itemize}
\item
  \texttt{mtry}: Number of variables randomly selected as testing conditions at each split of decision trees. default value is \texttt{sqr(col)}.
  Increasing \texttt{mtry} generally improves the performance of the model as each node has a higher number of options to be considered. However, this is not necessarily true as this decreases the diversity of individual trees. At the same time, it will decrease the speed. Hence, it needs to strike the right balance.
\item
  \texttt{ntree}: Number of trees to grow. the default value is 500. A higher number of trees give you better performance but makes your code slower. You should choose as high a value as your processor can handle because this makes your predictions stronger and more stable.
\end{itemize}

In the rest of the section, we demonstrate the process of using CV to fine-tune RF model's parameters \texttt{mtry} and \texttt{ntree}. In general, different optimization strategies can be used to find a model's optimal parameters. The two most commonly used methods for RF are \textbf{Random search} and \textbf{Grid search}.

\emph{Random Search}. Define a search space as a bounded domain of parameter values and randomly sample points in that domain.

\emph{Grid Search}. Define a search space as a grid of parameter values and evaluate every position in the grid.

Let us try them one at a time.

\hypertarget{random-search}{%
\subsection*{Random Search}\label{random-search}}


Random search provided by the package \texttt{caret} with the method ``\texttt{rf}'' (\emph{Random forest}) in function \texttt{train} can only tune parameter \texttt{mtry}\footnote{Not all machine learning algorithms are available in caret for tuning. The choice of parameters was decided by the developers of the package. Only those parameters that have a large effect are available for tuning in caret. For the \texttt{RF} method, only \texttt{mtry} parameter is available in caret for tuning. The reason is its effect on the final accuracy and that it must be found empirically for a dataset}.

Let us continue using what we have found from the previous sections, that are：

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  model \texttt{rf.8} with 9 predictors.
\item
  \emph{CV} with \texttt{3-folds} and \texttt{repeat\ 10\ times}.
\end{enumerate}

Let us also fix ``\texttt{ntree\ =\ 500}'' and ``\texttt{tuneLength\ =\ 15}'', and use \texttt{random} search to find \texttt{mtry}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#library(caret)}
\CommentTok{#library(doSNOW)}
\CommentTok{# Random Search}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2222}\NormalTok{)}
\CommentTok{# #use teh best sampling results that is K=3 ant T=10}
\CommentTok{# cv.3.folds <- createMultiFolds(rf.label, k = 3, times = 10)}
\CommentTok{# }
\CommentTok{# # Set up caret's trainControl object.}
\CommentTok{# ctrl.1 <- trainControl(method = "repeatedcv", number = 3, repeats = 10, index = cv.3.folds, search="random")}
\CommentTok{# }
\CommentTok{# # set up cluster for parallel computing}
\CommentTok{# cl <- makeCluster(6, type = "SOCK")}
\CommentTok{# registerDoSNOW(cl)}
\CommentTok{# }
\CommentTok{# # Set seed for reproducibility and train}
\CommentTok{# set.seed(34324)}
\CommentTok{# }
\CommentTok{# #use rf.train.8 with 9 predictors }
\CommentTok{# }
\CommentTok{# #RF_Random <- train(x = rf.train.8, y = rf.label, method = "rf", tuneLength = 15, ntree = 500, trControl = ctrl.1)}
\CommentTok{# #save(RF_Random, file = "./data/RF_Random_search.rda")}
\CommentTok{# }
\CommentTok{# #Shutdown cluster}
\CommentTok{# stopCluster(cl)}

\CommentTok{# Check out results}
\KeywordTok{load}\NormalTok{(}\StringTok{"./data/RF_Random_search.rda"}\NormalTok{)}
\KeywordTok{print}\NormalTok{(RF_Random)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Random Forest 
## 
## 891 samples
##   9 predictor
##   2 classes: '0', '1' 
## 
## No pre-processing
## Resampling: Cross-Validated (3 fold, repeated 10 times) 
## Summary of sample sizes: 594, 594, 594, 594, 594, 594, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##   2     0.8435466  0.6643066
##   3     0.8453423  0.6690529
##   4     0.8437710  0.6665398
##   5     0.8419753  0.6630091
##   6     0.8397306  0.6586318
##   7     0.8383838  0.6556425
##   8     0.8379349  0.6544327
##   9     0.8353535  0.6495571
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 3.
\end{verbatim}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{11-fine-tune-models_files/figure-latex/plotRandommtry-1} 

}

\caption{The best mtry numbers on model's accuracy produced by `Random` search.}\label{fig:plotRandommtry}
\end{figure}

We can see that the random search for \texttt{mtry} has found the best value is 3. When the model uses the parameter \texttt{mtry\ =\ 3} it can have an accuracy of 84.53\%.

\hypertarget{grid-search}{%
\subsection*{Grid Search}\label{grid-search}}


Grid search is generally searching for more than one parameter. Each axis of the grid is a parameter, and points in the grid are specific combinations of parameters. Because caret train can only tune one parameter, the grid search is now a linear search through a vector of candidate values.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ctrl.2 <- trainControl(method="repeatedcv", number=3, repeats=10, index = cv.3.folds, search="grid")}
\CommentTok{# }
\CommentTok{# set.seed(3333)}
\CommentTok{# # set Grid search with a vector from 1 to 15.}
\CommentTok{# }
\CommentTok{# tunegrid <- expand.grid(.mtry=c(1:15))}
\CommentTok{# }
\CommentTok{# # set up cluster for parallel computing}
\CommentTok{# cl <- makeCluster(6, type = "SOCK")}
\CommentTok{# registerDoSNOW(cl)}
\CommentTok{# }
\CommentTok{# }
\CommentTok{# #RF_grid_search <- train(y = rf.label, x = rf.train.8,  method="rf", metric="Accuracy", trControl = ctrl.2, tuneGrid = tunegrid, tuneLength = 15, ntree = 500)}
\CommentTok{# }
\CommentTok{# }
\CommentTok{# #Shutdown cluster}
\CommentTok{# stopCluster(cl)}
\CommentTok{# #save(RF_grid_search, file = "./data/RF_grid_search.rda")}

\KeywordTok{load}\NormalTok{(}\StringTok{"./data/RF_grid_search.rda"}\NormalTok{)}
\KeywordTok{print}\NormalTok{(RF_grid_search)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Random Forest 
## 
## 891 samples
##   9 predictor
##   2 classes: '0', '1' 
## 
## No pre-processing
## Resampling: Cross-Validated (3 fold, repeated 10 times) 
## Summary of sample sizes: 594, 594, 594, 594, 594, 594, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##    1    0.8232323  0.6140400
##    2    0.8439955  0.6652153
##    3    0.8452301  0.6691079
##    4    0.8443322  0.6675864
##    5    0.8428732  0.6645467
##    6    0.8398429  0.6584647
##    7    0.8379349  0.6548634
##    8    0.8390572  0.6571467
##    9    0.8370370  0.6529631
##   10    0.8365881  0.6519263
##   11    0.8359147  0.6504591
##   12    0.8370370  0.6525838
##   13    0.8365881  0.6520535
##   14    0.8356902  0.6502470
##   15    0.8354658  0.6494413
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 3.
\end{verbatim}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{11-fine-tune-models_files/figure-latex/plotGridmtry-1} 

}

\caption{The best mtry numbers on model's accuracy produced by `Grid` search.}\label{fig:plotGridmtry}
\end{figure}

The Grid search method identified the best parameter for \texttt{mtry} is also 3. When \texttt{mtry\ =\ 3}, the model's estimated accuracy reaches 84.52\%.

We can see that both search methods have the same \texttt{mtry} suggestions.

\hypertarget{manual-search}{%
\subsection*{Manual Search}\label{manual-search}}


Let us consider another parameter \textbf{\texttt{ntree}} in the \texttt{RF\ model}. Since our \texttt{train} method from \texttt{caret} cannot tune \texttt{ntree}, we have to write our own function to search the best value of parameter \texttt{ntree}. This method is also called \textbf{Manual Search}. The idea is to write a loop repeating the same model's fitting process a certain number of times. Each time Within a loop, a different value of the parameter to be tuned is used, and the model's results are accumulated, Finally, a manual comparison is made to figure out what is the best value of the tuned parameter.

To tune the RF model's parameter \texttt{ntree}, we set \texttt{mtry=3} from the above section and use a list of 4 values (100, 500, 1000, 1500)\footnote{These are generally used \texttt{ntree} values. For demonstration purposes we only choose these values, you can try more different values.} and find which one produces the best result.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Manual Search we use control 1 random search}
\NormalTok{model_list <-}\StringTok{ }\KeywordTok{list}\NormalTok{()}

\NormalTok{tunegrid <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}\DataTypeTok{.mtry =} \DecValTok{3}\NormalTok{)}
\NormalTok{control <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method=}\StringTok{"repeatedcv"}\NormalTok{, }\DataTypeTok{number=}\DecValTok{3}\NormalTok{, }\DataTypeTok{repeats=}\DecValTok{10}\NormalTok{, }\DataTypeTok{search=}\StringTok{"grid"}\NormalTok{)}

\CommentTok{# # the following code have been commented out just for produce the markdown file. so it will not wait for ran a long time}
\CommentTok{# # set up cluster for parallel computing}
\CommentTok{# cl <- makeCluster(6, type = "SOCK")}
\CommentTok{# registerDoSNOW(cl)}
\CommentTok{# }
\CommentTok{# }
\CommentTok{# #loop through different settings}
\CommentTok{# }
\CommentTok{# for (n_tree in c(100, 500, 1000, 1500)) \{}
\CommentTok{#   }
\CommentTok{#   set.seed(3333)}
\CommentTok{#   fit <- train(y = rf.label, x = rf.train.8,  method="rf", metric="Accuracy",  tuneGrid=tunegrid, trControl= control, ntree=n_tree)}
\CommentTok{# }
\CommentTok{#   key <- toString(n_tree)}
\CommentTok{#   model_list[[key]] <- fit}
\CommentTok{# \}}
\CommentTok{# }
\CommentTok{# #Shutdown cluster}
\CommentTok{# stopCluster(cl)}
\CommentTok{# save(model_list, file = "./data/RF_manual_search.rda")}
\CommentTok{# # the above code comneted out for output book file}

\KeywordTok{load}\NormalTok{(}\StringTok{"./data/RF_manual_search.rda"}\NormalTok{)}
\CommentTok{# compare results}
\NormalTok{results <-}\StringTok{ }\KeywordTok{resamples}\NormalTok{(model_list)}
\KeywordTok{summary}\NormalTok{(results)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## summary.resamples(object = results)
## 
## Models: 100, 500, 1000, 1500 
## Number of resamples: 30 
## 
## Accuracy 
##           Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's
## 100  0.7979798 0.8249158 0.8367003 0.8383838 0.8535354 0.8855219    0
## 500  0.8013468 0.8324916 0.8451178 0.8418631 0.8518519 0.8821549    0
## 1000 0.8013468 0.8282828 0.8434343 0.8415264 0.8518519 0.8787879    0
## 1500 0.8013468 0.8324916 0.8451178 0.8430976 0.8518519 0.8855219    0
## 
## Kappa 
##           Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's
## 100  0.5686275 0.6277878 0.6498418 0.6540654 0.6778695 0.7539114    0
## 500  0.5751073 0.6439474 0.6681725 0.6614719 0.6854823 0.7462468    0
## 1000 0.5751073 0.6327199 0.6676526 0.6608493 0.6823330 0.7394356    0
## 1500 0.5751073 0.6409731 0.6714760 0.6640467 0.6857492 0.7539114    0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#plot }
\KeywordTok{dotplot}\NormalTok{(results)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{11-fine-tune-models_files/figure-latex/manulsearch-1} 

}

\caption{The impact of the mtry numbers on model's accuracy.}\label{fig:manulsearch}
\end{figure}

We can see with the default \emph{mtry =3} setting, the best \emph{ntree} value is 1500. The model can reach 84.31\% accuracy.

\hypertarget{summary-9}{%
\section*{Summary}\label{summary-9}}


In this chapter, we have demonstrated the process of fine-tuning a prediction model's parameters so to achieve the best performance of the model to eliminate the possible model's overfit. We not only performed the fine-tune of a model's parameters but also demonstrated the process of fine-tune the other two factors that may cause the model's overfitting. They are the train data sampling and the predictors' selection.

We use the RF model as an example, starting from the order of all attributes prediction power, we have figured out the best collection of predictors including the number of predictors and the actual predictors. We've concluded the best predictor list is,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Predictor}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Sex, Title, Fare_pp, Ticket_class, Pclass, Ticket, Age, Friend_size, Deck"
\end{verbatim}

We also demonstrated the process of training dataset sampling. That is a basic technique used in the smallest dataset. Data sampling has a great impact on the model's performance. We have demonstrated the technique using \emph{k-folds CV}. We have concluded that the best training data sample is \textbf{\texttt{3-folds\ with\ repeats\ 10\ times}}.

And finally, we demonstrated the methods used to fine-tune a model's parameters. With RF, the only two parameters are: \texttt{mtry} and \texttt{ntree}. We have illustrated ``Random search'', ``Grid search'' and ``Manual search'' methods and find out the best parameters, based on the fixed predictors and the sampling, are \textbf{\texttt{mtry\ =\ 3}} and \textbf{\texttt{ntree\ =\ 1500}}.

Let us use these parameters to produce a model on the training dataset and make a prediction on the test dataset. We can then submit the final result to Kaggle for evaluation.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{tunegrid <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}\DataTypeTok{.mtry =} \DecValTok{3}\NormalTok{)}
\NormalTok{control <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method=}\StringTok{"repeatedcv"}\NormalTok{, }\DataTypeTok{number=}\DecValTok{3}\NormalTok{, }\DataTypeTok{repeats=}\DecValTok{10}\NormalTok{, }\DataTypeTok{search=}\StringTok{"grid"}\NormalTok{)}

\CommentTok{# # # the following code have been commented out just for produce the markdown file. so it will not wait for ran a long time}
\CommentTok{# # # set up cluster for parallel computing}
\CommentTok{# cl <- makeCluster(6, type = "SOCK")}
\CommentTok{# registerDoSNOW(cl)}
\CommentTok{#  }
\CommentTok{# Final_model <- train(y = rf.label, x = rf.train.8,  method="rf", metric="Accuracy",  tuneGrid=tunegrid, trControl= control, ntree=1500)}
\CommentTok{# }
\CommentTok{# #Shutdown cluster}
\CommentTok{# stopCluster(cl)}
\CommentTok{# }
\CommentTok{# save(Final_model, file = "./data/Final_model.rda")}
\CommentTok{# # # the above code commented out for output book file}

\KeywordTok{load}\NormalTok{(}\StringTok{"./data/Final_model.rda"}\NormalTok{)}

\CommentTok{# Make predictions}
\NormalTok{Prediction_Final <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(Final_model, test.submit.df)}
\CommentTok{#table(Prediction_Final)}

\CommentTok{# Write out a CSV file for submission to Kaggle}
\NormalTok{submit.df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{PassengerId =}\NormalTok{ test}\OperatorTok{$}\NormalTok{PassengerId, }\DataTypeTok{Survived =}\NormalTok{ Prediction_Final)}

\KeywordTok{write.csv}\NormalTok{(submit.df, }\DataTypeTok{file =} \StringTok{"./data/Prediction_Final.csv"}\NormalTok{, }\DataTypeTok{row.names =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We have got a score of 0.76076. Recall that our base model without fine-tune the Kaggle scores was 0.75598. It shows that our RF model has been increased by 0.5 percent. It seems not a lot but the technique and the process are far more important the increase of the accuracy.

\hypertarget{exercises-8}{%
\section*{Exercises}\label{exercises-8}}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Write a function to train Random forest models from a single predictor to full-house predictors to compare models' prediction accuracy and find the best number of predictors and the predictors.
\item
  Train Random forest models using the number of predictors find from exercise one and find out the best combination of the same number of predictors and the prediction accuracy.
\end{enumerate}

\hypertarget{report}{%
\chapter{Report}\label{report}}

\begin{center}\includegraphics[width=0.8\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/images/analytical-report} \end{center}

The last two steps in the process of doing a data science project are ``\textbf{Results Interpretation}'' and ``\textbf{Report and Communication}''. This chapter will explain the tasks and how to accomplish the tasks in these two final steps.

\textbf{Results Interpretation}\index{Results Interpretation}. There are three different explanations about the \emph{Results interpretation}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The first is explanation is from business people. They require data scientists to interpret or explain any results and findings from the analyses. These results or findings generally regard any patterns or insights discovered by the data analyses. The purpose is to provide reasons for and build credibility about the results. providing explanations can build a kind of trust in the results. This may require reporting on the process and methods used in the process to produce the results.
\item
  The second understanding of the \emph{Result interpretation} is the kind of data scientist's own assessment or evaluation of any findings. This is to find out if there are any defects or places for improvement. It was demonstrated in the process of \emph{Data modelling} in the previous chapters.
\item
  The third explanation about the \emph{Results interpretation} particularly in the area of business intelligence (BI)\index{BI}. \emph{Results interpretation} means an assessment of the business impact of the analytical results. This can only be done by the business people who have the domain expertise.
\end{enumerate}

What we mean by the \emph{results interpretation} here is a summary of all the efforts we have put into the data science project so far. Recognising data analysis may be an iteration process that may need continuous efforts, we should do a periodical summary and layout any further analyses necessary.

\textbf{Report and Communication} means to produce a narrative of the results, and draw a conclusion on how close the results address the original problems, and communicating about the outputs of analyses with interesting parties. When communicating with non-technical people, visual forms are more intuitive and worth a lot of words. So It makes sense to use data visualization tools or dashboards to communicate with people for your analytical results.

\hypertarget{content-of-report}{%
\section{Content of Report}\label{content-of-report}}

Perhaps the most asked question is what should be included in a report. Unfortunately, there is no unique answer to the question. The contents of a report depend on a number of factors such as the requirements of a project, the parties involved in the project like business organisations, and financial support bodies. Keeping in mind that the purpose of a report is to explain how close the analytical results address the original problems. It could include answers for the following questions:

\begin{itemize}
\item
  Does the results answer the original question and how?
\item
  Does the result help to defend against any objections and how?
\item
  Are there any limitations on the results? or any angles haven't been considered?
\end{itemize}

This chapter will provide an example report for the Titanic problem. This example report will include three parts: \textbf{Results explanation}, \textbf{Model interpretation}, and \textbf{Further analysing suggestions}.

\hypertarget{result-explainition}{%
\section{Result Explainition}\label{result-explainition}}

At results explanation, we will report on the efforts that we have put into solving the ``Titanic prediction problem'' through a process of \textbf{Data understanding}, \textbf{Data preprocess}, \textbf{Predictors selection}, \textbf{Model construction}, \textbf{Model cross validation} and \textbf{Model fine tune} . The report will provide a summary of the jobs done on the data analytical process.

\begin{itemize}
\item
  At the ``\emph{Data understanding}'' step, we went through individual attributes of both \texttt{train} and \texttt{test} datasets and examined their quality and quantity. We discovered the attributes that have the \emph{missing values} and some other problems. The discoveries set up the goals for the \emph{data preprocess} to be accomplished.
\item
  We typically examined the values of the response variable \emph{\texttt{Survived}} and its distributions. We recognised from the train datasets that about 2/3 of the passengers have perished. We have also examined relations between the individual attributes and the response variable. We've found that some attributes have no direct connections or impact on survival such as \emph{\texttt{Name}} and \emph{\texttt{Ticket}}. There is no evidence that someone perished because of her name or her ticket number. However, we've found that the \texttt{name} and the \texttt{Ticket} number contain information that can have an impact on survival such as \texttt{Title} and \texttt{Deck} (number, which reflects the location on the ship). This information needs to be abstracted by a technique called ``\textbf{Features Re-engineering}''\index{Features Re-engineering}.
\item
  At the ``\emph{Data preprocess}'', we have filled the missing values using different techniques. For attributes with a small portion of missing values, we filled the missing value with the average value, the random values from artificially generated data samples that have the same statistical characteristics of the attribute, or the values predicted with a machine learning model. For attributes With a large proportion of missing values, we created a new attribute that reflects the present (or absent) of the values. We re-engineered (created) many new attributes, so they can reflect the relations between the attributes and the response variable in a more meaningful and more accurate way.
\item
  At the ``\emph{Predictors selection}'', we have carried out \emph{correlation analyses} between individual attributes and the response variable and among the attributes themselves. \texttt{PCA} was used to figure out the most influential attributes despite that the method is mostly used for \emph{dimension reduction}\index{dimension reduction}.
\item
  ``\emph{Model building}'' is a key task in any data science project. \emph{Titanic prediction} is a binary classification problem. It can be addressed with many models including \textbf{Regression models} which are not ideal for a binary classification problem. We have tried the two most commonly used models ``\textbf{Decision tree}'' and ``\textbf{Random Forest}''. We can see that each model has a different prediction performance. During the model construction, the goal was to pursue a higher estimated prediction accuracy since we don't have access to the model's prediction accuracy at the production stage. This is problematic because a model can have a higher estimated accuracy during the model construction but has a much lower prediction accuracy while in real use or in production. It is difficult to know whether that is overfitting or underfitting in the model construction stage.
\item
  ``\emph{Cross Validation}'' (CV) is a commonly used technique to find and eliminate a model's overfitting problem. CV uses only a portion of training data in the model construction and uses the rest portion to test the constructed model since the leftover training data has values of the response variable\footnote{It is called label in machine learning}. So a comparison can be made and the prediction accuracy can be calculated. At the CV stage, We not only validated the models we have constructed but also several different types of models to compare their performance.
\item
  Most of the models are not in their best state when first build. A model's performance can be improved with \emph{``Fine Tune''} of the model's parameters and the use of the training dataset. We've fine-tuned a random forest model. With different trails on the \texttt{train} dataset provided, we have found the proper number of predictors and the actual combination of the predictors to use. The proper proportion of the training data was investigated and discovered.
\end{itemize}

Going through the entire data analyses process, we have produced our prediction model that is the \texttt{Random\ forest}. The best one is \textbf{\texttt{RF\_model2}}.

\hypertarget{model-interpretation}{%
\section{Model Interpretation}\label{model-interpretation}}

There are a number of ways to interpret a model. The commonly used methods are explaining \textbf{model's performance} and its \textbf{predictors' importance}.

\hypertarget{models-performance-measure}{%
\subsection*{Model's Performance Measure}\label{models-performance-measure}}


There are many different metrics that can be used to evaluate a prediction model. Depends on the type of the model, the different metrics are used to measure its performance. Random forest models built with the \texttt{Caret} package from R, the default metrics are \textbf{\texttt{Accuracy}} and \textbf{\texttt{Kappa}}. We have seen them in Chapters 9, 10, and Chapter 11.

\textbf{Accuracy} is the percentage of correctly classified instances out of all instances. It is useful on binary classification problems because it can be clear on exactly how the accuracy breaks down across the classes through a \emph{confusion matrix}\index{confusion matrix}.

\textbf{Kappa} is also called Cohen's Kappa\index{Cohen’s Kappa}. It is a normalized accuracy measurement. The normalization is at the baseline of random chance on the dataset. It is a more useful measure to use on problems that have an imbalance in the classes (e.g.~70-30 split for classes 0 and 1 and you can achieve 70\% accuracy by predicting all instances are for class 0).

For example, our best random forest model was \texttt{RF\_model2}. We can use these metrics to briefly explain our model's performance. Let us load model RF\_model2 and print out its model details:

\textbackslash begin\{figure\}

\{\centering \includegraphics[width=0.8\linewidth]{C:/Users/gangmin.li/Documents/Books/Bookdown-Do-Data/Capture/RF_model2}

\}

\textbackslash caption\{The detials of RF\_model2\}\label{fig:rfmodel2results}
\textbackslash end\{figure\}
We can see that the model's estimated accuracy (by the model construction) is \textbf{83.16\%}. That is \texttt{1-OOB}. Notice that the default parameters of the model are: \texttt{mtry\ =\ 2} and \texttt{ntree\ =\ 500}.

Looking into the model's confusion matrix, the RF\_model2's overall prediction errors can be interpreted based on the two classes ``\texttt{survived}'' or ``\texttt{perished}'': error on the predicted perished is 9.28\% and error on the predicted survive is 28.07\%. It is important to know the differences between them, as in many applications, they reflect the ``\emph{positive error}\index{positive error}'' and the ``\emph{negative error}''\index{negative error}. Depending on the interest, one class is regarded as the positive, the other will be the negative class. However, our model's performance is better on \texttt{perished} than on \texttt{survived}.

We can drill down a bit more, for example, we can check the top 10 decision trees' accuracy (error rate) among the 500 randomly generated trees by the \texttt{RF\ model}, and we can also check the average error rate among the 500 trees.

\begin{verbatim}
##             OOB          0         1
##  [1,] 0.2192192 0.16176471 0.3100775
##  [2,] 0.1963964 0.13157895 0.3004695
##  [3,] 0.1953010 0.12351544 0.3115385
##  [4,] 0.2110092 0.13404255 0.3344710
##  [5,] 0.2009804 0.11729622 0.3354633
##  [6,] 0.2028640 0.13203883 0.3157895
##  [7,] 0.1953216 0.12571429 0.3060606
##  [8,] 0.1900922 0.11588785 0.3093093
##  [9,] 0.1881414 0.11090573 0.3125000
## [10,] 0.1755379 0.09392265 0.3058824
\end{verbatim}

\begin{verbatim}
##       OOB               0                 1         
##  Min.   :0.1582   Min.   :0.08379   Min.   :0.2544  
##  1st Qu.:0.1650   1st Qu.:0.08925   1st Qu.:0.2807  
##  Median :0.1661   Median :0.09107   Median :0.2865  
##  Mean   :0.1670   Mean   :0.09298   Mean   :0.2858  
##  3rd Qu.:0.1684   3rd Qu.:0.09654   3rd Qu.:0.2895  
##  Max.   :0.2192   Max.   :0.16176   Max.   :0.3355
\end{verbatim}

To verify the constructed model's prediction accuracy, We did cross validations on the model \texttt{RF\_model2} and produced a CV model \texttt{RF\_model2\_cv}, We can show the results too.

\begin{verbatim}
## Random Forest 
## 
## 712 samples
##   8 predictor
##   2 classes: '0', '1' 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 641, 640, 642, 641, 640, 641, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##    2    0.8105369  0.5725714
##   17    0.8404408  0.6518406
##   32    0.8312040  0.6320845
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 17.
\end{verbatim}

The CV model confirmed the RF model's best accuracy is 84.04\% and the kappa is 65.18\%. But it revealed that the model has an overfitting problem. We, therefore, fine-tuned the RF model (with the different predictors that were identified by the fine tune itself). The fine-tuned model's accuracy is also around 84\%.

\hypertarget{visualise-models-prediction}{%
\subsection*{Visualise Model's Prediction}\label{visualise-models-prediction}}


We have said that a graph is worth hundreds of words. Visualising analytical results is important and useful. But directly visualise a model's prediction is difficult since the RF model has over 3 dimensions. Humans are not good at understanding any visual objects that have more than 3 dimensions even there are ways of visualization. So we need to find a way of reducing a model's dimension. \textbf{\texttt{Rtsne}} package from R is exactly designed for this purpose. You can check \texttt{Rtsne} document to find more details. Basically, Function \texttt{Rtsne} converts a multiple dimension dataframe's values into a 2D-coordinates. We can then use these 2d-coordinates to plot our response variable's value on it.

\textbackslash begin\{figure\}

\{\centering \includegraphics{12-report_files/figure-latex/visualmodelPrediction-1}

\}

\textbackslash caption\{2D Visualization of Model RE\_model2's Prediciton\}\label{fig:visualmodelPrediction}
\textbackslash end\{figure\}
The above code filtered the \texttt{train} dataset with only predictors used in \texttt{RF\_model2} through the \texttt{features} variable. It calls function \texttt{Rtsne} to generate two-dimension coordinates for each data sample in \texttt{train} based on the values of the attributes specified by \texttt{features}. The coordinates are stored in the sub-array \texttt{Y}. So we can dot plot the value of \texttt{survived} on the coordinates. Figure \ref{fig:visualmodelPrediction} shows the two-class distributions based on the features' values, which were used as predictors in the \texttt{RF\_model2}.

\hypertarget{importance-of-the-models-predictors}{%
\subsection*{Importance of the Model's Predictors}\label{importance-of-the-models-predictors}}


Another element of a model, which is worthwhile to report, is the predictors' importance. In our best random forest model RF\_model2, The predictors' importance is shown in Figure \ref{fig:RFmodel2impPlot}.

\begin{figure}

{\centering \includegraphics{12-report_files/figure-latex/RFmodel2impPlot-1} 

}

\caption{The ordered importance of the predictors}\label{fig:RFmodel2impPlot}
\end{figure}

\begin{verbatim}
##          Sex        Title      Fare_pp       Pclass Ticket_class   Group_size 
##  0.101687822  0.087587574  0.040794545  0.038645459  0.028333273  0.027816253 
##    Age_group     Embarked 
##  0.016016542  0.006786278
\end{verbatim}

From Figure \ref{fig:RFmodel2impPlot}, we can see that the \texttt{MeanDecreaseAccu} and the \texttt{MeanDecreaseGini} measure provide a different order. The difference reflects the different evaluation metrics. Briefly,

\begin{itemize}
\item
  \textbf{Mean Decrease in Impurity (MDI)}, the metric is the \texttt{GINI} index, it can be biased towards categorical features which contain many categories.
\item
  \textbf{Mean Decrease in Accuracy (MDA)}, the metrics is accuracy, it can provide low importance to other correlated features if one of them is given high importance.
\end{itemize}

If a model's performance report sets up the tasks for further analysis, then the predictors' importance report can set up the attributes to which features re-engineering should start.

\hypertarget{further-analysis}{%
\section{Further Analysis}\label{further-analysis}}

The previous section reports the constructed model (e.g.~\texttt{RF\_model2}) in terms of how it comes about and what was its limitations:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The model \texttt{RF\_model2} is not the best one and it is seriously overfitting. Its performance on the test dataset should be improved.
\item
  If any further work is planned, then it should start by considering re-engineer \emph{\texttt{Title}} and \emph{\texttt{Sex}} since they are the most important predictors in model \texttt{RF\_model2}.
\end{enumerate}

This section will demonstrate how to improve a constructed model's performance. We still use \texttt{RF\_model2} as an example. A good place to start is where it gets things wrong! To spot where things went wrong is difficult from numbers. A good technique is using graphs. However model \texttt{RF\_model2} has 500 decision trees. It is difficult to visualize 500 trees.

Recall that we have a decision tree model \texttt{model3}. It has the same predictors as the \texttt{RF\_model2}. We can use this decision tree (see Figure \ref{fig:simplottreemodel3} to find the place where things may go wrong.

\textbackslash begin\{figure\}

\{\centering \includegraphics{12-report_files/figure-latex/simplottreemodel3-1}

\}

\textbackslash caption\{The simple decision tree of RF\_model2\}\label{fig:simplottreemodel3}
\textbackslash end\{figure\}

From Figure \ref{fig:simplottreemodel3}, we can see that the single place that we got things wrong is the left branch of the first test condition, where the adult male passengers (as ``\texttt{Title\ =\ MR}'') has 81 passengers being wrongly predicted as survived. This is also confirmed by our model that the error rate of predicting passengers' survival is higher than the error rate of predicting passengers' perished. So re-engineer the attribute ``\texttt{Title}'' is a good place to start. This also coincides with the suggestion from the previous section where the importance order of the predictors used in the \texttt{RF\_model2}.

Now we will just demonstrate how to further re-engineering the \texttt{Title} attribute. The values of \texttt{Title} in the \texttt{train} dataset are as follows:

\begin{verbatim}
## 
## Master   Miss     Mr    Mrs  Other 
##     61    260    757    197     34
\end{verbatim}

We can see that there are 34 records in the training dataset which has the value of \texttt{Other} in the \texttt{Title} attribute. It is a good place where further purification can be done.

Let us go back to the raw dataset and abstract title for the name attribute.

\begin{verbatim}
## 
##         Capt          Col          Don         Dona           Dr     Jonkheer 
##            1            4            1            1            8            1 
##         Lady        Major       Master         Miss         Mlle          Mme 
##            1            2           61          260            2            1 
##           Mr          Mrs           Ms          Rev          Sir the Countess 
##          757          197            2            8            1            1
\end{verbatim}

It becomes obvious that the value of \texttt{Title} which has been categorized as \texttt{other} is too simplified. We can abstract more information such as gender and age from them. That information is useful for the prediction. It is also inappropriate to keep them as separate categories since some of them have a small number of instances, use them could lead to overfitting of the model.

Further, bin or bucket them into a more appropriate category is required. We can do so with the knowledge of nobility, locality (country of origin), and other knowledge such as time (at the beginning of the 20 century). For example, ``\texttt{Dona}'' and ``\texttt{the\ Countess}'' are female nobility equivalent to ``\texttt{Lady}'', and ``\texttt{Ms}'' and ``\texttt{Mlle}'' are essentially the same with ``\texttt{Miss}''; ``\texttt{Mme}'' is a military title equivalent to ``\texttt{Madame}'', so it can be categorized as ``\texttt{Mrs}''; ``\texttt{Jonkheer}'' is an honorific nobility in the Netherlands; and ``\texttt{Don}'' is the title of a university lecturer, they can be categorized as ``\texttt{Sir}''; ``\texttt{Col}'', ``\texttt{Capt}'', and ``\texttt{Major}'' are military ranks and can be replaced with a more general title ``\texttt{Officer}''. With all of these, we can reduce the number of title's category.

\begin{verbatim}
## 
##      Dr    Lady  Master    Miss      Mr     Mrs Officer     Rev     Sir 
##       8       3      61     264     757     198       7       8       3
\end{verbatim}

We can convert \texttt{Title} into a factor to plot their relations with the value of \texttt{Survived}.

\begin{figure}

{\centering \includegraphics{12-report_files/figure-latex/newtitlewithsuv-1} 

}

\caption{Surival Rates for new.Title}\label{fig:newtitlewithsuv}
\end{figure}

We could stop here since we have purified the Title's value \emph{other} with a more precise category in terms of semantic meaning. However, we notice that some values still have very small numbers. We should re-categorize those with small numbers categories like ``\texttt{Lady}'' and ``\texttt{Sir}'' into categories with larger numbers and keep the survival ratio as close as possible. We can categorize ``\texttt{Lady}'' into ``\texttt{Mrs}'', ``\texttt{Sir}'' and ``\texttt{Rev}'' into ``\texttt{Mr}'', For neutral titles like ``\texttt{Dr}'' and ``\texttt{Officer}'', we can categorize them into the title ``\texttt{Mr}'' and ``\texttt{Mrs}'' according to sex.

\begin{verbatim}
## 
##      Dr    Lady  Master    Miss      Mr     Mrs Officer     Rev     Sir 
##       0       0      61     264     782     202       0       0       0
\end{verbatim}

We can check the title against gender to see if any mistakes made.

\begin{figure}

{\centering \includegraphics{12-report_files/figure-latex/newcattitlewithsuv-1} 

}

\caption{Surival Rates for re-categorised new.Title}\label{fig:newcattitlewithsuv}
\end{figure}

After re-categorized the small number of titles, we only have 4 categories of titles. From the plot, we can see their survival radio is matched with the Survive radio of the attribute \texttt{Sex}.

We could use this re-engineered title attributes ``\textbf{\texttt{New\_Title}}'' to re-build RF models. The overall accuracy of the new models should be increased. The following code is an example of showing that. The new model has indeed increased the overall model's prediction accuracy by 0.45\%. It is not a lot but it approves the point that features re-engineer is a place to do a model's performance improvement.

\begin{verbatim}
## 
## Call:
##  randomForest(formula = as.factor(Survived) ~ Sex + Fare_pp +      Pclass + New_Title + Age_group + Group_size + Ticket_class +      Embarked, data = RE_data[1:891, ], importance = TRUE) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 2
## 
##         OOB estimate of  error rate: 16.05%
## Confusion matrix:
##     0   1 class.error
## 0 504  45  0.08196721
## 1  98 244  0.28654971
\end{verbatim}

We can further do the same with many other attributes or a combination of multiple attributes.

\hypertarget{summary-10}{%
\section*{Summary}\label{summary-10}}


This chapter demonstrated the final two steps in doing a data science project: ``\textbf{Results Interpretation}'' and ``\textbf{Report and Communication}''. They are ignored by most data scientists if the analytical work is not required by the funding body or mandated project initiator. I would suggest that try to write some kind of report or analytical results' interpretation even for a data experimental project. The report can be brief or completed. However, it is important to go through a series of thoughts about the results obtained in response to the initial data analytical problem. Asking whether or not that the results answer the original question? Are there any limitations on the results? or any angles haven't been considered? Most people agree that the analytical results of a data science project is not an engineering solution of a problem. It may need multiple rounds of recursive actions. Sometimes, the analytical results is a starting point of another circle or project. With this understanding, a periodical report is even more important.

A report can contain different contents from process summary to particular model explanation and the interpretation of the results. Numbers with some contextual explanation are useful but graphs can speak more than numbers and text. Therefore a lot of reports using graphical dashboards and data visualization tools.

In the end, it is the sense and opinion which data analytical results supported counts. Furthermore, how these senses and opinions are understood and accepted by other people rather than the data scientists are the goal of a data science project.

\hypertarget{exercises-9}{%
\section*{Exercises}\label{exercises-9}}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Visualise prediction results is a useful way to find the problem. using `\textbf{Rtsne}' package from R to visualize decision tree model2 both the left branch and the right branch's prediction, compare them.
\end{enumerate}

\begin{verbatim}
features <- c("Sex", "Fare_pp", "Pclass", "Title", "Age_group", "Group_size", "Ticket_class", "Embarked")

Tree.left <- train[train$Title == "Mr",]

set.seed(984357)

tsne.left <- Rtsne(Tree.left[, features], check_duplicates = FALSE)

ggplot(NULL, aes(x = tsne.left$Y[, 1], y = tsne.left$Y[, 2],
                 color = Tree.left$Survived)) +
  geom_point() +
  labs(color = "Survived") +
  ggtitle("Visualization of left branch of tree where title is 'Mr'")

#
Tree.right <- train[train$Title != "Mr",]

set.seed(984357)
tsne.right <- Rtsne(Tree.right[, features], check_duplicates = FALSE)
ggplot(NULL, aes(x = tsne.right$Y[, 1], y = tsne.right$Y[, 2],
                 color = Tree.right$Survived)) +
  geom_point() +
  labs(color = "Survived") +
  ggtitle("Visualization of right branch of the tree")
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Considering re-engineer passengers with the same tickets.
\end{enumerate}

\hypertarget{appendix-apendix-the-r-code-of-the-entire-project}{%
\chapter{(APPENDIX) Apendix: The R code of the entire project}\label{appendix-apendix-the-r-code-of-the-entire-project}}

The entire code are stored in 6 files:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  TitanicDataAnalysis1\_UnderstandData.R
\item
  TitanicDataAnalysis2\_Data\_Preprocess.R
\item
  TitanicDataAnalysis3\_Model\_Construction.R
\item
  TitanicDataAnalysis4\_Model\_Cross\_Valid.R
\item
  TitanicDataAnalysis5\_Model\_Fine\_Tune.R
\item
  TitanicDataAnalysis6\_Analyse\_Report.R
\end{enumerate}

They are also available on-line.

\hypertarget{titanicdataanalysis1_understand_data.r}{%
\section*{TitanicDataAnalysis1\_Understand\_Data.R}\label{titanicdataanalysis1_understand_data.r}}


\begin{verbatim}
############################################################################
# Copyright 2020 Gangmin Li
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# The base of this code was from Dave Langer "Intoduction to Data Science."
# Credit to him. You can find him on Github: https://github.com/EasyD
# I have also integrated other sources like Titanic Forum, Python code
# and some Chinese R code.
#
# Notably,
# https://www.kaggle.com/startupsci/titanic-data-science-solutions
#
# The whole purpose of this is to teach My students on Data Science.
#
# This R source code file corresponds to video 1 of the YouTube series
# "Introduction to Data Science with R" located at the following URL:
#     http://www.youtube.com/watch?v=32o0DnuRjfg
#
# The task is to build a model based on the train data
# then to predict test data who can survive in Kaggle Titanic competition
##########################################################################
# 1. Understand Data
##########################################################################

### Load raw data

train <- read.csv("train.csv", header = TRUE)
test <- read.csv("test.csv", header = TRUE)

# RStudio help function
# what help with any R function just type ?
# for example ?read.csv in Console

# First explore of datasets
# the first R code you can use is str.
# use ?str to find more

help(str)

# use str to explore train and test
str(train)
str(test)

# Add a "Survived" variable to the test set to allow for combining data sets
test <- data.frame(Survived = rep("NA", nrow(test)), test[,])

# Now check test. you can see that it has 12 variable now.
# And Survived is the first variable. because we used test[,]
# if you want add Survived into the second position Do this,
# test <- data.frame(test[1], Survived = rep("NA", nrow(test)), test[,2:ncol(test)])

# Combine datasets together. actually append test to train
data <- rbind(train, test)
# We may need to keep the raw data into a file in case we need it later.
write.csv(data, "data.cvs", row.names = FALSE )

#### We have done combined datasest with only a few line of code
# notice that the type of Survived has been changed to Chr.
# This is because we used "NA" as its value
# A bit about R data types
# ?str structure of dataset
# chr, int
# Factor in R is 'category'. it likes a selection from a list.
# for example, Cabin Factor w/187 levels. It means there are 187 selections.
# Sex Factor w/ 2 "female", "male", 2,1 means, two options 2- female, 1- male.
#
# NA : not available (absent value, missing value)

str(data)

# Exam PassengerID, type INT, we can check total number and the number of unique values.
# If they are equal and both equal to the number of records. it means there are
# unique and has no missing value.
length(data$PassengerId)
length(unique(data$PassengerId))

### Exam Survived
data$Survived <- as.factor(data$Survived)
table(data$Survived)

# Calculate the survive rate in train data is 38% and the death rate is 61.61%
prop.table(table(as.factor(train$Survived)))

### Examine Pclass value,
# Look into Kaggle's explanation about Pclass: it is a proxy for social class i.e. rich or poor
# It should be factor and it does not make sense to stay in int.
data$Pclass <- as.factor(data$Pclass)
test$Pclass <- as.factor(test$Pclass)
train$Pclass <- as.factor(train$Pclass)

# Distribution across classes
table(data$Pclass)

# Distribution across classes with survive
table(data$Pclass, data$Survived)

# Calculate the distribution on Pclass
# Overall passenger distribution on classes.
prop.table(table(data$Pclass))

# Train data passenger distribution on classes.
prop.table(table(train$Pclass))

# Test data passenger distribution on classes.
prop.table(table(test$Pclass))

# Calculate death distribution across classes with Train data
SurviveOverClass <- table(train$Pclass, train$Survived)
# Convert SurviveOverClass into data frame
SoC.data.fram <- data.frame(SurviveOverClass)
# Retrieve death distribution in classes
Death.distribution.on.class <- SoC.data.fram$Freq[SoC.data.fram$Var2==0]
prop.table(Death.distribution.on.class)

# Calculate death rate in train data
# Distribution across classes with survive in Train data
SurviveOverClass <- table(train$Pclass, train$Survived)
# Convert SurviveOverClass into data frame
SoC.data.fram <- data.frame(SurviveOverClass)
# Retrieve death distribution in classes
Death.distribution.on.class <- SoC.data.fram$Freq[SoC.data.fram$Var2==0]
prop.table(Death.distribution.on.class)

# Try summary on data
summary.data.frame(data)

## Exploratory data analysis with graph
# Load up ggplot2 package to use for visualizations
# load it into memory
library(ggplot2)

# High class passenger has more chance of survive than passenger with lower class
# Hypothesis - Rich passengers can but expensive ticket. class=1 is more expensive
# survived at a higher rate
ggplot(train, aes(x = Pclass, fill = factor(Survived))) +
  geom_bar(width = 0.5) +
  xlab("Pclass") +
  ylab("Total Count") +
  labs(fill = "Survived")

# It sort of proved the survive rate with social class
# more people perished in the third class

### Exam Name attribute
# the original name typed as factor, which we really don't want (shows the uniqueness)
# convert Name type
data$Name <- as.character(data$Name)

# Confirm the name has 1037 unique values
length(unique(data$Name))

# Find the two duplicate names
# First used which function to get the duplicate names and store them as a vector dup.names
# check it up ?which.
dup.names <- data[which(duplicated(data$Name)), "Name"]

# Echo out
dup.names

### Exam Sex attribute
# Retrial male and females. then check their numbers.
summary(data$Sex)

#Plot Sex distribution on entire dataset and get general an impression
ggplot(data[1:891,], aes(x = Sex)) +
  geom_bar(fill="steelblue") +
  xlab("Sex") +
  ylab("Total Count")

# plot Survived over Sex on train. use data[1:891,]
ggplot(data[1:891,], aes(x = Sex, fill = Survived)) +
  geom_bar() +
  xlab("Sex") +
  ylab("Total Count") +
  labs(fill = "Survived")

### Examine Age
# Summary over data, train and test.
summary(data$Age)
summary(train$Age)
summary(test$Age)

#It makes sense to change Age type to Factor to see distribution
summary(as.factor(data$Age))

# Plot distribution of age group
ggplot(data, aes(x = Age)) +
  geom_histogram(binwidth = 10, fill="steelblue") +
  xlab("Age") +
  ylab("Total Count")

# Plot Survived on age group on train dataset
ggplot(data[1:891,], aes(x = Age, fill = Survived)) +
  geom_histogram(binwidth = 10) +
  xlab("Age") +
  ylab("Total Count")

### Exam SibSp, Its original type is int
summary((data$SibSp))

# How many possible unique values?
length(unique(data$SibSp))

# Treat it as a factor, so we know the value distribution
data$SibSp <- as.factor(data$SibSp)
summary(data$SibSp)

# Plot entire SibSp distribution among the 7 values
ggplot(data, aes(x = SibSp)) +
  geom_bar() +
  xlab("SibSp") +
  ylab("Total Count")+
  coord_cartesian()

# Plot on the Survived on SibSp
ggplot(data[1:891,], aes(x = SibSp, fill = Survived)) +
  geom_bar() +
  xlab("SibSp") +
  ylab("Total Count")  +
  labs(fill = "Survived")

### Exam Parch
summary(data$Parch)

# How many possible values?
length(unique(data$Parch))

# Treat is as a factor so we know the value distribution
data$Parch <- as.factor(data$Parch)
summary(data$Parch)

# Plot entire Parch distribution among the 8 posibilites
ggplot(data, aes(x = Parch)) +
  geom_bar() +
  xlab("Parch") +
  ylab("Total Count")

# Plot on the Survived on SibSp
ggplot(data[1:891,], aes(x = Parch, fill = Survived)) +
  geom_bar() +
  xlab("Parch") +
  ylab("Total Count")  +
  labs(fill = "Survived")

### Exam  ticket
summary(data$Ticket)
length(unique(data$Ticket))
str(data$Ticket)
which(is.na(data$Ticket))

# Plot it value
ggplot(data[1:891,], aes(x = Ticket)) +
  geom_bar() +
  xlab("Ticket") +
  ylab("Total Count")

# Plot on the survive on Ticket
ggplot(data[1:891,], aes(x = Ticket, fill = Survived)) +
  geom_bar() +
  xlab("Ticket") +
  ylab("Total Count")  +
  labs(fill = "Survived")

### Examine Fare
summary(data$Fare)
length(unique(data$Fare))

# Can't make fare a factor, treat as numeric & visualize with histogram
ggplot(data, aes(x = Fare)) +
  geom_histogram(binwidth = 5) +
  ggtitle("Fare Distribution") +
  xlab("Fare") +
  ylab("Total Count") +
  ylim(0,200)

# Let's check to see if fare has predictive power
ggplot(data[1:891,], aes(x = Fare, fill = Survived)) +
  geom_histogram(binwidth = 5) +
  xlab("fare") +
  ylab("Total Count") +
  ylim(0,50) +
  labs(fill = "Survived")

# Explore Fare distribution between train and test to see if they are overlapped?
ggplot(train, aes(x = Fare)) +
  geom_histogram(binwidth = 5) +
  ggtitle("Fare Distribution") +
  xlab("Fare") +
  ylab("Total Count") +
  ylim(0,200)

ggplot(test, aes(x = Fare)) +
  geom_histogram(binwidth = 5) +
  ggtitle("Fare Distribution") +
  xlab("Fare") +
  ylab("Total Count") +
  ylim(0,200)

### Cabin
# Examine cabin values
str(data$Cabin)
# Cabin really isn't a factor, make a string and the display first 100
data$Cabin <- as.character(data$Cabin)
data$Cabin[1:100]

# Find number of the missing value
table(train[which(train$Cabin ==""), "Cabin"])

# Analysis of the cabin variable
str(data$Cabin)

# Cabin really isn't a factor, make a string and the display first 100
data$Cabin <- as.character(data$Cabin)
data$Cabin[1:100]

# Find out number of the missing value in the train
train$Cabin <- as.character(train$Cabin)
table(train[which(train$Cabin ==""), "Cabin"])

# Replace empty cabins with a "U"
#data[which(data$Cabin == ""), "Cabin"] <- "U"
data$Cabin[1:100]

# Take a look at just the first char as a factor
cabin.first.char <- as.factor(substr(data$Cabin, 1, 1))
str(cabin.first.char)
levels(cabin.first.char)

# Add to combined data set and plot
data$cabin.first.char <- cabin.first.char

# High level plot
ggplot(data[1:891,], aes(x = cabin.first.char, fill = Survived)) +
  geom_bar() +
  ggtitle("Survivability by cabin.first.char") +
  xlab("cabin.first.char") +
  ylab("Total Count") +
  ylim(0,750) +
  labs(fill = "Survived")

### Examine Embark
str(data$Embarked)
summary(data$Embarked)

# Plot Embarked data distribution and the Survived data over it
ggplot(data, aes(x = Embarked)) +
  geom_bar(width=0.5) +
  xlab("Passenger embarked port") +
  ylab("Total Count")

ggplot(data[1:891,], aes(x = Embarked, fill = Survived)) +
  geom_bar(width=0.5) +
   xlab("embarked") +
  ylab("Total Count") +
  labs(fill = "Survived")

##Calculate death RATE distribution over Embarked port with Train data
# We use table in R, you can check with ?table. A good example is
# mytable <- table(A,B) # A will be rows, B will be columns
# mytable # print table

# margin.table(mytable, 1) # A frequencies (summed over B)
# margin.table(mytable, 2) # B frequencies (summed over A)

# prop.table(mytable) # cell percentages
# prop.table(mytable, 1) # row percentages
# prop.table(mytable, 2) # column percentages

# We need prop.table to get column percentage which is the survived

# creat Embarked and Survived contingency table
SurviveOverEmbarkedTable <- table(train$Embarked, train$Survived)
# Death-0/survived-1 value distribution (percentage) based on embarked ports
# prop.table(mytable, 2) give us column (Survived) percentages
Deathandsurvivepercentage <- prop.table(SurviveOverEmbarkedTable, 2)
# Plot
M <- c("c-Cherbourg", "Q-Queenstown", "S-Southampton")
barplot(Deathandsurvivepercentage[2:4,1]*100, xlab =(""), ylim=c(0,100), ylab="Death distribution in percentage %",  names.arg = M, col="steelblue", main="Death distribution", border="black", beside=TRUE)
barplot(Deathandsurvivepercentage[2:4,2]*100, xlab =(""), ylim=c(0,100), ylab="Death distribution in percentage %",  names.arg = M, col="blue", main="Death distribution", border="black", beside=TRUE)

## Calculate survived RATE distribution based on embarked ports
# Death-0/survived-1 value distribution (percentage) based on embarked ports
# prop.table(mytable, 1) give us row (Port) percentages
# col-1 (Survived=0, perished) and col-2 (Survived =1, survived)
DeathandsurviveRateforeachport <- prop.table(SurviveOverEmbarkedTable, 1)
#plot
barplot(Deathandsurvivepercentage[2:4,1]*100, xlab =(""), ylim=c(0,100), ylab="Death rate in percentage %",  names.arg = M, col="red", main="Death rate comparison among mebarked ports", border="black", beside=TRUE)

##END 1 ######################################################
\end{verbatim}

\hypertarget{titanicdataanalysis2_datapreprocess.r}{%
\section*{TitanicDataAnalysis2\_Data+Preprocess.R}\label{titanicdataanalysis2_datapreprocess.r}}


\begin{verbatim}
##############################################################
# 2. Data preprocess
###############################################################
## Assume we had imported both train and test dataset and we have combined them
# into on data
# train <- read.csv("train.csv", header = TRUE)
# test <- read.csv("test.csv", header = TRUE)
## Integrate into one file for checking save to do the dame for both files.
# data <- bind_rows(train, test) # compare with data <- rbind(train, test)
#
# If we save the file from the previous code we can load it directly
data <- read.csv("data.csv", header = TRUE)

# Check our combined dataset details
glimpse(data) # compare with str(data)

# Define a function to check missing values
missing_vars <- function(x) {
  var <- 0
  missing <- 0
  missing_prop <- 0
  for (i in 1:length(names(x))) {
    var[i] <- names(x)[i]
    missing[i] <- sum(is.na(x[, i])|x[, i] =="" )
    missing_prop[i] <- missing[i] / nrow(x)
  }

  (missing_data <- data.frame(var = var, missing = missing, missing_prop = missing_prop) %>%
      arrange(desc(missing_prop)))
}
#############################################
## Dealing with missing values
#############################################
# Large number of missing values, itself can be meaningful.
# Add newly created attribute and assign it with new values
data$HasCabinNum <- ifelse((data$Cabin != ""), "Has", "HasNo")

# Examine the relations between our newly created cabin replacement's
# 'HasCabinNum' with the attribute Survival using plot

# The forst plot is the numbers
# Make sure survived is in factor type
p1 <- data %>%
  filter(!is.na(Survived)) %>%
  ggplot(aes(x = factor(HasCabinNum), fill = factor(Survived))) +
  geom_bar(width = 0.5) +
  xlab("HasCabinNum") +
  ylab("Total Count") +
  labs(fill = "Survived")+
  ggtitle("Newly created HasCabinNum attribute on Survived")

# The 2nd plot shows survive percentage of HasCabinNum
p2 <- data %>%
  filter(!is.na(Survived)) %>%
  ggplot(aes(x = factor(HasCabinNum), fill = factor(Survived))) +
  geom_bar(position = "fill", width = 0.5) +
  scale_y_continuous(labels = scales::percent, breaks = seq(0, 1, 0.1)) +
  scale_fill_discrete(name = "Survived") +
  labs(x = "HasCabinNum", y = "Percentage of Survived") +
  ggtitle("Newly created HasCabinNum attribute (Proportion Survived)")

##### Dealing with missing values in Cabin #####

### 1. Replace missing value in Age with its average
ageEverage <- summarise(data, Average = mean(Age, na.rm = TRUE))
# create a new attribute Age_RE1 and assign it with new values
data$Age_RE1 <- ifelse(is.na(data$Age), as.numeric(ageEverage), as.numeric(data$Age))

# Plot newly altered age attribute
# Make sure survived is in factor type
p1 <- data %>%
  filter(!is.na(Survived)) %>%
  ggplot(aes(x = factor(Age_RE1), fill = factor(Survived))) +
  geom_bar(width = 0.5) +
  xlab("Age_RE1") +
  ylab("Total Count") +
  labs(fill = "Survived")+
  ggtitle("Survived value on Age_RE1")

# Show survive percentage on HasCabinNum
p2 <- data %>%
  filter(!is.na(Survived)) %>%
  ggplot(aes(x = factor(Age_RE1), fill = factor(Survived))) +
  geom_bar(position = "fill", width = 0.5) +
  scale_y_continuous(labels = scales::percent, breaks = seq(0, 1, 0.1)) +
  scale_fill_discrete(name = "Survived") +
  labs(x = "Age_RE1", y = "Percentage of Survived") +
  ggtitle("Survived percentage on Age_RE1")

### 2. Take a random number range between `min` and `max` age,
# and keep the mean and standard deviation unchanged.
# calculate the non-NA mean and std
mean <- mean(data[["Age"]], na.rm = TRUE) # take  mean
std <- sd(data[["Age"]], na.rm = TRUE) # take  std
# Replace NA with a list that maintian the mean and std
temp_rnum <- rnorm(sum(is.na(data$Age)), mean=mean, sd=std)
# Add new attribute Age_RE2
data$Age_RE2 <- ifelse(is.na(data$Age), as.numeric(temp_rnum), as.numeric(data$Age))
summary(data$Age_RE2)
# There are possible negative values too, replace them with positive values
data$Age_RE2[(data$Age_RE2)<=0] <- sample(data$Age[data$Age>0], length(data$Age_RE2[(data$Age_RE2)<=0]), replace=F)

# Check result
summary(data$Age_RE2)

# Plot newly altered age attribute
# Make sure survived is in factor type
p1 <- data %>%
  filter(!is.na(Survived)) %>%
  ggplot(aes(x = factor(Age_RE2), fill = factor(Survived))) +
  geom_bar(width = 0.5) +
  xlab("Age_RE2") +
  ylab("Total Count") +
  labs(fill = "Survived")+
  ggtitle("Survived value on Age_RE2 attribute")

# Show survive percentage on HasCabinNum
p2 <- data %>%
  filter(!is.na(Survived)) %>%
  ggplot(aes(x = factor(Age_RE2), fill = factor(Survived))) +
  geom_bar(position = "fill", width = 0.5) +
  scale_y_continuous(labels = scales::percent, breaks = seq(0, 1, 0.1)) +
  scale_fill_discrete(name = "Survived") +
  labs(x = "Age_RE2", y = "Percentage of Survived") +
  ggtitle("Survived percentage on Age_RE2 attribute")

###Using machine generate model
# to produce new values based on other exiting values
# confirm Age missing values
# get the origianl so it can be compared later
data$Age_RE3 <- data$Age
summary(data$Age_RE3)
# Construct a decision tree with selected attributes and ANOVA method
Agefit <- rpart(Age_RE3 ~ Survived + Pclass + Sex + SibSp + Parch + Fare + Embarked,
                data=data[!is.na(data$Age_RE3),],
                method="anova")
#Fill AGE missing values with prediction made by decision tree prediction
data$Age_RE3[is.na(data$Age_RE3)] <- predict(Agefit, data[is.na(data$Age_RE3),])
#Confirm the missing values have been filled
summary(data$Age_RE3)

p1 <- data %>%
  filter(!is.na(Survived)) %>%
  ggplot(aes(x = factor(Age_RE3), fill = factor(Survived))) +
  geom_bar(width = 0.5) +
  xlab("Age_RE3") +
  ylab("Total Count") +
  labs(fill = "Survived")+
  ggtitle("Survived value on Age_RE3 attribute")

# Show survive percentage on HasCabinNum
p2 <- data %>%
  filter(!is.na(Survived)) %>%
  ggplot(aes(x = factor(Age_RE3), fill = factor(Survived))) +
  geom_bar(position = "fill", width = 0.5) +
  scale_y_continuous(labels = scales::percent, breaks = seq(0, 1, 0.1)) +
  scale_fill_discrete(name = "Survived") +
  labs(x = "Age_RE3", y = "Percentage of Survived") +
  ggtitle("Survived percentage on Age_RE3 attribute")

# Choose use one results as Age, we use machine generated
data$Age <- data$Age_RE3
data <- subset(data, select = -c(Age_RE1, Age_RE2, Age_RE3))

### Deal with Fare Attribute
#
# just one missing value so replace it with mean or median value
data[is.na(data$Fare), ]
data$Fare[is.na(data$Fare)] <- median(data$Fare, na.rm = T)

### Embarked Attribute
# we take two steps:
# 1. find out the passenger has a shared ticket or not.
# If the ticket is shared than find the travel companion's
# embarked port and take that as the passenger's embarked port;
# 2. If the ticket is not shared or shared partner's
# embarked port is also missing, find out the ticket price
# per person and compare with other ticket's price per
# person to allocate the embarked port.

# List info of the missing records to figure out the fare and the ticket?
data[(data$Embarked==""), c("Embarked", "PassengerId",  "Fare", "Ticket")]
# we want find out if the fare is a single ticket or a group ticket.
# we need to find out is there other passenger share the ticket?
data[(data$Ticket=="113572"), c("Ticket", "PassengerId", "Embarked", "Fare")]

# Calculate fare_PP per person
fare_pp <- data %>%
  group_by(Ticket, Fare) %>%
  dplyr::summarize(Friend_size = n()) %>%
  mutate(Fare_pp = Fare / Friend_size)
data <- left_join(data, fare_pp, by = c("Ticket", "Fare"))

# Plot Fare per person on Embarked port
data %>%
  filter((Embarked != "")) %>%
  ggplot(aes(x = Embarked, y = Fare_pp)) +
  geom_boxplot() +
  geom_hline(yintercept = 40, col = "deepskyblue4")

# Assign `C` to the embarked missing value.
data$Embarked[(data$Embarked)==""] <- "C"

# Confirm the missing values have been fulfilled.
missing_vars(data)

#######################################
##### Attribute Re-engineering #####
#######################################
#
### Title from Name attribute
#
# Abstract Title out
data$Title <- gsub('(.*, )|(\\..*)', '', data$Name)
data %>%
  group_by(Title) %>%
  dplyr::count() %>%
  arrange(desc(n))

# Group those less common title’s into an ‘Other’ category.
data$Title <- ifelse(data$Title %in% c("Mr", "Miss", "Mrs", "Master"), data$Title, "Other")
# Checking the table of *Title* vs *Sex* shows nothing anomalous
L<- table(data$Title, data$Sex)
knitr::kable(L, digits = 2, booktabs = TRUE, caption = "Title and sex confirmation")

# Plot the results
data %>%
  filter(!is.na(Survived)) %>%
  ggplot(aes(x = factor(Title), fill = factor(Survived))) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent, breaks = seq(0, 1, 0.1)) +
  scale_fill_discrete(name = "Survived") +
  labs(x = "Title", y = "Survival Percentage") +
  ggtitle("Title attribute (Proportion Survived)")

### Deck from Cabin attribute
data$Cabin <- as.character(data$Cabin)
data$Deck <- ifelse((data$Cabin == ""), "U", substr(data$Cabin, 1, 1))
# Plot our newly created attribute relation with Survive
p1 <- ggplot(data[1:891,], aes(x = Deck, fill = factor(Survived))) +
  geom_bar(width = 0.5) +
  labs(x = "Deck number", y = "Total account") +
  labs(fill = "Survived")

# Plot percentage of survive
p2 <- data %>%
  filter(!is.na(Survived)) %>%
  ggplot(aes(x = factor(Deck), fill = factor(Survived))) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent, breaks = seq(0, 1, 0.1)) +
  scale_fill_discrete(name = "Survived") +
  labs(x = "Deck number", y = "Percentage") +
  ggtitle("Newly created Deck number (Proportion Survived)")

### Extract ticket class from ticket number

data$Ticket <- as.character(data$Ticket)
data$Ticket_class <- ifelse((data$Ticket != " "), substr(data$Ticket, 1, 1), "")
data$Ticket_class <- as.factor(data$Ticket_class)

# Plot our newly created attribute relation with Survive
p1 <- data %>%
  filter(!is.na(Survived)) %>%
  ggplot(aes(x = Ticket_class, fill = factor(Survived))) +
  geom_bar(width = 0.5) +
  labs(x = "Ticket_class", y = "Total account") +
  labs(fill = "Survived value over Ticket class")

# Plot percentage of survive
p2 <- data %>%
  filter(!is.na(Survived)) %>%
  ggplot(aes(x = factor(Ticket_class), fill = factor(Survived))) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent, breaks = seq(0, 1, 0.1)) +
  scale_fill_discrete(name = "Survived") +
  labs(x = "Ticket_class", y = "Percentage") +
  ggtitle("Survived percentage over Newly created Ticket_class")

### Travel in Groups
data$Family_size <- data$SibSp + data$Parch + 1
data$Group_size <- pmax(data$Family_size, data$Friend_size)

# Plot our newly created attribute's prediction power
p1 <- data %>%
  filter(!is.na(Survived)) %>%
  ggplot(aes(x = Group_size, fill = factor(Survived))) +
  geom_histogram() +
  scale_y_continuous(breaks = seq(0, 700, 100)) +
  scale_x_continuous(breaks = seq(0, 10)) +
  scale_fill_discrete(name = "Survived") +
  labs(x = "Group Size: max(Family Size, Group Size)", y = "Count") +
  ggtitle("Survived count over groupsize")

# Plot percentage of survive
p2 <- data %>%
  filter(!is.na(Survived)) %>%
  ggplot(aes(x = Group_size, fill = factor(Survived))) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent, breaks = seq(0, 1, 0.1)) +
  scale_fill_discrete(name = "Survived") +
  labs(x = "Group_size", y = "Percentage") +
  ggtitle("Survived percentage over Newly created Group_size")

### Age in Groups
# Set bins
Age_labels <- c('0-9', '10-19', '20-29', '30-39', '40-49', '50-59', '60-69', '70-79')
# Assign labels
data$Age_group <- cut(data$Age, c(0, 10, 20, 30, 40, 50, 60, 70, 80), include.highest=TRUE, labels= Age_labels)

# Plot
p1 <- data %>%
  filter(!is.na(Survived)) %>%
  ggplot(aes(x = Age_group, y = ..count.., fill = factor(Survived))) +
  geom_bar() +
  ggtitle("Survived value ove newly created Age_group")

# Plot percentage of survive
p2 <- data %>%
  filter(!is.na(Survived)) %>%
  ggplot(aes(x = Age_group, fill = factor(Survived))) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent, breaks = seq(0, 1, 0.1)) +
  scale_fill_discrete(name = "Survived") +
  labs(x = "Age group", y = "Percentage") +
  ggtitle("Survived percentage ove newly created Age_group")

### Fare per passenger
data$Fare_pp <- data$Fare/data$Friend_size

# Plot Fare_PP against Survived
p1<- data %>%
  filter(!is.na(Survived)) %>%
  ggplot(aes(x = Fare_pp, fill = factor(Survived))) +
  geom_histogram(binwidth = 2) +
  scale_y_continuous(breaks = seq(0, 500, 50)) +
  scale_fill_discrete(name = "Survived") +
  labs(x = "Fare (per person)", y = "Count") +
  ggtitle("Survived value over Fare_pp")
p1
# Plot percentage of survive
p2 <- data %>%
  filter(!is.na(Survived)) %>%
  ggplot(aes(x = factor(Fare_pp), fill = factor(Survived))) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent, breaks = seq(0, 1, 0.1)) +
  scale_fill_discrete(name = "Survived") +
  labs(x = "Fare per person", y = "Percentage") +
  ggtitle("Survived rate over newly created Fare_PP")
p2

# Plot in box plot
data %>%
  filter(!is.na(Survived)) %>%
  filter(Fare > 0) %>%
  ggplot(aes(factor(Survived), Fare_pp)) +
  geom_boxplot(alpha = 0.2) +
  scale_y_continuous(trans = "log2") +
  geom_point(show.legend = FALSE) +
  geom_jitter()

## Build Re-engineered Dataset
glimpse(data)
# Remove abundant attributs
RE_data <- subset(data, select = -c(Name, Cabin, Fare))
# Write in file
write.csv(RE_data, file = "RE_Data.CSV", row.names = FALSE)
### END 2 Data preprocess #################################################
\end{verbatim}

\hypertarget{titanicdataanalysis3_model_construction.r}{%
\section*{TitanicDataAnalysis3\_Model\_Construction.R}\label{titanicdataanalysis3_model_construction.r}}


\begin{verbatim}
####################################################
# Prediction Model Construction
####################################################
# This file contains prediction model construction
#
#  1. Decision trees
#  2. Random forest
#
##### Predictor selection
# load Library
library(dplyr)# data manipulation
library(tidyverse)
library(caret) # tuning & cross-validation
library(gridExtra) # visualizations


#load re-engineered dataset
RE_data <- read.csv("RE_data.csv", header = TRUE)

# check data
glimpse(RE_data)
summary(RE_data)

### A quick correlation analysis
# and plot of the numeric attributes
# to get an idea of how they might relate to one another.

# convert non numeric types to numeric
RE_data <- RE_data %>% mutate_if(is.factor,  as.numeric)

# plot correlation among numeric attributes
cor <- RE_data %>% select(., -c(Ticket, PassengerId)) %>%
  cor(use="pairwise.complete.obs") %>%
  corrplot::corrplot.mixed(upper = "circle", tl.col = "black")

# show results in a table
library(kableExtra) # markdown tables
lower <- round(cor,2)
lower[lower.tri(cor, diag=TRUE)]<-""
lower <- as.data.frame(lower)
knitr::kable(lower, booktabs = TRUE,
             caption = 'Coorelations among attributes') %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive", font_size = 8))

### PCA Analysis
# Calculate Eigenvalues of the attributes
data.pca <- prcomp(RE_data[1:891,c(-1, -2)], center = TRUE, scale = TRUE)
summary(data.pca)

library(AMR)
#AMR::ggplot_pca(data.pca)
ggplot_pca(data.pca) #default shows PC1 and PC2, the two most imprtance attibutes
#biplot(data.pca)

#plot other components for example PC3 and PC4
ggplot_pca(data.pca, ellipse=TRUE, choices=c(3,4))

################################################
### Decision tree models
################################################
#load rpart the library which support decision tree
library(rpart)
# Build our first model with Rpart, only use Sex attribute
# load our re-engineered data set and separate train and test datasets
RE_data <- read.csv("RE_data.csv", header = TRUE)
train <- RE_data[1:891, ]
test <- RE_data[892:1309, ]

#build a decision tree model use rpart.
model1 <- rpart(Survived ~ Sex, data = train, method="class")

### Assess model's performance
#library caret is a comprehensive library support all sorts of model analysis

library(caret)
options(digits=4) # set decimal points of numbers.
# assess the model's accuracy by make a prediction on the train data.
Predict_model1_train <- predict(model1, train, type = "class")
#build a confusion matrix to make comparison
conMat <- confusionMatrix(as.factor(Predict_model1_train), as.factor(train$Survived))
#show confusion matrix
conMat$table
#show percentage of same values - accuracy
predict_train_accuracy <- conMat$overall["Accuracy"]
predict_train_accuracy

# The firs prediction by the first decision tree
Prediction1 <- predict(model1, test, type = "class")

# produce a submit with Kaggle required format that is only two attributes: PassengerId and Survived
submit1 <- data.frame(PassengerId = test$PassengerId, Survived = Prediction1)
# Write it into a file "Tree_Model1.CSV"
write.csv(submit1, file = "Tree_Model1.CSV", row.names = FALSE)

##Submit to kaggle and get a score: 0.76555

# Inspect prediction
summary(submit1$Survived)
prop.table(table(submit1$Survived, dnn="Test survive percentage"))
#train survive ratio
prop.table(table(as.factor(train$Survived), dnn="Train survive percentage"))

##
## The result shows that among total of 418 passenger in the test dataset,
## 266 passenger predicted perished (with survived value 0), which counts
## as 64% and 152 passenger predicted to be survived (with survived value 1)
## and which count as 36%. This is not too far from the radio on the
## training dataset, which was 62% survived and 38% perished.

# add Sex back to the submit and form a new data frame called compare
compare <- data.frame(submit1[1], Sex = test$Sex, submit1[2])
# Check train sex and Survived ratios
prop.table(table(train$Sex, train$Survived), margin = 1)
# Check predicted sex radio
prop.table(table(compare$Sex, dnn="Gender ratio in Test"))
#check predicted Survive and Sex radio
prop.table(table(compare$Sex, compare$Survived), margin = 1)

## It is clear that our model is too simple: it predicts any male
## will be perished and every female will be survived! This is approved
## by the gender (male and female) ratio in the test dataset is identical
## to the death ratio in our prediction result.

#plot our decision tree
# load some useful libraries
library(rattle)
library(rpart.plot)
library(RColorBrewer)
#
prp(model1)
fancyRpartPlot(model1)

### Tree Model2 with 5 Core Predictors
# A tree model with the top five attributes
set.seed(1234)
model2 <- rpart(Survived ~ Sex + Pclass + HasCabinNum + Deck + Fare_pp, data = train, method="class")

# Assess model's accuracy with train data
Predict_model2_train <- predict(model2, train, type = "class")
conMat <- confusionMatrix(as.factor(Predict_model2_train), as.factor(train$Survived))
conMat$table
#conMat$overall
predict2_train_accuracy <- conMat$overall["Accuracy"]
predict2_train_accuracy

# make a prediction and submit to Kaggle
Prediction2 <- predict(model2, test, type = "class")
submit2 <- data.frame(PassengerId = test$PassengerId, Survived = Prediction2)
write.csv(submit2, file = "Tree_model2.CSV", row.names = FALSE)

### kaggle score: **0.76555**
# plot our full house classifier
prp(model2, type = 0, extra = 1, under = TRUE)
# plot our full house classifier
fancyRpartPlot(model2)

# build a comparison data frame to record each prediction results
Tree_compare <- data.frame(test$PassengerId, predict1=Prediction1, predict2=Prediction2)
# Find differences
dif <- Tree_compare[Tree_compare[2]!=Tree_compare[3], ]
#show dif
dif

### Tree model3 construction using more predictors
model3 <- rpart(Survived ~ Sex + Fare_pp + Pclass + Title + Age_group + Group_size + Ticket_class  + Embarked,
                data=train,
                method="class")
# This model will be used in later chapters so save it in to a file for later to be loaded into memory
save(model3, file = "model3.rda")

#Assess prediction accuracy on train data
Predict_model3_train <- predict(model3, train, type = "class")
conMat <- confusionMatrix(as.factor(Predict_model3_train), as.factor(train$Survived))
conMat$table

#conMat$overall
predict3_train_accuracy <- conMat$overall["Accuracy"]
predict3_train_accuracy

# make prediction and submission, Score: 0.77033
Prediction3 <- predict(model3, test, type = "class")
submit3<- data.frame(PassengerId = test$PassengerId, Survived = Prediction3)
write.csv(submit3, file = "tree_model3.CSV", row.names = FALSE)

# plot our full house classifier
prp(model3, type = 0, extra = 1, under = TRUE)
# plot our full house classifier
fancyRpartPlot(model3)

# build a comparison data frame to record each prediction results
compare <- data.frame(test$PassengerId, predict2 = Prediction2 , predict3 = Prediction3)
# Find differences
dif <- compare[compare[2] != compare[3], ]
#show dif
print.data.frame(dif, row.names = FALSE)

### Tree model4, full-house classifier apart from name and ticket
model4 <- rpart(Survived ~ Sex + Pclass + Age + SibSp + Parch + Embarked + HasCabinNum + Friend_size + Fare_pp + Title + Deck + Ticket_class + Family_size + Group_size + Age_group,
                #model4 <- rpart(Survived ~ .,
                data=train,
                method="class")
#assess prediction accuracy on train data
Predict_model4_train <- predict(model4, train, type = "class")
conMat <- confusionMatrix(as.factor(Predict_model4_train), as.factor(train$Survived))
conMat$table
#conMat$overall
predict4_train_accuracy <- conMat$overall["Accuracy"]
predict4_train_accuracy

# make prediction and submission. Kaggle score: 0.75119
Prediction4 <- predict(model4, test, type = "class")
submit4 <- data.frame(PassengerId = test$PassengerId, Survived = Prediction4)
write.csv(submit4, file = "Tree_model4.CSV", row.names = FALSE)

# plot our full house classifier
prp(model4, type = 0, extra = 1, under = TRUE)
fancyRpartPlot(model4)

# build a comparison data frame to record each prediction results
compare <- data.frame(test$PassengerId, predict3 = Prediction3 , predict4 = Prediction4)
# Find differences
dif2 <- compare[compare[2] != compare[3], ]
#show dif
dif2

library(tidyr)
# Tree models comparison
Model <- c("Model1","Model2","Model3","Model4")
Pre <- c("Sex", "Sex, Pclass, HasCabinNum, Deck, Fare_pp", "Sex, Fare_pp, Pclass, Title, Age_group, Group_size, Ticket_class, Embarked", "All")
Train <- c(78.68, 81.48, 85.19, 85.41)
Test <- c(76.56, 76.56, 77.03, 75.12)
df1 <- data.frame(Model, Pre, Train, Test)
df2 <- data.frame(Model, Train, Test)
knitr::kable(df1, longtable = TRUE, booktabs = TRUE, digits = 2, col.names =c("Models", "Predictors", "Accuracy on Train", "Accuracy on Test"),
             caption = 'The Comparision among 4 decision tree models'
)
# bar plot of comparison
df.long <- gather(df2, Dataset, Accuracy, -Model, factor_key =TRUE)
ggplot(data = df.long, aes(x = Model, y = Accuracy, fill = Dataset)) +
  geom_col(position = position_dodge())

##############################################
#  Random Forest models
##############################################
# Install the random forest library, if you have not
# install.packages('randomForest')
# load library
library(randomForest)
library(plyr)
# library(caret)

# load data if you have not
RE_data <- read.csv("RE_data.csv", header = TRUE)

# RE_data <- mutate_if(RE_data, is.numeric, as.factor)
#
train <- RE_data[1:891, ]
test <- RE_data[892:1309, ]

# convert variables into factor because RF can be Classification and Regression
train$Survived <- as.factor(train$Survived)
# convert other attributes which really are categorical data but in form of numbers
train$Pclass <- as.factor(train$Pclass)
train$Group_size <- as.factor(train$Group_size)
#confirm types
sapply(train, class)

# Build the random forest model1 uses Pclass, Sex, HasCabinNum, Deck and Fare_pp
set.seed(1234) #for reproduction
RF_model1 <- randomForest(Survived ~ Sex + Pclass + HasCabinNum
                          + Deck + Fare_pp,
                          data=train, importance=TRUE)

# Make your prediction using the validate dataset
RF_prediction1 <- predict(RF_model1, train)

#check up
conMat<- confusionMatrix(RF_prediction1, train$Survived)
conMat$table

# Misclassification error
paste('Accuracy =', round(conMat$overall["Accuracy"],2))
paste('Error =', round(mean(train$Survived != RF_prediction1), 2))

# produce a submit with Kaggle required format that is only two attributes: PassengerId and Survived
test$Pclass <- as.factor(test$Pclass)
test$Group_size <- as.factor(test$Group_size)

# make prediction, Kaggle score is: 0.76555
RF_prediction <- predict(RF_model1, test)
submit <- data.frame(PassengerId = test$PassengerId, Survived = RF_prediction)
# Write it into a file "RF_Result.CSV"
write.csv(submit, file = "RF_Result1.CSV", row.names = FALSE)

# Record the results
RF_model1_accuracy <- c(80, 84, 76.555)

### RE_model2 with more predictors
set.seed(2222)
RF_model2 <- randomForest(as.factor(Survived) ~ Sex + Fare_pp + Pclass + Title + Age_group + Group_size + Ticket_class  + Embarked,
                          data = train,
                          importance=TRUE)
# This model will be used in later chapters, so save it in a file and it can be loaded later.
save(RF_model2, file = "RF_model2.rda")

# RF_model2 Prediction
RF_prediction2 <- predict(RF_model2, train)
#check up
conMat<- confusionMatrix(RF_prediction2, train$Survived)
conMat$table
# Misclassification error
paste('Accuracy =', round(conMat$overall["Accuracy"],2))
paste('Error =', round(mean(train$Survived != RF_prediction2), 2))
# produce a submission and submit to Kaggle
test$Pclass <- as.factor(test$Pclass)
test$Group_size <- as.factor(test$Group_size)

# make prediction on test and submit. Score: 0.78947
RF_prediction <- predict(RF_model2, test)
submit <- data.frame(PassengerId = test$PassengerId, Survived = RF_prediction)
# Write it into a file "RF_Result.CSV"
write.csv(submit, file = "RF_Result2.CSV", row.names = FALSE)

# Record RF_model2's results
RF_model2_accuracy <- c(83.16, 92, 78.95)

### RF_model3 construction with the maximum predictors
set.seed(2233)
RF_model3 <- randomForest(Survived ~ Sex + Pclass + Age +
                            SibSp + Parch + Embarked +
                            HasCabinNum + Friend_size +
                            Fare_pp + Title + Deck +
                            Ticket_class + Family_size
                          + Group_size + Age_group,
    data = train, importance=TRUE)

# Display RE_model3's details
RF_model3
# Make a prediction on Train
RF_prediction3 <- predict(RF_model3, train)
#check up
conMat<- confusionMatrix(RF_prediction3, train$Survived)
conMat$table
# Misclassification error
paste('Accuracy =', round(conMat$overall["Accuracy"],2))
paste('Error =', round(mean(train$Survived != RF_prediction3), 2))

# produce a submit with Kaggle
test$Pclass <- as.factor(test$Pclass)
test$Group_size <- as.factor(test$Group_size)

# make prediction, Kaggle score: 0.77033
RF_prediction <- predict(RF_model3, test)
submit <- data.frame(PassengerId = test$PassengerId, Survived = RF_prediction)
# Write it into a file "RF_Result.CSV"
write.csv(submit, file = "RF_Result3.CSV", row.names = FALSE)

# Record RF_model3's results
RF_model3_accuracy <- c(83, 94, 77)


### RF_models Comparison
library(tidyr)
Model <- c("RF_Model1","RF_Model2","RF_Model3")
Pre <- c("Sex, Pclass, HasCabinNum, Deck, Fare_pp", "Sex, Fare_pp, Pclass, Title, Age_group, Group_size, Ticket_class, Embarked", "Sex, Pclass, Age, SibSp, Parch, Embarked, HasCabinNum, Friend_size, Fare_pp, Title, Deck, Ticket_class, Family_size, Group_size, Age_group")

Learn <- c(80.0, 83.16, 83.0)
Train <- c(84, 92, 78)
Test <- c(76.555, 78.95, 77.03)
df1 <- data.frame(Model, Pre, Learn, Train, Test)
df2 <- data.frame(Model, Learn, Train, Test)
knitr::kable(df1, longtable = TRUE, booktabs = TRUE, digits = 2, col.names =c("Models", "Predictors", "Accuracy on Learn", "Accuracy on Train", "Accuracy on Test"),
             caption = 'The Comparision among 3 Random Forest models'
)
# Plot bar comparison
df.long <- gather(df2, Dataset, Accuracy, -Model, factor_key =TRUE)
ggplot(data = df.long, aes(x = Model, y = Accuracy, fill = Dataset)) +
  geom_col(position = position_dodge())

### End 3 Model Construction ################################################
\end{verbatim}

\hypertarget{titanicdataanalysis4_model_cross_validation.r}{%
\section*{TitanicDataAnalysis4\_Model\_Cross\_Validation.R}\label{titanicdataanalysis4_model_cross_validation.r}}


\begin{verbatim}
####################################################
# Model Cross Validation and Fine Tune
####################################################
# This file contains Cross Validation
#  1. CV-tree models
#  2. CV-random forest models
#
##### Predictor selection

library(caret)
library(rpart)
library(rpart.plot)

#read Re-engineered dataset
RE_data <- read.csv("RE_data.csv", header = TRUE)

#Factorize response variable
RE_data$Survived <- factor(RE_data$Survived)

#Separate Train and test data.
train <- RE_data[1:891, ]
test <- RE_data[892:1309, ]

# Setup model's train and valid dataset
set.seed(1000)
samp <- sample(nrow(train), 0.8 * nrow(train))
trainData <- train[samp, ]
validData <- train[-samp, ]

# set random for reproduction
set.seed(3214)
# specify parameters for cross validation
control <- trainControl(method = "repeatedcv",
                        number = 10, # number of folds
                        repeats = 5, # repeat times
                        search = "grid")

###############################################
# CV on Tree models
###############################################

### CV on tree model2
set.seed(1010)
# Create model from cross validation train data
# Tree_model2_cv <- train(Survived ~ Sex + Pclass + HasCabinNum + Deck + Fare_pp,
#                         data = trainData,
#                         method = "rpart",
#                         trControl = control)

# Due to the computation cost once a model is trained.
# it is better to save it and load later rather than compute a gain
#save(Tree_model2_cv, file = "Tree_model2_cv.rda")
load("Tree_model2_cv.rda")

# Show CV model's details
print.train(Tree_model2_cv)
# CV model's estimated accuracy
model_accuracy <- Tree_model2_cv$results$Accuracy[1]
paste("Estimated accuracy:", format(model_accuracy, digits = 4))

# Visualize cross validation tree
rpart.plot(Tree_model2_cv$finalModel, extra=4)
plot.train(Tree_model2_cv)

# Record the model's accuracy on *trainData*, *validData*,
# and *test* dataset. Remember *trainData* and *validData*
# are randomly partitioned from the train dataset.
### Access accuracy on different datasets

# prediction's Confusion Matrix on the trainData
predict_train <-predict(Tree_model2_cv, trainData)
conMat <- confusionMatrix(predict_train, trainData$Survived)
conMat$table
# prediction's Accuracy on the trainData
predict_train_accuracy <- conMat$overall["Accuracy"]
predict_train_accuracy

# prediction's Confusion Matrix on the validData
predict_valid <-predict(Tree_model2_cv, validData)
conMat <- confusionMatrix(predict_valid, validData$Survived)
conMat$table
# prediction's Accuracy on the validData
predict_valid_accuracy <- conMat$overall["Accuracy"]
predict_valid_accuracy

# predict on test
predict_test <-predict(Tree_model2_cv, test)
submit <- data.frame(PassengerId = test$PassengerId, Survived = as.factor(predict_test))
write.csv(submit, file = "Tree_model2_CV.CSV", row.names = FALSE)
# test accuracy 0.75837
paste("Test Accuracy:", 0.7584)

# accumulate model's accuracy
name <- c("Esti Accu", "Train Accu", "Valid Accu", "Test Accu")
Tree_model2_CV_accuracy <- c(model_accuracy, predict_train_accuracy, predict_valid_accuracy, 0.7584)
names(Tree_model2_CV_accuracy) <- name
Tree_model2_CV_accuracy

### CV on model3
#
set.seed(1234)
# Tree_model3_cv <- train(Survived ~ Sex + Fare_pp + Pclass + Title + Age_group + Group_size + Ticket_class  + Embarked,
#
#                        data = trainData,
#                        method = "rpart",
#                        trControl = control)
#
# save(Tree_model3_cv, file = "Tree_model3_cv.rda")
load("Tree_model3_cv.rda")
# show model details
print.train(Tree_model3_cv)
# CV model's estimated accuracy
model_accuracy <- Tree_model3_cv$results$Accuracy[1]
paste("Estimated accuracy:", format(model_accuracy, digits = 4))

#Visualize cross validation tree
rpart.plot(Tree_model3_cv$finalModel, extra=4)
plot.train(Tree_model3_cv)

### Access accuracy on different datasets
# prediction's Confusion Matrix on the trainData
predict_train <-predict(Tree_model3_cv, trainData)
conMat <- confusionMatrix(predict_train, trainData$Survived)
conMat$table
# prediction's Accuracy on the trainData
predict_train_accuracy <- format(conMat$overall["Accuracy"], digits=4)
paste("trainData Accuracy:", predict_train_accuracy)

# prediction's Confusion Matrix on the validData
predict_valid <-predict(Tree_model3_cv, validData)
conMat <- confusionMatrix(predict_valid, validData$Survived)
conMat$table
# prediction's Accuracy on the validData
predict_valid_accuracy <- format(conMat$overall["Accuracy"], digits=4)
paste("validData Accuracy:", predict_valid_accuracy)

#predict on test
predict_test <-predict(Tree_model3_cv, test)
submit <- data.frame(PassengerId = test$PassengerId, Survived = as.factor(predict_test))
write.csv(submit, file = "Tree_model3_CV.CSV", row.names = FALSE)

## test accuracy is 0.77751
paste("Test Accuracy:", 0.7775)

# accumulate model's accuracy
Tree_model3_CV_accuracy <- c(model_accuracy, predict_train_accuracy, predict_valid_accuracy, 0.7775)
names(Tree_model3_CV_accuracy) <- name
Tree_model3_CV_accuracy

##############################################
### Cross Validation on Random Forest Models
##############################################

# Random Forest model RF_model1_cv
# set seed for reproduction
set.seed(2307)
# RF_model1_cv <- train(Survived ~ Sex + Pclass + HasCabinNum +      Deck + Fare_pp,
#                        data = trainData,
#                        method = "rf",
#                        trControl = control)
# save(RF_model1_cv, file = "RF_model1_cv.rda")
load("RF_model1_cv.rda")

# Show CV mdoel's details
print(RF_model1_cv)
print(RF_model1_cv$results)

# Record model's accuracy
model_accuracy <- format(RF_model1_cv$results$Accuracy[2], digits = 4)
paste("Estimated accuracy:", model_accuracy)

### Access accuracy on different datasets
# prediction's Confusion Matrix on the trainData
predict_train <-predict(RF_model1_cv, trainData)
conMat <- confusionMatrix(predict_train, trainData$Survived)
conMat$table
# prediction's Accuracy on the trainData
predict_train_accuracy <- format(conMat$overall["Accuracy"], digits=4)
paste("trainData Accuracy:", predict_train_accuracy)

# prediction's Confusion Matrix on the validData
predict_valid <-predict(RF_model1_cv, validData)
conMat <- confusionMatrix(predict_valid, validData$Survived)
conMat$table
# prediction's Accuracy on the validData
predict_valid_accuracy <- format(conMat$overall["Accuracy"], digits=4)
paste("validData Accuracy:", predict_valid_accuracy)

# predict on test
predict_test <-predict(RF_model1_cv, test)
submit <- data.frame(PassengerId = test$PassengerId, Survived = as.factor(predict_test))
write.csv(submit, file = "RF_model1_CV.CSV", row.names = FALSE)

## test accuracy 0.74641
paste("Test Accuracy:", 0.7464)

# accumulate model's accuracy
RF_model1_cv_accuracy <- c(model_accuracy, predict_train_accuracy, predict_valid_accuracy, 0.7464)
names(RF_model1_cv_accuracy) <- name
RF_model1_cv_accuracy

###
# Random Forest model RF_model2_cv
##
# set seed for reproduction
set.seed(2300)

# RF_model2_cv <- train(Survived ~ Sex + Fare_pp + Pclass + Title + Age_group + Group_size + Ticket_class + Embarked,
#                        data = trainData,
#                        method = "rf",
#                        trControl = control)
# # This model will be used in chapter 12. so it is saved into a file for late to be loaded
# save(RF_model2_cv, file = "RF_model2_cv.rda")

load("RF_model2_cv.rda")

# Show CV mdoel's details
print(RF_model2_cv)
print(RF_model2_cv$results)

# Record model's accuracy
mode2_accuracy <- format(RF_model2_cv$results$Accuracy[2], digits = 4)
paste("Estimated accuracy:", mode2_accuracy)

### Access accuracy on different datasets
# prediction's Confusion Matrix on the trainData
predict_train <-predict(RF_model2_cv, trainData)
conMat <- confusionMatrix(predict_train, trainData$Survived)
conMat$table

# prediction's Accuracy on the trainData
predict_train_accuracy <- format(conMat$overall["Accuracy"], digits=4)
paste("trainData Accuracy:", predict_train_accuracy)

# prediction's Confusion Matrix on the validData
predict_valid <-predict(RF_model2_cv, validData)
conMat <- confusionMatrix(predict_valid, validData$Survived)
conMat$table
# prediction's Accuracy on the validData
predict_valid_accuracy <- format(conMat$overall["Accuracy"], digits=4)
paste("validData Accuracy:", predict_valid_accuracy)

#predict on test
predict_test <-predict(RF_model2_cv, test)
submit <- data.frame(PassengerId = test$PassengerId, Survived = as.factor(predict_test))
write.csv(submit, file = "RF_model2_CV.CSV", row.names = FALSE)

## test accuracy 0.75119
paste("Test Accuracy:", 0.7512)

# accumulate model's accuracy
RF_model2_cv_accuracy <- c(mode2_accuracy, predict_train_accuracy, predict_valid_accuracy, 0.7512)
names(RF_model2_cv_accuracy) <- name
RF_model2_cv_accuracy

### make a comparison among tree and random forest models
library(tidyr)
Model <- c("Tree_M2","Tree_M3","RF_model1","RF_model2")

# Show individual models' accuracy
Tree_model2_CV_accuracy
Tree_model3_CV_accuracy
RF_model1_cv_accuracy
RF_model2_cv_accuracy

#preparee for table construction
Pre <- c("Sex, Pclass, HasCabinNum, Deck, Fare_pp", "Sex, Fare_pp, Pclass, Title, Age_group, Group_size, Ticket_class, Embarked", "Sex, Pclass, HasCabinNum, Deck, Fare_pp", "Sex, Fare_pp, Pclass, Title, Age_group, Group_size, Ticket_class, Embarked")
#
Learn <- c(as.numeric(Tree_model2_CV_accuracy[1])*100, as.numeric(Tree_model3_CV_accuracy[1])*100, as.numeric(RF_model1_cv_accuracy[1])*100, as.numeric(RF_model2_cv_accuracy[1])*100)
#
Train <- c(as.numeric(Tree_model2_CV_accuracy[2])*100, as.numeric(Tree_model3_CV_accuracy[2])*100, as.numeric(RF_model1_cv_accuracy[2])*100, as.numeric(RF_model2_cv_accuracy[2])*100)
#
Valid <- c(as.numeric(Tree_model2_CV_accuracy[3])*100, as.numeric(Tree_model3_CV_accuracy[3])*100, as.numeric(RF_model1_cv_accuracy[3])*100, as.numeric(RF_model2_cv_accuracy[3])*100)
#
Test <- c(as.numeric(Tree_model2_CV_accuracy[4])*100, as.numeric(Tree_model3_CV_accuracy[4])*100, as.numeric(RF_model1_cv_accuracy[4])*100, as.numeric(RF_model2_cv_accuracy[4])*100)

# Construct Dataframe for table and plot
df1 <- data.frame(Model, Pre, Learn, Train, Valid, Test)
df2 <- data.frame(Model, Learn, Train, Valid, Test)

# show in table
knitr::kable(df1, longtable = TRUE, booktabs = TRUE, digits = 2, col.names =c("Models", "Predictors", "Accuracy on Learn", "Accuracy on Train", "Accuracy on Valid",  "Accuracy on Test"),
             caption = 'The Comparision among 4 CV models'
)

# plot results in bar chat
df.long <- gather(df2, Dataset, Accuracy, -Model, factor_key =TRUE)
ggplot(data = df.long, aes(x = Model, y = Accuracy, fill = Dataset)) +
  geom_col(position = position_dodge())

#############################################
## Multiple Models Comparison
#############################################
#
### Regression Model for Titanic
LR_Model <- glm(formula = Survived ~ Pclass + Title + Sex + Age_group + Group_size + Ticket_class  + Fare_pp + Embarked, family = binomial, data = trainData)

#summary(LR_Model_CV)
### Validate on trainData
Valid_trainData <- predict(LR_Model, newdata = trainData, type = "response") #prediction threshold
Valid_trainData <- ifelse(Valid_trainData > 0.5, 1, 0)  # set binary
#produce confusion matrix
confusion_Mat<- confusionMatrix(as.factor(trainData$Survived),as.factor(Valid_trainData))

# accuracy on traindata
Regression_Acc_Train <- round(confusion_Mat$overall["Accuracy"]*100,2)
paste('Model Train Accuracy =', Regression_Acc_Train)

### Validate on validData
validData_Survived_predicted <- predict(LR_Model, newdata = validData, type = "response")
validData_Survived_predicted  <- ifelse(validData_Survived_predicted  > 0.5, 1, 0)  # set binary prediction threshold
conMat<- confusionMatrix(as.factor(validData$Survived),as.factor(validData_Survived_predicted))

Regression_Acc_Valid <-round(conMat$overall["Accuracy"]*100,2)
paste('Model Valid Accuracy =', Regression_Acc_Valid)

### produce a prediction on test data
library(pROC)
auc(roc(trainData$Survived,Valid_trainData))  # calculate AUROC curve
#predict on test
test$Survived <- predict(LR_Model, newdata = test, type = "response")
test$Survived <- ifelse(test$Survived > 0.5, 1, 0)  # set binary prediction threshold
submit <- data.frame(PassengerId = test$PassengerId, Survived = as.factor(test$Survived))

write.csv(submit, file = "LG_model1_CV.CSV", row.names = FALSE)
# Kaggle test accuracy score:0.76555

# record accuracy
Regr_Acc <- c(Regression_Acc_Train, Regression_Acc_Valid, 0.76555)

acc_names <- c("Train Accu", "Valid Accu", "Test Accu")
names(Regr_Acc) <- acc_names
Regr_Acc

### Support Vector Machine Model for Titanic
#load library
library(e1071)

# fit the model using default parameters
SVM_model<- svm(Survived ~ Pclass + Title + Sex + Age_group + Group_size + Ticket_class + Fare_pp + Deck + HasCabinNum + Embarked, data=trainData, kernel = 'radial', type="C-classification")

#summary(SVM_model)
### Validate on trainData
Valid_trainData <- predict(SVM_model, trainData)
#produce confusion matrix
confusion_Mat<- confusionMatrix(as.factor(trainData$Survived), as.factor(Valid_trainData))

# output accuracy
AVM_Acc_Train <- round(confusion_Mat$overall["Accuracy"]*100,4)
paste('Model Train Accuracy =', AVM_Acc_Train)

### Validate on validData
validData_Survived_predicted <- predict(SVM_model, validData) #produce confusion matrix
conMat<- confusionMatrix(as.factor(validData$Survived), as.factor(validData_Survived_predicted))
# output accuracy
AVM_Acc_Valid <- round(conMat$overall["Accuracy"]*100,4)
paste('Model Valid Accuracy =', AVM_Acc_Valid)

### make prediction on test
# SVM failed to produce a prediction on test because test has Survived col and it has value NA. A work around is assign it with a num like 1.
test$Survived <-1

# predict results on test
Survived <- predict(SVM_model, test)
solution <- data.frame(PassengerId=test$PassengerId, Survived =Survived)
write.csv(solution, file = 'svm_predicton.csv', row.names = F)

# prediction accuracy on test
SVM_Acc <- c(AVM_Acc_Train, AVM_Acc_Valid, 0.78947)
names(SVM_Acc) <- acc_names

# print out
SVM_Acc

### Neural Network Models
# load library
library(nnet)

# train the model
xTrain = train[ , c("Survived", "Pclass","Title", "Sex","Age_group","Group_size", "Ticket_class", "Fare_pp", "Deck", "HasCabinNum", "Embarked")]

NN_model1 <- nnet(Survived ~ ., data = xTrain, size=10, maxit=500, trace=FALSE)

#How do we do on the training data?
nn_pred_train_class = predict(NN_model1, xTrain, type="class" )  # yields "0", "1"
nn_train_pred = as.numeric(nn_pred_train_class ) #transform to 0, 1
confusion_Mat<-confusionMatrix(as.factor(nn_train_pred), train$Survived)
# output accuracy
NN_Acc_Train <- round(confusion_Mat$overall["Accuracy"]*100,4)
paste('Model Train Accuracy =', NN_Acc_Train)

#How do we do on the valid data?
nn_pred_valid_class = predict(NN_model1, validData, type="class" )  # yields "0", "1"
nn_valid_pred = as.numeric(nn_pred_valid_class ) #transform to 0, 1
confusion_Mat<-confusionMatrix(as.factor(nn_valid_pred), validData$Survived)
# output accuracy
NN_Acc_Valid <- round(confusion_Mat$overall["Accuracy"]*100,4)
paste('Model valid Accuracy =', NN_Acc_Valid)

#make a prediction on test data
nn_pred_test_class = predict(NN_model1, test, type="class" )  # yields "0", "1"
nn_pred_test = as.numeric(nn_pred_test_class ) #transform to 0, 1
solution <- data.frame(PassengerId=test$PassengerId, Survived = nn_pred_test)
write.csv(solution, file = 'NN_predicton.csv', row.names = F)

###
# 0.8934,0.8547, 0.71052
NN_Acc <- c(NN_Acc_Train, NN_Acc_Valid, 0.71052)
names(NN_Acc) <- acc_names
NN_Acc

### Comparision among Different Models

library(tidyr)
Model <- c("Regression","SVM","NN", "Decision tree", "Random Forest")
Train <- c(Regression_Acc_Train, AVM_Acc_Train, NN_Acc_Train, 82.72, 83.16)
Valid <- c(Regression_Acc_Valid, AVM_Acc_Valid, NN_Acc_Valid, 81.01, 92)
Test <- c(76.56, 78.95, 71.05, 77.75, 78.95)
df1 <- data.frame(Model, Train, Valid, Test)

# show in table
knitr::kable(df1, longtable = TRUE, booktabs = TRUE, digits = 2, col.names =c("Models", "Accuracy on Train", "Accuracy on Valid","Accuracy on Test"),
             caption = 'The Comparision among 3 Machine Learning Models'
)

# plot in bar chat
df.long <- gather(df1, Dataset, Accuracy, -Model, factor_key =TRUE)
ggplot(data = df.long, aes(x = Model, y = Accuracy, fill = Dataset)) +
  geom_col(position = position_dodge())
### END 4 Model Cross Validation #############################################
\end{verbatim}

\hypertarget{titanicdataanalysis5_model_fine_tune.r}{%
\section*{TitanicDataAnalysis5\_Model\_Fine\_Tune.R}\label{titanicdataanalysis5_model_fine_tune.r}}


\begin{verbatim}
###########################################################################
# Model Fine Tune
###########################################################################
#
###############################
### Tuning a model's Predictor
###############################
# load necessary library
library(randomForest)
library(plyr)
library(caret)

# load our re-engineered data set and separate train and test datasets
RE_data <- read.csv("RE_data.csv", header = TRUE)
train <- RE_data[1:891, ]
test <- RE_data[892:1309, ]

# Train a Random Forest with the default parameters using full attributes
# Survived is our response variable and the rest can be predictors except pasengerID.

rf.train <- subset(train, select = -c(PassengerId, Survived))

# we set rf.label for later use but it is the dependednt variable
rf.label <- as.factor(train$Survived)

# RandomForest cannot handle factors with over 53 levels
rf.train$Ticket <- as.numeric(train$Ticket)

set.seed(1234) # for reproduction
rf.1 <- randomForest(x = rf.train, y = rf.label, importance = TRUE)
rf.1

#rf.1 model with full house predictors has error rate: 15.49%

# Check the order of the predictors prediction power.
pre.or <- sort(rf.1$importance[,3], decreasing = TRUE)
pre.or
varImpPlot(rf.1)

## The idea in here is using the predictors order (prediction)
# to build all models each has one predictor less. so we can
# compare models to find which is has the best accuracy
# WE can take that model as the best to bench marking the predictor.

# rf.2 as an example
rf.train.2 <- subset(rf.train, select = -c(Parch))
set.seed(1234)
rf.2 <- randomForest(x = rf.train.2, y = rf.label, importance = TRUE)
rf.2
#The *rf.2*  model's accuracy 84.85%,  error` (15.15%).

### we can repeat the process to get all the models and their accuracy
# List themin a table
library(tidyr)

Model <- c("rf.1","rf.2","rf.3","rf.4","rf.5","rf.6","rf.7","rf.8","rf.9","rf.10","rf.11","rf.12","rf.13","rf.14","rf.15","rf.16")
Pre <- c("Sex", "Title", "Fare_pp", "Ticket_class", "Pclass", "Ticket", "Age", "Friend_size", "Deck", "Age_group", "Group_size", "Family_size", "HasCabinNum", "SibSp", "Embarked", "Parch")
#Produce models predictor list
Pred <- rnorm(16)
tem <- NULL
for (i in 1:length(Pre)) {
  tem  <- paste(tem, Pre[i], sep = " ")
  #Using environment variable setting
  ls  <- paste("Pred[",i,"]", sep="")
  eq  <- paste(paste(ls, "tem", sep="<-"), collapse=";")
  eval(parse(text=eq))
}
Pred <- sort(Pred, decreasing = TRUE)

Error <- c(15.49, 15.15, 14.93, 15.26, 14.7, 14.7, 14.03, 13.58, 14.48, 15.6, 16.27, 16.95, 17.51, 20.31, 20.76, 21.32)
Accuracy <- 100 - Error

df <- data.frame(Model, Pred, Accuracy)

knitr::kable(df, longtable = TRUE, booktabs = TRUE, digits = 2, col.names =c("Models", "Predictors", "Accuracy"),
             caption = 'Model Predictors Comparision'
)

# load the best model and record its predictors
# save(rf.8, file = "rf_model.rda")
load("rf_model.rda")

Predictor <- c("Sex, Title, Fare_pp, Ticket_class, Pclass, Ticket, Age, Friend_size, Deck")
Predictor

#################################
### Tuning Training Data Samples
#################################
# Set Prediction Accuracy Benchmark

# Let's start with a submission of rf.8 to Kaggle
# to find the difference between model's OOB and the accuracy

# Subset our test records and features
test.submit.df <- test[, c("Sex", "Title", "Fare_pp", "Ticket_class", "Pclass", "Ticket", "Age", "Friend_size", "Deck")]
test.submit.df$Ticket <- as.numeric(test.submit.df$Ticket)

# Make predictions
rf.8.preds <- predict(rf.8, test.submit.df)
table(rf.8.preds)

# Write out a CSV file for submission to Kaggle
submit.df <- data.frame(PassengerId = test$PassengerId, Survived = rf.8.preds)

write.csv(submit.df, file = "RF8_SUB.csv", row.names = FALSE)
# After our submission we have scores 0.75598 from Kaggle,
# but the OOB predicts that we should score 0.8642.

#
### The 1 sampling methods: 10 Folds CV Repeat 10 Times
#
# check to see if the samples are the same or close to the same ratio
library(caret)
library(doSNOW)

set.seed(2348)
# rf.label is the Survived in the train dataset.
# ? createMultiFolds to find out more. train (891)
cv.10.folds <- createMultiFolds(rf.label, k = 10, times = 10)

# Check stratification: survived ratio in the train dataset
table(rf.label)
342 / 549

# check 10-folds random split each folds ratio (34 is an example)
table(rf.label[cv.10.folds[[34]]])
308 / 494
#confirmed the stratification both have the similer ratio

# Set up caret's trainControl object using 10-folds repeated CV
ctrl.1 <- trainControl(method = "repeatedcv", number = 10, repeats = 10, index = cv.10.folds)

# Model construction with "`10-folds repeated CV`" is a very expensive
# R has a package called **"doSNOW"**, that facilities the use of
# multi-core processor and permits parallel computing in
# a pseudo cluster mode

## Set up doSNOW package for multi-core training.
# cl <- makeCluster(6, type = "SOCK")
# registerDoSNOW(cl)
# # Set seed for reproducibility and train
# set.seed(34324)
#
# rf.8.cv.1 <- train(x = rf.train.8, y = rf.label, method = "rf", tuneLength = 3, ntree = 500, trControl = ctrl.1)
#
# #Shutdown cluster
# stopCluster(cl)
# save(rf.8.cv.1, file = "rf.8.cv.1.rda")
# Check out results
load("rf.8.cv.1.rda")
# Check out results
rf.8.cv.1

# prediction accuracy reduced from 0.8642 to 0.8511,
# but not pessimistic enough to the test accuracy, it is 0.75598.

#
### The 2 sampling methods: 5 Folds CV Repeat 10 Times
#
set.seed(5983)
# cv.5.folds <- createMultiFolds(rf.label, k = 5, times = 10)
#
# ctrl.2 <- trainControl(method = "repeatedcv", number = 5, repeats = 10, index = cv.5.folds)
#
# cl <- makeCluster(6, type = "SOCK")
# registerDoSNOW(cl)
#
# set.seed(89472)
# rf.8.cv.2 <- train(x = rf.train.8, y = rf.label, method = "rf", tuneLength = 3, ntree = 500, trControl = ctrl.2)
#
# #Shutdown cluster
# stopCluster(cl)
# save(rf.8.cv.2, file = "rf.8.cv.2.rda")
# # Check out results
load("rf.8.cv.2.rda")
# Check out results
rf.8.cv.2

# We can see that 5-fold CV is a little better.
# The accuracy is moved under 85%. The model's training data set
# is moved from 9/10 to 4/5, which is 713 now.

#
### The 3 sampling methods: 3 Folds CV Repeat 10 Times
#
set.seed(37596)
# cv.3.folds <- createMultiFolds(rf.label, k = 3, times = 10)
#
# ctrl.3 <- trainControl(method = "repeatedcv", number = 3, repeats = 10, index = cv.3.folds)
#
# cl <- makeCluster(6, type = "SOCK")
# registerDoSNOW(cl)
#
# set.seed(94622)
# rf.8.cv.3 <- train(x = rf.train.8, y = rf.label, method = "rf", tuneLength = 3, ntree = 500, trControl = ctrl.3)
#
# #Shutdown cluster
# stopCluster(cl)
#
# save(rf.8.cv.3, file = "rf.8.cv.3.rda")
# # # Check out results
load("rf.8.cv.3.rda")
# Check out results
rf.8.cv.3

# We can see the accuracy has further decreased (0.8387579).
# Let us also reduced the number of times that the samples
# are repeated used in the training (repeat times).
# Let us see if the sample repeat times reduce to 3,
# if the accuracy can be further reduced.

#
### The 4 sampling methods: 5 Folds CV Repeat 10 Times
#
# set.seed(396)
# cv.3.folds <- createMultiFolds(rf.label, k = 3, times = 3)
#
# ctrl.4 <- trainControl(method = "repeatedcv", number = 3, repeats = 3, index = cv.3.folds)
#
# cl <- makeCluster(6, type = "SOCK")
# registerDoSNOW(cl)
#
# set.seed(9622)
# rf.8.cv.4 <- train(x = rf.train.8, y = rf.label, method = "rf", tuneLength = 3, ntree = 50, trControl = ctrl.4)
#
# #Shutdown cluster
# stopCluster(cl)
#save(rf.8.cv.4, file = "rf.8.cv.4.rda")
# # # Check out results
load("rf.8.cv.4.rda")
# Check out results
rf.8.cv.4

#################################
### Tuning Model’s Parameters
#################################
# random  search to find mtry for RF

#library(caret)
#library(doSNOW)

### Random Search
set.seed(2222)
# #use teh best sampling results that is K=3 ant T=10
# cv.3.folds <- createMultiFolds(rf.label, k = 3, times = 10)
#
# # Set up caret's trainControl object.
# ctrl.1 <- trainControl(method = "repeatedcv", number = 3, repeats = 10, index = cv.3.folds, search="random")
#
# # set up cluster for parallel computing
# cl <- makeCluster(6, type = "SOCK")
# registerDoSNOW(cl)
#
# # Set seed for reproducibility and train
# set.seed(34324)
#
# #use rf.train.8 with 9 predictors
#
# #RF_Random <- train(x = rf.train.8, y = rf.label, method = "rf", tuneLength = 15, ntree = 500, trControl = ctrl.1)
# #save(RF_Random, file = "RF_Random_search.rda")
#
# #Shutdown cluster
# stopCluster(cl)

# Check out results
load("RF_Random_search.rda")
print(RF_Random)

# plot
plot(RF_Random)
### mtry =3 by random search

### Grid Search
# ctrl.2 <- trainControl(method="repeatedcv", number=3, repeats=10, index = cv.3.folds, search="grid")
#
# set.seed(3333)
# # set Grid search with a vector from 1 to 15.
#
# tunegrid <- expand.grid(.mtry=c(1:15))
#
# # set up cluster for parallel computing
# cl <- makeCluster(6, type = "SOCK")
# registerDoSNOW(cl)
#
#
# #RF_grid_search <- train(y = rf.label, x = rf.train.8,  method="rf", metric="Accuracy", trControl = ctrl.2, tuneGrid = tunegrid, tuneLength = 15, ntree = 500)
#
#
# #Shutdown cluster
# stopCluster(cl)
# #save(RF_grid_search, file = "RF_grid_search.rda")

load("RF_grid_search.rda")
#print results
print(RF_grid_search)
#plot
plot(RF_grid_search)

##
### Manual Search we use control 1 random search
##  use n_tree in c(100, 500, 1000, 1500)
model_list <- list()

tunegrid <- expand.grid(.mtry = 3)
control <- trainControl(method="repeatedcv", number=3, repeats=10, search="grid")

# # the following code have been commented out just for produce the markdown file. so it will not wait for ran a long time
# # set up cluster for parallel computing
# cl <- makeCluster(6, type = "SOCK")
# registerDoSNOW(cl)
#
#
# #loop through different settings
#
# for (n_tree in c(100, 500, 1000, 1500)) {
#
#   set.seed(3333)
#   fit <- train(y = rf.label, x = rf.train.8,  method="rf", metric="Accuracy",  tuneGrid=tunegrid, trControl= control, ntree=n_tree)
#
#   key <- toString(n_tree)
#   model_list[[key]] <- fit
# }
#
# #Shutdown cluster
# stopCluster(cl)
# save(model_list, file = "RF_manual_search.rda")
# # the above code comneted out for output book file

load("RF_manual_search.rda")
# compare results
results <- resamples(model_list)
summary(results)
dotplot(results)
# We can see with the default *mtry =3* setting,
# the best *ntree* value is 1500.
# The model can reach 84.31% accuracy

###submit the final result to Kaggle for evaluation
set.seed(1234)

tunegrid <- expand.grid(.mtry = 3)
control <- trainControl(method="repeatedcv", number=3, repeats=10, search="grid")

# # # the following code have been commented out just for produce the markdown file. so it will not wait for ran a long time
# # # set up cluster for parallel computing
# cl <- makeCluster(6, type = "SOCK")
# registerDoSNOW(cl)
#
# Final_model <- train(y = rf.label, x = rf.train.8,  method="rf", metric="Accuracy",  tuneGrid=tunegrid, trControl= control, ntree=1500)
#
# #Shutdown cluster
# stopCluster(cl)
#
# save(Final_model, file = "Final_model.rda")
# # # the above code commented out for output book file

load("Final_model.rda")

# Make predictions
Prediction_Final <- predict(Final_model, test.submit.df)
#table(Prediction_Final)

# Write out a CSV file for submission to Kaggle
submit.df <- data.frame(PassengerId = test$PassengerId, Survived = Prediction_Final)

write.csv(submit.df, file = "Prediction_Final.csv", row.names = FALSE)

## We have got a score of 0.76076
##### End 5 Model Fine Tune ##############################################
\end{verbatim}

\hypertarget{titanicdataanalysis6_analyse_report.r}{%
\section*{TitanicDataAnalysis6\_Analyse\_Report.R}\label{titanicdataanalysis6_analyse_report.r}}


\begin{verbatim}
##########################################################################
# Reprot and futher improvment
# ####################################################
#prepare data for the code to be run independent of other chapters
data <- read.csv("data.csv", header = TRUE)
RE_data <- read.csv("RE_data.csv", header = TRUE)
train <- RE_data[1:891, ]
test <- RE_data[892:1309, ]

# model need to be loaded in to memory
load("RF_model2.rda")
RF_model2

# The model's estimated accuracy (by the model construction) is **83.16%**.
# The default parameters of the model are: `mtry = 2` and `ntree = 500`

# The Top 10 trees and the summary of OOB of RF_model2
head(RF_model2$err.rate, 10)
summary(RF_model2$err.rate)

#The cross validations on the model `RF_model2`,
load("RF_model2_cv.rda")
RF_model2_cv

# 2D Visualization of Model RE_model2's Prediciton
#install.packages("Rtsne")
# library(Rtsne)
# library(ggplot2)

# Rtsne needs a seed to ensure consistent output between runs.
set.seed(984357)
features <- c("Sex", "Fare_pp", "Pclass", "Title", "Age_group", "Group_size", "Ticket_class", "Embarked")
#generate 2-d coordinate
Model_tsne <- Rtsne(train[, features], check_duplicates = FALSE)
# Plot
ggplot(NULL, aes(x = Model_tsne$Y[, 1], y = Model_tsne$Y[, 2], color = as.factor(train$Survived))) +
  geom_point() +
  labs(color = "Survived")

# The importance of the predictorsRF_model2
# library(randomForest)
# library(caret)
varImpPlot(RF_model2, main = "")

# print out the values
pre.or <- sort(RF_model2$importance[,3], decreasing = TRUE)
print(pre.or)

###############################
## Further Analysis
###############################

# The decision tree of RF_model2
# library(rpart.plot)
load("model3.rda")
prp(model3, type = 0, extra = 1, under = TRUE)


## further re-engineering `Title` attribute
#let us further re-Engineer title check the value
table(RE_data$Title)

# Parse out title from the raw data
data$Title <- gsub('(.*, )|(\\..*)', '', data$Name)
table(data$Title)

# Further bin or bucket them into a more appropriate category
# We can do with the knowledge of nobility, locality (country of origin)
# and other knowledge such as time (at the beginning of the 20 century).
# For example, "`Dona`" and "`the Countess`" are female nobility equivalent
# to "`Lady`", and  "`Ms`" and "`Mlle`" are essentially the same with "`Miss`";
# "`Mme`" is a military title equivalent to "`Madame`", so it can be
# categorised as "`Mrs`"; "`Jonkheer`" is an honorific nobility in the
# Netherlands; and "`Don`" is title of a university lecturer, they can be
# categorises as "`Sir`"; "`Col`", "`Capt`", and "`Major`" are military
# ranks and can be replaced with a more general title "`Officer`".
# With all of these, we can reduce the numbers of title's category.

# Re-map titles to be more exact
data$Title[data$Title %in% c("Dona", "the Countess")] <- "Lady"
data$Title[data$Title %in% c("Ms", "Mlle")] <- "Miss"
data$Title[data$Title == "Mme"] <- "Mrs"
data$Title[data$Title %in% c("Jonkheer", "Don")] <- "Sir"
data$Title[data$Title %in% c("Col", "Capt", "Major")] <- "Officer"
table(data$Title)

# Collapse titles based on visual analysis
indexes <- which(data$new.Title == "Lady" |
                   (data$new.Title == "Dr" &
                      data$Sex == "female") |
                   (data$new.Title == "Officer"&
                      data$Sex == "female")
)
data$new.Title[indexes] <- "Mrs"

indexes <- which(data$new.Title == "Rev" |
                   data$new.Title == "Sir" |
                   (data$new.Title == "Officer" &
                      data$Sex == "male")|
                   (data$new.Title == "Dr" &
                      data$Sex == "male")  )
data$new.Title[indexes] <- "Mr"

table(data$new.Title)

# Check any other gender slip-ups?
length(which(data$sex == "female" &
               (data$new.Title == "Master" |
                  data$new.Title == "Mr")))
length(which(data$sex == "male" &
               (data$new.Title == "Miss" |
                  data$new.Title == "Mrs")))

# Visualize
ggplot(data[1:891,], aes(x = new.Title, fill = as.factor(Survived))) +
  geom_bar()

# check up the impact of the new-title
set.seed(2222)
RE_data$New_Title <- data$new.Title
RF_model2_new <- randomForest(as.factor(Survived) ~ Sex + Fare_pp + Pclass + New_Title + Age_group + Group_size + Ticket_class  + Embarked,
                              data = RE_data[1:891,],
                              importance=TRUE)

RF_model2_new

# The new model has increased the over accuracy with 0.45%.
# It is not a lot but it approves the point that features re-engineer
# is a place to do a model's performance improvement.
###########################################################################
\end{verbatim}

  \bibliography{book.bib,packages.bib}

\printindex

\end{document}
