<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1.1 Build a decision tree in Hunt’s Algorithm | 07-decision-tree.utf8</title>
  <meta name="description" content="" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="1.1 Build a decision tree in Hunt’s Algorithm | 07-decision-tree.utf8" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1.1 Build a decision tree in Hunt’s Algorithm | 07-decision-tree.utf8" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="prediction-with-decision-trees.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Authoring Books with R Markdown</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="prediction-with-decision-trees.html"><a href="prediction-with-decision-trees.html"><i class="fa fa-check"></i><b>1</b> Prediction with Decision Trees</a><ul>
<li class="chapter" data-level="1.1" data-path="build-a-decision-tree-in-hunts-algorithm.html"><a href="build-a-decision-tree-in-hunts-algorithm.html"><i class="fa fa-check"></i><b>1.1</b> Build a decision tree in Hunt’s Algorithm</a><ul>
<li class="chapter" data-level="1.1.1" data-path="build-a-decision-tree-in-hunts-algorithm.html"><a href="build-a-decision-tree-in-hunts-algorithm.html#best_split"><i class="fa fa-check"></i><b>1.1.1</b> How to Determine the Best Split Condition?</a></li>
<li class="chapter" data-level="1.1.2" data-path="build-a-decision-tree-in-hunts-algorithm.html"><a href="build-a-decision-tree-in-hunts-algorithm.html#the-simplest-decision-tree-for-titanic"><i class="fa fa-check"></i><b>1.1.2</b> The Simplest Decision Tree for Titanic</a></li>
<li class="chapter" data-level="1.1.3" data-path="build-a-decision-tree-in-hunts-algorithm.html"><a href="build-a-decision-tree-in-hunts-algorithm.html#the-most-complecated-decision-tree-for-titanic"><i class="fa fa-check"></i><b>1.1.3</b> The Most Complecated Decision Tree for Titanic</a></li>
<li class="chapter" data-level="1.1.4" data-path="build-a-decision-tree-in-hunts-algorithm.html"><a href="build-a-decision-tree-in-hunts-algorithm.html#the-rational-decision-tree-for-titanic"><i class="fa fa-check"></i><b>1.1.4</b> The Rational Decision tree for Titanic</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="build-a-decision-tree-in-hunts-algorithm" class="section level2">
<h2><span class="header-section-number">1.1</span> Build a decision tree in Hunt’s Algorithm</h2>
<p>Hunt’s algorithm builds a decision tree in a recursive fashion by partitioning the training dataset into successively purer subsets. Hunt’s algorithm takes three input values:</p>
<ol style="list-style-type: decimal">
<li>A training dataset, <span class="math inline">\(D\)</span> with a number of attributes,</li>
<li>A subset of attributes <span class="math inline">\(Att_{list}\)</span> and its testing criterion together to form a test condition, such as <code>'age&gt;=25'</code> is a test condition, where, <code>'age'</code> is the attribute and <code>'&gt;=25'</code> is the test criterion.</li>
<li>A <code>Attribute_selection_method</code>, it refers a procedure to determine the best splitting.</li>
</ol>
<p>The general recursive procedure is defined as below <span class="citation">[@Tan2005]</span>:</p>
<ol style="list-style-type: decimal">
<li>Create a node <span class="math inline">\(N\)</span>, suppose the training dataset when reach to note <span class="math inline">\(N\)</span> is <span class="math inline">\(D_{N}\)</span>. Initially, <span class="math inline">\(D_{N}\)</span> is the entire training set <span class="math inline">\(D\)</span>. Do the following:</li>
<li>If <span class="math inline">\(D_{t}\)</span> contains records that belong the same class <span class="math inline">\(y_{t}\)</span>, then <span class="math inline">\(t\)</span> is a leaf node labeled as <span class="math inline">\(y_{t}\)</span>;</li>
<li>If <span class="math inline">\(D_{t}\)</span> is not empty set but <span class="math inline">\(Att_{list}\)</span> is empty, (there is no more test attributes left untested), then <span class="math inline">\(t\)</span> is a leaf node labeled by the the label of the majority records in the dataset;</li>
<li>If <span class="math inline">\(D_{t}\)</span> contains records that belong to more than one class and <span class="math inline">\(Att_{list}\)</span> is not empty, use <code>Attribute_selection_method</code> to choose next best attribute from the <span class="math inline">\(Att_{list}\)</span> and remove that list from <span class="math inline">\(Att_{list}\)</span>. use the attribute and its condition as next test condition. 5. Repeat steps 2,3 and 4 until all the records in the subset belong to the same class.</li>
</ol>
<div id="best_split" class="section level3">
<h3><span class="header-section-number">1.1.1</span> How to Determine the Best Split Condition?</h3>
<p>The method used to define the best split makes different decision tree algorithms. There are many measures that can be used to determine the best way to split the records. These measures are defined in terms of the class distribution of the records before and after splitting. The best splitting is the one that has more purity after the splitting. If we were to split <span class="math inline">\(D\)</span> into smaller partitions according to the outcomes of the splitting criterion, ideally each partition after splitting would be pure (i.e., all the records that fall into a given partition would belong to the same class). Instead of define a split’s purity the impurity of its child node is used. There are a number of commonly used impurity measurements: <strong>Entropy</strong>, <strong>Gini Index</strong> and <strong>Classification Error</strong>.</p>
<p><strong>Entropy:</strong> measures the degree of uncertainty, impurity, or disorder. The formula for calculate entropy is as shown below:</p>
<p><span class="math display" id="eq:entropy">\[\begin{equation} 
E(x)= ∑_{i=1}^{n}p_ilog_2(p_i),
  \tag{1.1}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(p\)</span> represents the probability, and <span class="math inline">\(E(x)\)</span> represents the entropy.</p>
<p><strong>Gini Index:</strong> also called Gini impurity, measures the degree of probability of a particular variable being incorrectly classified when it is chosen randomly. The degree of the Gini index varies between zero and one, where zero denotes that all elements belong to a certain class or only one class exists, and one denotes that the elements are randomly distributed across various classes. A Gini index of 0.5 denotes equally distributed elements into some classes.</p>
<p>The formula used to calculate Gini index is shown below:</p>
<p><span class="math display" id="eq:Gini">\[\begin{equation} 
GINI(x) = 1- ∑_{i=1}^{n}p_i^2,
  \tag{1.2}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(p_i\)</span> is the probability of an object being classified to a particular class.</p>
<p><strong>Classification Error</strong> measures the misclassified class labels. It is calculated with the formula shows below:
<span class="math display" id="eq:clerror">\[\begin{equation} 
Classification error(x)= 1 - max_{i}p_i.
  \tag{1.3}
\end{equation}\]</span></p>
<p>Among these three impurity measurements, Gini is Used by the CART (classification and regression tree) algorithm for classification trees, and Entropy is Used by the ID3, C4.5 and C5.0 tree-generation algorithms.</p>
<p>With above explanation we can now say that the aims of a decision tree algorithm is to reduce Entropy level from the root to the leaves and the best tree is the one that takes order from the most to the least in reducing Entropy level. The good news is that we do not need to calculate impurity of each test condition to build a decision tree. The most tools have the tree construction built in already. But it is still important to understand the algorithms.</p>
</div>
<div id="the-simplest-decision-tree-for-titanic" class="section level3">
<h3><span class="header-section-number">1.1.2</span> The Simplest Decision Tree for Titanic</h3>
<p>In the Titanic problem, Let’s take a quick review of the possible attributes we could use. Previously we understand that apart from the PassengerID, Passenger Name (passenger name has been re-engineered into titles), all other attributes can all be used to predict a passenger’s death or survival since they all have some power of prediction.</p>
<p>Let us consider a simple decision tree firstly.</p>
<p>The simplest decision tree perhaps is the one only has one test condition and two possible outcomes. In terms of a tree, we called it one internal node and two branches. There are only one attribute meet with the requirements. That is <em>Sex</em>, so our decision tree will be build only base on passenger’s gender.</p>
<p>We need a number of libraries to make our code works.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb1-1"></a><span class="kw">library</span>(rpart)</span>
<span id="cb1-2"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb1-2"></a><span class="co"># build our first model. we only use Sex attribute, check help on rpart, </span></span>
<span id="cb1-3"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb1-3"></a><span class="co"># this model only takes Sex as predictor and Survived as the consequencer</span></span>
<span id="cb1-4"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb1-4"></a>train &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;train.csv&quot;</span>, <span class="dt">header =</span> <span class="ot">TRUE</span>)</span>
<span id="cb1-5"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb1-5"></a>test &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;test.csv&quot;</span>, <span class="dt">header =</span> <span class="ot">TRUE</span>)</span>
<span id="cb1-6"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb1-6"></a>model &lt;-<span class="st"> </span><span class="kw">rpart</span>(Survived <span class="op">~</span><span class="st"> </span>Sex, <span class="dt">data =</span> train,</span>
<span id="cb1-7"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb1-7"></a>              <span class="dt">method=</span><span class="st">&quot;class&quot;</span>)</span></code></pre></div>
<p>Simple! isn’t it? There are only three lines of code. you can see we have the first two lines to build two variables ‘train’ and ‘test’ to hold our training dataset and testing dataset. The model is simple a function invocation, the function is called ‘rpart’.</p>
<p>R function did the job for us so we do not need go through the model construction phase to build our classifier. The decision tree has been already built. Now we can test our model by making predictions on the test dataset. For Kaggle competition, you can produce the first prediction and can submit to Kaggle (of course the prediction is not very good in terms of accuracy since the simplicity of the model).</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb2-1"></a><span class="co"># The firs prediction produced by the first decision tree which only used one predictor Sex</span></span>
<span id="cb2-2"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb2-2"></a>Prediction &lt;-<span class="st"> </span><span class="kw">predict</span>(model, test, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)</span></code></pre></div>
<p>Our prediction is produced. If we want to submit to Kaggle, we can convert it into Kaggle required format and save it into a file called “myFirstResult.CSV”. Here, the importance is knowing the procedure.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb3-1"></a><span class="co"># produce a submit with Kaggle required format that is only two attributes: PassengerId and Survived</span></span>
<span id="cb3-2"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb3-2"></a>submit &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">PassengerId =</span> test<span class="op">$</span>PassengerId, <span class="dt">Survived =</span> Prediction)</span>
<span id="cb3-3"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb3-3"></a><span class="co"># Wrtie it into a file &quot;myFirstResult.CSV&quot;</span></span>
<span id="cb3-4"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb3-4"></a><span class="kw">write.csv</span>(submit, <span class="dt">file =</span> <span class="st">&quot;myFirstResult.CSV&quot;</span>, <span class="dt">row.names =</span> <span class="ot">FALSE</span>)</span></code></pre></div>
<p>Once we submit this result to Kaggle. Kaggle will make your results and provide a feedback. That is a good way to know how good the model performed. The Kaggle feedback tells us we have got 76.555% accurate! That is not too bad, isn’t it?</p>
<p>Let us have a brief check on our prediction.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb4-1"></a><span class="co"># Inspect prediction</span></span>
<span id="cb4-2"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb4-2"></a><span class="kw">summary</span>(submit<span class="op">$</span>Survived)</span></code></pre></div>
<pre><code>##   0   1 
## 266 152</code></pre>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb6-1"></a><span class="kw">prop.table</span>(<span class="kw">table</span>(submit<span class="op">$</span>Survived))</span></code></pre></div>
<pre><code>## 
##         0         1 
## 0.6363636 0.3636364</code></pre>
<p>The result shows that among total of 418 passenger in the test dataset, 266 passenger predicted perished (with survived value 0), which counts as 63.63 percent and 152 passenger predicted to be survived (with survived value 1) and which count as 36.36 percent.</p>
<p>We know that our model only had one test condition which is <em>Sex</em>. From the train dataset we knew that the gender ratio was very similar to this number.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb8-1"></a><span class="co"># add Sex back to the submit and form a new data frame called compare</span></span>
<span id="cb8-2"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb8-2"></a>compare &lt;-<span class="st"> </span><span class="kw">data.frame</span>(submit[<span class="dv">1</span>], <span class="dt">Sex =</span> test<span class="op">$</span>Sex, submit[<span class="dv">2</span>])</span>
<span id="cb8-3"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb8-3"></a><span class="co"># Check train sex and Survived ratios</span></span>
<span id="cb8-4"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb8-4"></a><span class="kw">prop.table</span>(<span class="kw">table</span>(train<span class="op">$</span>Sex, train<span class="op">$</span>Survived), <span class="dv">1</span>)</span></code></pre></div>
<pre><code>##         
##                  0         1
##   female 0.2579618 0.7420382
##   male   0.8110919 0.1889081</code></pre>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb10-1"></a><span class="co"># Check predicted sex radio</span></span>
<span id="cb10-2"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb10-2"></a><span class="kw">prop.table</span>(<span class="kw">table</span>(compare<span class="op">$</span>Sex))</span></code></pre></div>
<pre><code>## 
##    female      male 
## 0.3636364 0.6363636</code></pre>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb12-1"></a><span class="co">#check predicted Survive and Sex radio</span></span>
<span id="cb12-2"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb12-2"></a><span class="kw">prop.table</span>(<span class="kw">table</span>(compare<span class="op">$</span>Sex, compare<span class="op">$</span>Survived), <span class="dv">1</span>)</span></code></pre></div>
<pre><code>##         
##          0 1
##   female 0 1
##   male   1 0</code></pre>
<p>It is clear that our model is too simple: it predict any male will be perished and every female will be survived! This is approved by the male and female ratio in the test dataset is identical to the death ratio in our prediction result.</p>
<p>Further, our predicted results’ survival ratio on sex is male 0% and female is 100%. It make sense, isn’t it? since our model was trained using training dataset. The gender survive ratio were male only 18.89 and the death rate was 81%. Similarly, Female survival rate was 74.2 percent and death only has 25.79 percent.</p>
<p>Any prediction model will have to go for majority. But, you are not satisfied with this model, aren’t you? You will not happy with the model only looking into sex of given dataset and predict a passenger’s fate with gender!</p>
<p>This is only the starting, we can improve on it, a lot.</p>
<p>R has provided many useful library for classification, we can make use of them and improve our classifier. The first thing is that we want see our model (tree) and have a sense of the results produced, rather than simple call a function and produce a result. R has a lot of functions to help too.<br />
plot is a visual tool we can use to visual our model.</p>
<p><img src="07-decision-tree_files/figure-html/unnamed-chunk-6-1.svg" width="672" /></p>
<p>This graph is pretty and informative. The first box top number is the voting (either 0 - dead or 1-survived). The two percentages shows the value of the spliting (also called <strong>voting</strong> or <strong>confidence</strong>). The final number on each node shows the percent of population which resides in this node. Also the color of nodes signify the two classes here. For example, the root node, “0” (death) shows the way root node is voting; “.62” and “.38” represents the proportion of those who die and those who survive; 100% implies that the entire population resides in root node.</p>
</div>
<div id="the-most-complecated-decision-tree-for-titanic" class="section level3">
<h3><span class="header-section-number">1.1.3</span> The Most Complecated Decision Tree for Titanic</h3>
<p>Let us try another extreme, we use all the attributes without selection or discrimination, which knowing from the <strong>understanding data</strong> that have some prediction power and not too many levels (possibilities). Among of attributes, we choose <em>Pclass</em>, <em>Sex</em>, <em>Age</em>,,<em>SibSp</em>, <em>Parch</em>, <em>Fare</em>, <em>Cabin</em> and <em>Embarked</em>. We somehow escape <em>Name</em> and <em>Ticket</em>, which we believe that they not really have any power of prediction.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb14-1"></a><span class="co"># The full-house classifier apart from name and ticket </span></span>
<span id="cb14-2"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb14-2"></a>model &lt;-<span class="st"> </span><span class="kw">rpart</span>(Survived <span class="op">~</span><span class="st"> </span>Pclass <span class="op">+</span><span class="st"> </span>Sex <span class="op">+</span><span class="st"> </span>Age <span class="op">+</span><span class="st"> </span>SibSp <span class="op">+</span><span class="st"> </span>Parch <span class="op">+</span><span class="st"> </span>Fare <span class="op">+</span><span class="st"> </span>Embarked,</span>
<span id="cb14-3"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb14-3"></a>              <span class="dt">data=</span>train,</span>
<span id="cb14-4"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb14-4"></a>              <span class="dt">method=</span><span class="st">&quot;class&quot;</span>)</span>
<span id="cb14-5"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb14-5"></a>Prediction &lt;-<span class="st"> </span><span class="kw">predict</span>(model, test, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb14-6"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb14-6"></a>submit &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">PassengerId =</span> test<span class="op">$</span>PassengerId, <span class="dt">Survived =</span> Prediction)</span>
<span id="cb14-7"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb14-7"></a><span class="kw">write.csv</span>(submit, <span class="dt">file =</span> <span class="st">&quot;myFullhouseResult.CSV&quot;</span>, <span class="dt">row.names =</span> <span class="ot">FALSE</span>)</span></code></pre></div>
<p>You have produced a new prediction. We can aslo submit our result to Kaggle website for second evaluation.</p>
<p>This time, we can see the score has been increased to something like 0.77511.</p>
<p>Let us examine our classifier again by plot it in a graph.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb15-1"></a><span class="co"># plot our full house classifier </span></span>
<span id="cb15-2"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb15-2"></a><span class="kw">fancyRpartPlot</span>(model)</span></code></pre></div>
<p><img src="07-decision-tree_files/figure-html/unnamed-chunk-8-1.svg" width="672" /></p>
<p>The above decision tree appeared much complicated than the first one and it goes a lot deeper than what we saw last time. Note that the both trees are binary trees (have two branches). For test conditions that more than two possible answers have been changed to a binary by auto add a split with them. For example, age are numbers and have 10s of possibilities, our model simply split it by a test condition <code>"Age &gt;= 6.5"</code>. Conditions have been automatically set for others attributes as well such as <code>"Pclass &gt;= 2.5"</code>, <code>"SibSp&gt;=2.5"</code>, and <code>"Fare &gt;= 18"</code>, etc.</p>
<p>This conditions are not ideal, they can be changed if you know how to optimize decision tree. For the moment it looks very promising that resonates with the famous naval law that “women and kids first” is visible in our model.</p>
<p>If you want look into the difference between our two predictions, you can do,</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb16-1"></a><span class="co"># build a comparison data frame  to record each prediction results</span></span>
<span id="cb16-2"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb16-2"></a>compare &lt;-<span class="st"> </span><span class="kw">data.frame</span>(submit[<span class="dv">1</span>], <span class="dt">predict1 =</span> compare<span class="op">$</span>Survived , <span class="dt">predict2 =</span> Prediction)</span>
<span id="cb16-3"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb16-3"></a><span class="co"># Find differences</span></span>
<span id="cb16-4"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb16-4"></a>dif &lt;-<span class="st"> </span>compare[compare[<span class="dv">2</span>] <span class="op">!=</span><span class="st"> </span>compare[<span class="dv">3</span>], ]</span>
<span id="cb16-5"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb16-5"></a><span class="co">#show dif</span></span>
<span id="cb16-6"><a href="build-a-decision-tree-in-hunts-algorithm.html#cb16-6"></a>dif</span></code></pre></div>
<pre><code>##     PassengerId predict1 predict2
## 2           893        1        0
## 19          910        1        0
## 33          924        1        0
## 34          925        1        0
## 37          928        1        0
## 38          929        1        0
## 73          964        1        0
## 81          972        0        1
## 88          979        1        0
## 90          981        0        1
## 99          990        1        0
## 133        1024        1        0
## 139        1030        1        0
## 141        1032        1        0
## 158        1049        1        0
## 166        1057        1        0
## 170        1061        1        0
## 189        1080        1        0
## 197        1088        0        1
## 198        1089        1        0
## 200        1091        1        0
## 202        1093        0        1
## 215        1106        1        0
## 269        1160        1        0
## 281        1172        1        0
## 282        1173        0        1
## 285        1176        1        0
## 308        1199        0        1
## 346        1237        1        0
## 355        1246        1        0
## 366        1257        1        0
## 368        1259        1        0
## 377        1268        1        0
## 413        1304        1        0</code></pre>
<p>We can see the second classifier have produced 34 different predictions in comparison with the first classifier. That is a great improvement.</p>
</div>
<div id="the-rational-decision-tree-for-titanic" class="section level3">
<h3><span class="header-section-number">1.1.4</span> The Rational Decision tree for Titanic</h3>
<p>Now let us use our re-engineered train dataset to train our classifier to see how could we improve our prediction results.</p>
</div>
</div>
<!-- </div> -->
            </section>

          </div>
        </div>
      </div>
<a href="prediction-with-decision-trees.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown/edit/master/inst/examples/%s",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "none"
},
"search": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
