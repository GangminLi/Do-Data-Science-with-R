

## Titiannic Prediction with Random Forest

As we can see from the above section, decision tree does not preform well in our prediction. One of the improvements on decision tree prediction is using **Random Forest** model. 

Random Forest model is one of the powerful ensembling machine learning algorithm which works by creating multiple decision trees and combining the output generated by each of the decision trees through a voting mechanism to produce the final output based on the majority of decision trees' votes.  The Figure \@ref(fig:forest) is an example of what a random forest classifier in general looks like:

```{r forest,  out.width = "50%", fig.align ="center", echo =FALSE, fig.cap="Example of the Random Forest."}
knitr::include_graphics(here::here("images", "Random_forest_diagram_complete.png"))
```

In random forest, the decision tree classifier uses different training datasets; each training dataset contains different values and has different power in terms of prediction. Multiple decision tree are created with the help of these datasets. Based on the output of each models, a vote is carried out to find the result with the highest frequency. A test set is evaluated based on these outputs to get the final predicted results. 

### Steps to Build a Random Forest

1. Randomly select $k$ attributes from total $m$ attributes where $k < m$
2. Among the $k$ attributes, calculate the node $d$ using the **best split point**
3. Split the node into a number of nodes using the **best split method**. See Section \@ref(best_split), by default R randomForest uses Gini impurity values
4. Repeat the previous steps build an individual decision tree
5. Build a forest by repeating all steps for $n$ number times to create $n$ number of trees

After the random forest trees and classifiers are created, predictions can be made using the following steps:

1. Run the test data through the rules of each decision tree to predict the outcome and then 
2. Store that predicted target outcome
3. Calculate the votes for each of the predicted targets
4. Output the most highly voted predicted target as the final prediction 

Similar with the decision tree model, random forest also has many implementations already built. You do not need to write code to do the actual model construction. In R, you can use a package called  'randomForest'. There are a number of terminologies that are used in random forest algorithms need to be understood, such as:

1. **Variance**. When there is a change in the training data algorithm, this is the measure of that change. 

2. **Bagging**. This is a variance-reducing method that trains the model based on random subsamples of training data. 

3. **Out-of-bag (oob)** error estimate - The random forest classifier is trained using bootstrap aggregation, where each new tree is fit from a bootstrap sample of the training dataset. The out-of-bag (oob) error is the average error for each calculation using predictions from the trees that do not contain their respective bootstrap sample. This enables the random forest classifier to be adjusted and validated during training.


### Titanic prediciton with a Random Forest

Let’s now look at how we can implement the random forest algorithm for our Titanic prediction. R provides `'randomForest'` package. You can check the details of the package. We will try the original `train` dataset first and finally using our preprocessed data sets.

```{r echo = FALSE, warning=FALSE, message=FALSE}
# Install the random forest library
install.packages('randomForest')
#load library
library(randomForest) 
# load raw data if you have not
train <- read.csv("train.csv", header = TRUE)
test <- read.csv("test.csv", header = TRUE)
test <- data.frame(test[1], Survived = rep("NA", nrow(test)), test[ ,2:ncol(test)])


# Deal with missing values NA. we have checked missing value 
# in Chap 4 with individual attributes and dealt with in Chap 5.
# Here we have a simple function check them all.
# Function to count number of missing values

NuMissing <- function(x) sum(is.na(x))

# Apply the above function to every column in the train dataset
library(plyr)
L <- colwise(NuMissing)(train)
# write to a table
knitr::kable(L, digits = 2, booktabs = TRUE, caption = "Origianl Training Data: NA's")
```
We can see that only attribute *Age* has 177 `NAs`  and None of others has `NAs`. We know that some other variable has empty values. That empty values do not stop the random forest model construction, so we can leave them for the moment. We only need to replace these `NAs` using one of the imputation methods. Let us simple use *Age*'s mean value to replace the `NAs`.
```{r}
# Reaplece NAs in Age with its mean value
ageEverage <- summarise(train, Average = mean(Age, na.rm = TRUE))
train$Age[is.na(train$Age)] <- ageEverage$Average
k1 <- colwise(NuMissing)(train)
knitr::kable(k1, digits = 2, caption = "Training Data: NA's")

```

We can observe from the above summary results (before and after imputation) that changes have worked. We now can use randomForest package to build RF model on train1 data:

```{r}
# Check attributes types 
sapply(train, class)
sapply(test, class)

# convert  variables into factor
train$Survived <- as.factor(train$Survived)
train$Pclass <- as.factor(train$Pclass)

# Convert 
# convert  variables into factor
test$Pclass <- as.factor(test$Pclass)

# Create sample data (model train and valid datasets) for RF to use. we chose a split of 70:30
samp <- sample(nrow(train), 0.7 * nrow(train))
trainData <- train[samp, ]
validData <- train[-samp, ]

# let’s build the random forest model
RFmodel <- randomForest(Survived ~ Pclass + Sex + Age,, data = trainData, importance=TRUE, ntree = 1000)

```
Let us check our *Sex* variable importance:
```{r}
importance(RFmodel)
```
Let us make a prediction, 
```{r}
# Make your prediction using the test set
RF_prediction <- predict(RFmodel, validData)
```

Let us check our random forest model's performance by compare our predicted value with the original value on *Survived* with our `validData` by Confusion Matrix:
```{r}
options('digits'= 3)
conf_matrix <- RF_prediction$confusion
#knitr::kable(conf_matrix, digits = 2, caption = "My first RF model's prediciton Errors: ")
```

let us check our model,
```{r}
model
```
The model made 34.4% wrong prediction on death and 15.4 percent wrong prediciton on survived and data OOB estimate of error rate is 22.8%. It is pretty bad. 

let try again, this teim we use all the possible attributes as we did with decision tree.


We need to change attribute types which is a factor and has over 53 levels. They are *Names*, *Ticket* and *Cabin*.
```{r}
#modelFull <- randomForest(Survived ~ Pclass + Sex + Age + SibSp + Parch +  Embarked, data = trainData, importance=TRUE, ntree = 2000)
```
Let us make a prediction, 
```{r}
# Make your prediction using the test set
my_RF_prediction2 <- predict(modelFull, testData)
```

Let us check our random forest model's performance by compare our predicted value with the original value on *Survived* iwth our `testData` by Confusion Matrix:
```{r}
options('digits'= 3)
conf_matrix <- modelFull$confusion
knitr::kable(conf_matrix, digits = 2, caption = "My first RF model's prediciton Errors: ")
```


