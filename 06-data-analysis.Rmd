
# Data Analysis

```{r out.width = "80%", fig.align ="center", echo =FALSE}
knitr::include_graphics(here::here("images", "Prediction.png"))

```


“This will be the year of AI!” ...

“Mobil device will dominate the world.” ...

"Pay by face will be a reality in 20XX" ...

All of these are based the data collected. 

How many tiems you heard about this? It basically tells us that data tells us what is going to happening.

The core of Data Sciecne is analyzing data and interpreting what data tells us.

## Predictive Data Analysis

We have used other two data analyzing methods in the previous chapters: **Descriptive data analysis** and **Exploratiory data analysis**. This chapter will practice **Predictive data analysis (PDA)**.

PDA as a method encompasses both of DDA and EDA. It is truying to analyze current and historical data to make predictions about future or unknown data values. The way todo it is building a predictive model through training dataset and testing with testing dataset. After a model is created then it will be evaluated, improved and finally applied to unknown data for applications. 

A classic example of predictive model is a customer scoring as shown in Figure \@ref(fig:modelexam). Customer scoring model factors together individual customer’s attributes (properties or attributes), weights them and adds them up to produce an overall score.

```{r modelexam, fig.align="center", out.width = "80%", echo=FALSE, fig.cap ="Example of predictive model for customer score" }

knitr::include_graphics(here::here("images", "examplemodel.jpg"))
```

The process of constructing and prediction model is called **predictive modeling**. Predictive modeling is generally involves three steps: **Predictor selection**, **model construction** and **model evaluation**.

### Predictor Selection {-} 

Predictor, in data science, is an attribute that a prediction model used to predict values of another attribute. The attribute to be predicted is called **consequencer** (in some cases also called dependence). Generally, there are a large number of attributes an data object can have and be potentially used as predictors by a model to produce consequencer. The most models do not use all of the data attributes instead only used a number of selected attributes, then it is needed examining the relationship between each predictor and the consequencer using appropriate methods.  Filter and wrapper are the most common methods used in attributes selection: 


-	**Filters**. Filters is a method that examines each predictor in turn. A numerical measure is calculated, representing the strength of the correlation^[Correlation, in statistics, is a measurement of any statistical relationship two attributes. It can be any associations. It commonly refers to the degree to which a pair of attributes are linearly related.] between the predictor attribute and the consequencer. Only predictor attributes where the correlation measure^[The most commonly used measurement of correaltion between two attributes is the "Pearson's correlation coefficient", commonly called simply "the correlation coefficient".] exceeds a given threshold are selected.

-	**Wrappers**. A wrapper takes a group of predictors and considers the “value add” of each attribute compared to other attributes in the group. If two attributes tell you more or less the same thing (e.g. age and date of birth) then one will be discarded because it adds no value. Step-wise linear regression^[In statistics, stepwise linear regression is a method of fitting regression models in which the selection of predictors is carried out by a  procedure that in each step, one attribute is considered for addition to or subtraction from the set of selected attributes based on some pre-specified criterion.] and principal component analysis^[Principal component analysis (PCA) is the process of computing the principal components and using only the first few principal components and ignoring the rest in a prediction or data dimension reduction. The principal components are often computed by eigendecomposition of the data covariance matrix or singular value decomposition of the data matrix.] are two popular wrapper methods.

Attribute selection is a parsimonious process that aims to identify a minimal set of predictors for the maximum gain (predictive accuracy). This approach is the opposite of data pre-process where as many meaningful attributes as possible are considered for potential use. 

It is also important to recognize that attribute selection cloud be an iterative process that occurs throughout the model building process. It finishes after no more improvement can be achieved in terms of model accuracy. 

### Model Construction{-}

Model construction normally involves two phases: **induction** and **deduction**. 

-	Induction is also called model learning, which means learn to predict; 

- Deduction is called model apply, which means model applied to predict. 

The division of model learn and model apply allows a predictive model to be mature while induction using **training dataset** to construct a model and deduction using **testing dataset** to test and adjust the model constructed.

Depends on applications different prediction models can use different mathematical approach and algorithms. Model constricted for classification problem can use decision trees while scoring prediction model can use regressions. There are also **rule based models** and **machine learning** models.


### Model Validation{-}

As explained earlier, a major problem when building predictive models is that it is easy to find relationships that are the result of random patterns in the training and testing datasets, but which may not exist in the unseen datasets. This problem is called model "over-fitting". The result is that if you measure the performance of the model using the test dataset the results will be over-optimistic. The **over-fitting problem** will affect model’s performance when presented with new data when the model is deployed. 

To determine if over-fitting has occurred, the model needs to be tested on "**validation dataset**".  Validation datasets is a subset from the given datasets that have targeted attributes values. This subset was not used to construct the model. Validation dataset is genially taken from the training datasets with certain percentage. 

Over-fitting is quite common and this is not necessarily a problem. However, if the degree of over-fitting is large, the model may need to be reconstructed using a different set of attributes.

Apart from checking model's over-fitting, Depends on the model being constructed, there a number of evaluation methods are available to perform the model validation such as **Confusion Matrix** for nominal output like class labels, AUC (Area Under Curve),accuracy and other evaluation metrics are used fo r evaluate other models.  

## Prediction Models

There are many predictive models exists for different purposes. Many different methods can be used to create a model, and more are being developed all the time. Three broad predictive models based on the model format and the way it is built: 

### Math model.

Mathematical formulated model is the model produced by mathematical formula which combines multiple predictors (attributes) to predict a response (we called it targeted attribute). A predictor is a single attribute in a data object that contributes to the result of the prediction, which is consequencer (also called dependents in same applications). 

A well-known example of math model is regression model. A linear regression model is a target function $f$ that maps each attribute set $X$ into a continuous-valued output $y$ with minimum error. 

\begin{equation} 
  y = f(x) = f(x)= ω_1 x+ω_0,
  (\#eq:binom)
\end{equation} 



where $ω_0$ and $ω_1$ are parameters of the model and are called the *regression coefficients*. The model is to find the parameters $(ω_1, ω_0)$ that minimize the sum of the squared error (SSE),

\begin{equation} 
 SSE= \Sigma^{N}_{i=1}[y_i-f(x_i)]^2 = \Sigma^{N}_{i=1}[y_i - ω_1 x + ω_0 ]^2
  (\#eq:sse)
\end{equation} 

### Rule-based model

In a rule-based model, the model is a collection of rules. Such as `if the customer is rural, and her monthly usage is high, then the customer will probably renew`. 
In rule-based model, a model is a collection of `if … then …` rules. Table 11.2 shows an example of a classification model generated by a rule-based classifier for the vertebrate classification problem.

```{r include=FALSE, echo=FALSE}
rules <-  matrix(c("(Gives Birth = no) ^ (Aerial Creature = yes) -> Birds", 
"(Gives Birth = no) ^ (Aquatic Creature = yes) -> Fishes",
"(Gives Birth = yes) ^ (Body Temperature = warm-blooded) -> Mammals", 
"(Gives Birth = no) ^ (Aerial Creature = no) -> Reptiles", 
"(Aquatic Creature = semi) -> Amphibians"), ncol=1, byrow=TRUE )
colnames(rules) <- c("Rules of the vertebrate classification")
rownames(rules) <- c("r1", "r2", "r3", "r4", "r5")
ruletable <- as.table(rules) 
```

```{r echo = FALSE}
knitr::kable(head(ruletable),
booktabs = TRUE,
  caption = 'Example of a rule set for the vertebrate classification problem.'
)
```
The rules for the model are represented in a disjunctive normal form $R=(r_1 \vee r_2\vee … \vee r_k)$, where $R$ is known as the rule set and $r_i$ are the model rules.
Each rule is expressed in a form of:

\begin{equation} 
r_i:   (Condition_i) →  y_i.
  (\#eq:rule)
\end{equation} 

The left-hand side of the rule is called the **rule antecedent or precondition**. It contains a conjunction of attribute test:

\begin{equation} 
condition_i = (A_1 op v_1 ) ∧ (A_2 op v_2 ) ∧ … ∧(A_k  op v_k ),
  (\#eq:condition)
\end{equation} 

Where $(A_j\quad op\quad v_j )$ is an attribute-value pair and $op$ is a relation operator chosen from the set $ \{ =, ≠, <, >, ≤, ≥ \} $. Each attribute test $(A_j\quad op \quad v_j )$ is known as a conjunct. The right hand of the rule is called the rule consequent which contains the value of conceqencer $y_i$.


### Machine Learning Model

In many applications the relationship between the predictor set and the concequencer is non-deterministic or it is too difficult to either formulate a model or figure out rules by human. In these cases, advanced technologies are used to generate prediction models automatically taking advantage of massive computer storage and fast computation power of distributed and cloud based computing infrastructure. The models used are ether Neural networks  or statistical math models. 

In the machine learning models, different predictive models like regression, decision tree, and decision forest can be utilized and tested to produce a valid prediction. In general, predictive modeling software undertakes a mixture of training data go through number crunching, trial, and error correction and finally produce a working prediction model. During the process of machine generating model human involvement is much less but needed. It enables fine tune the model and improving on its performance. 

Classification is one form of the predictive analysis. In classification the prediction model is called **classifier**. Its input are training dataset, which has the targeted values in it; Its prediction results are class labels, which nrmally is the test dataset, which the atrgeted value is not there. The most commonly sued classifiers are: **Decision trees**, **Random Forest** and **Gaussian Naive Bayes**. 

Titanic problem as we understood is a prediction problem. The prediction on an passenger's death or survive based on train dataset is actually a classification problem. It on ly has two possibilities , it is also called binary classification. Because we only need to classify a passenger either belongs to survived class or perished class.    

## Data Analysis with Decision Trees

A decision tree is the most commonly used classification model, which in a flowchart-like tree structure. In a decision tree, each internal node (non-leaf node) denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (or terminal node) holds a class label. The topmost node in a tree is the root node. A typical decision tree is shown in Figure \@ref(ref:decisiontree). 


```{r decisiontree, fig.cap ="An example of decision tree", out.width = "50%", fig.align ="center", echo =FALSE}
knitr::include_graphics(here::here("images", "decision tree.png"))

```

It represents the concept buys_computer, that is, it predicts whether a customer is likely to purchase a computer or not. `Yes` is like to buy and `no` is unlikely to buy. Internal nodes are denoted by rectangles are test conditions, and leaf nodes are denoted by ovals, which are the final predictions. Some decision tree produce only branches, where each internal node branches to exactly two other nodes. Others can produce non-binary trees, like `age?` in the above tree has three branches. A prediction for a input data is actually a traverse from the tree root to a tree leaf through different tree branches. 

Decision tree can be built by the tree **induction** which is the learning of decision trees from class-labeled training sets. 

Once a decision tree has been constructed, classifying a test record is straightforward. Starting from the root node, we apply the test condition to the record and follow the appropriate branch based on the outcome of the test. This will lead us either to another internal node, for which a new test condition is applied, or to a leaf node. The class label associated with the leaf node is then assigned to the record. 

Build a decision tree classier needs to make two decisions: 1) which attributes to use for test conditions? 2) and in what order. Answering these two questions differently forms different decision tree construction algorithms. Each algorithm builds a decision tree differently. In terms of prediction, some of the trees are more accurate and cheaper to run than others. Finding the optimal tree is computationally expensive because of the exponential size of the search space. Nevertheless, efficient algorithms have been developed to induce a reasonably accurate decision tree in a reasonable amount of time. For example Hunt’s algorithm , ID3, C4.5 and CART algorithms are all this kind of algorithms for classification. The common feature of these algorithms is that they all employ a greedy strategy as demonstrated in the Hunt's algorithm:

### Steps to Build a decision tree in Hunt's Algorithm 

Hunt's algorithm builds a decision tree in a recursive fashion by partitioning the training dataset into successively purer subsets. Hunt's algorithm takes three input values:
1.  A	training dataset, $D$ with a number of attributes, 
2.  A subset of attributes  $Att_{list}$ and its testing criterion together to form a test condition, such as `age >= 25` is a test condition, where, `age` is the attribute and `>=25` is the test criterion. 
3.	A `Attribute_selection_method`, a procedure to determine the best splitting.

The general recursive procedure is defined as below [@Tan2005]:

1. Create a node $N$, suppose the training dataset when reach to note $N$ is $D_{N}$. Initially, $D_{N}$ is the entire training set $D$.  Do the following: 
2. If $D_{t}$ contains records that belong the same class $y_{t}$, then $t$ is a leaf node labeled as $y_{t}$;
3. If $D_{t}$ is not empty set but $Att_{list}$ is empty, (there is no more test attributes left untested), then $t$ is a leaf node labeled by the the label of the majority records in the dataset;
4. If $D_{t}$ contains records that belong to more than one class and $Att_{list}$ is not empty, use `Attribute_selection_method` to choose next best attribute from the $Att_{list}$ and remove that list from $Att_{list}$. use the attribute and its condition as next test condition. 5. Repeat steps 2,3 and 4 until all the records in the subset belong to the same class.

### How to Determine the Best Split Condition? {#best_split}

There are many measures that can be used to determine the best way to split the records. These measures are defined in terms of the class distribution of the records before and after splitting. The best splitting is the one that has more purity after the splitting. If we were to split D into smaller partitions according to the outcomes of the splitting criterion, ideally each partition after splitting would be pure (i.e., all the records that fall into a given partition would belong to the same class). Instead of define a split’s purity the impurity of its child node is used. There are a number of commonly used impurity measurements: **Entropy**, **Gini Index** and **Classification Error**.  

 **Entropy:** measures the degree of uncertainty, impurity, or disorder. The formula for calculate entropy is as shown below:

\begin{equation} 
E(x)= ∑_{i=1}^{n}p_ilog_2(p_i),
  (\#eq:entropy)
\end{equation} 

Where $p$ represents the probability, and $E(x)$ represents the entropy.

**Gini Index:** also called Gini impurity, measures the degree of probability of a particular variable being incorrectly classified when it is chosen randomly. The degree of the Gini index varies between zero and one, where zero denotes that all elements belong to a certain class or only one class exists, and one denotes that the elements are randomly distributed across various classes. A Gini index of 0.5 denotes equally distributed elements into some classes.

The formula used to calculate Gini index is shown below:

\begin{equation} 
GINI(x) = 1- ∑_{i=1}^{n}p_i^2,
  (\#eq:Gini)
\end{equation} 

Where $p_i$ is the probability of an object being classified to a particular class.

**Classification Error** measures the misclassified class labels. It is calculated with the formula shows below:
\begin{equation} 
Classification error(x)= 1 - max_{i}p_i.
  (\#eq:clerror)
\end{equation}

Among these three impurity measurements, Gini is Used by the CART (classification and regression tree) algorithm for classification trees, and Entropy is Used by the ID3, C4.5 and C5.0 tree-generation algorithms. 

With above explanation we can now say that the aims of a decision tree algorithm is to reduce Entropy level from the root to the leaves and the best tree is the one that takes order from the most to the least in reducing Entropy level. The good news is that we do not need to calculate impurity of each test condition to build a decision tree. The most tools have the tree construction built in already. But it is still important to understand the algorithms. 

### The Simplest Decision Tree for Titanic 

In the Titanic problem, Let’s take a quick review of the possible attributes we could use. Previously we understand that apart from PassengerID, Passenger Name (passenger name has been re-engineered into titles), all other attributes can all be used to do prediction since they all have some power of prediction. 

Let us consider a simple decision tree firstly. The most simple decision tree perhaps is the one only has one internal note and two branches. There are only one attribute meet with the requirements. That is *Sex*, so our decision tree will be build only base on passenger's gender. Here we go, We need a number of liberties to make our code works.


```{r}
library(rpart)
# build our first model. we only use Sex attribute, check help on rpart, 
# this model only takes Sex as predictor and Survived as the consequencer
train <- read.csv("train.csv", header = TRUE)
test <- read.csv("test.csv", header = TRUE)
model <- rpart(Survived ~ Sex, data = train,
              method="class")
```

Simple! isn't it? R function did the job for us so we do not need go through the model construction phase to build our classifier. The decision tree has been already built. Now we can make a prediction on the test dataset and produce our first prediction. We can submit our prediction to Kaggle. 

```{r}
# The firs prediction produced by the first decision tree which on ly used Sex
Prediction <- predict(model, test, type = "class")
```

Our prediction is produced, We can convert it into Kaggle required format and save it into a file called "myFirstResult.CSV".

```{r}
# produce a submit with Kaggle required format that is only two attributes: PassengerId and Survived
submit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)
# Wrtie it into a file "myFirstResult.CSV"
write.csv(submit, file = "myFirstResult.CSV", row.names = FALSE)
```
We can submit this result to Kaggle. Kaggle feedback is that we have got 76.555% accurate! That is not too bad. 

Let us have a brief check on our prediction.

```{r}
# Inspect prediction
summary(submit$Survived)
prop.table(table(submit$Survived))

```
The result shows that among total of 418 passenger in the test dataset, 266 passenger predicted perished, which counts as 63.63 percent and 152 passenger predicted to be survived and which count as 36.36 percent.   

We know that our model only had one test which is *Sex*. From the train dataset we knew that the 

```{r }
# add Sex back to the submit and form a new data frame called compare
compare <- data.frame(submit[1], Sex = test$Sex, submit[2])
# Check train sex and Survived ratios
prop.table(table(train$Sex, train$Survived), 1)
# Check predicted sex radio
prop.table(table(compare$Sex))
#check predicted Survive and Sex radio
prop.table(table(compare$Sex, compare$Survived), 1)
```
It is clear that our model is too simple: it predict andy male will be perished and every female will be survived. This is approved by the male and female ratio in the test dataset is identical to the death ratio in our prediction result. Further, pur results' survival ratio on sex is male 0% and female is 100%. It make sense, isn't it? since our model was trained using train data. teh gender survive ratio were male only 18.89 and the death rate was 81%. Similarly, Female survival rate was 74.2 percent and death only has 25.79 percent. Any prediction model will have to go for majority. 

This is only the starting, we can improve on it, a lot.

R has provided many useful library for classification, we can make use of them and improve our classifier.
```{r, echo = FALSE, warning=FALSE, message=FALSE}
# load some useful libraries
library(rattle)
library(rpart.plot)
library(RColorBrewer)

```
```{r }
#plot our decision tree
fancyRpartPlot(model)
```

This graph is pretty and informative. The first box top number is the voting (either 0 - dead or 1-survived). the two percentages shows the value of the voting (also called **confidence**). The final number on each node shows the percent of population which resides in this node. Also the color of nodes signify the two classes here. For example, the root node, "0" (death) shows the way root node is voting; ".62" and ".38" represents the proportion of those who die and those who survive; 100% implies that the entire population resides in root node.

### The Most Complecated Decision Tree for Titanic 

Let us try another extreme, we use all the attributes, which knowing from the **understanding data** step that have some prediction power and not too many levels (possibilities). Among of our attributes, choose *Pclass*, *Sex*, *Age*,,*SibSp*, *Parch*,  *Fare*, *Cabin* and *Embarked*, we only escaped *Name* and *Ticket* because we knew that they are not really have any power of prediction. This is basically the full house of attributes without any *Data preprocess*. 

```{r include=FALSE }
# The full-house classifier apart from name and ticket 
# model <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Cabin + Embarked,
#              data=train,
#              method="class")
# Prediction <- predict(model, test, type = "class")
# submit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)
# write.csv(submit, file = "myFullhouseResult.CSV", row.names = FALSE)
```

We have an error message. It tells us that the `test` dataset has some cabin values that our newly build model had never seen it. It means the value appeared in the `test` dataset is never appeared in the `train` dataset. so our model did not learn them. Let us remove *Cabin* attribute temporally  from our model construction. 

```{r}
# The full-house classifier apart from name and ticket 
model <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
              data=train,
              method="class")
Prediction <- predict(model, test, type = "class")
submit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)
write.csv(submit, file = "myFullhouseResult.CSV", row.names = FALSE)
```

You can submit our result to Kaggle website for second evaluation. You will see the score has been increased to something like 0.77511.

Let us examine our classifer again,
```{r}
# plot our full house classifier 
fancyRpartPlot(model)
```

The decision tree that have been built goes a lot deeper than what we saw last time. Note that the tree is a binary tree. For test conditions that more than two possible answers have been changed to a binary by auto add a split with them. For example, age are numbers and have 10s of possibilities, our model simple split it by a test dondition `Age >= 6.5`. Conditions have been set for others as well such as `Pclass >= 2.5`, `SibSp>=2.5`, and `Fare >= 18`, etc.. This conditions are not ideal, they can be changed if you know how to optimize decision tree. For the moment it  looks very promising that resonates with the famous naval law that "women and kids first" is visible in our model. 

If you want look into the difference between our two predictions, you can do,

```{r}
# build a comparison data frame  to record each prediction results
compare <- data.frame(submit[1], predict1 = compare$Survived , predict2 = Prediction)
# Find differences
dif <- compare[compare[2] != compare[3], ]
#show dif
dif
```
We can see the second classifier have produced 34 different predictions in comparison with the first classifier. That is a great improvement. 

### The Rational Decision tree for Titanic 

now let us use our re-engineered train Dataset to train our classifier to see how could we improve our prediction results

## Titiannic Prediction with Random Forest

As we can see from the above section, decision tree does not preform well in our prediction. One of the improvements on decision tree prediction is using **Random Forest** model. 

Random Forest model is one of the powerful ensembling machine learning algorithm which works by creating multiple decision trees and combining the output generated by each of the decision trees through a voting mechanism to produce the final output based on the majority of decision trees' votes.  The Figure \@ref(fig:forest) is an example of what a random forest classifier in general looks like:

```{r forest,  out.width = "50%", fig.align ="center", echo =FALSE, fig.cap="Example of the Random Forest."}
knitr::include_graphics(here::here("images", "Random_forest_diagram_complete.png"))
```

In random forest, the decision tree classifier uses different training datasets; each training dataset contains different values and has different power in terms of prediction. Multiple decision tree are created with the help of these datasets. Based on the output of each models, a vote is carried out to find the result with the highest frequency. A test set is evaluated based on these outputs to get the final predicted results. 

### Steps to Build a Random Forest

1. Randomly select $k$ attributes from total $m$ attributes where $k < m$
2. Among the $k$ attributes, calculate the node $d$ using the **best split point**
3. Split the node into a number of nodes using the **best split method**. See Section \@ref(best_split), by default R randomForest uses Gini impurity values
4. Repeat the previous steps build an individual decision tree
5. Build a forest by repeating all steps for $n$ number times to create $n$ number of trees

After the random forest trees and classifiers are created, predictions can be made using the following steps:

1. Run the test data through the rules of each decision tree to predict the outcome and then 
2. Store that predicted target outcome
3. Calculate the votes for each of the predicted targets
4. Output the most highly voted predicted target as the final prediction 

Similar with the decision tree model, random forest also has many implementations already built. You do not need to write code to do the actual model construction. In R, you can use a package called  'randomForest'. There are a number of terminologies that are used in random forest algorithms need to be understood, such as:

1. **Variance**. When there is a change in the training data algorithm, this is the measure of that change. 

2. **Bagging**. This is a variance-reducing method that trains the model based on random subsamples of training data. 

3. **Out-of-bag (oob)** error estimate - The random forest classifier is trained using bootstrap aggregation, where each new tree is fit from a bootstrap sample of the training dataset. The out-of-bag (oob) error is the average error for each calculation using predictions from the trees that do not contain their respective bootstrap sample. This enables the random forest classifier to be adjusted and validated during training.


### Titanic prediciton with a Random Forest

Let’s now look at how we can implement the random forest algorithm for our Titanic prediction. R provides `'randomForest'` package. You can check the details of the package, but it does not like attributes that has `NAs` values. So we have to dealt with `NA` values. We will try the original `train` dataset first and finally using our preprocessed data sets.

```{r echo = FALSE, warning=FALSE, message=FALSE}
#install.packages('randomForest')
# Install the random forest library
library(randomForest) 
# load raw data if you have not
train <- read.csv("train.csv", header = TRUE)
test <- read.csv("test.csv", header = TRUE)
test <- data.frame(test[1], Survived = rep("NA", nrow(test)), test[ ,2:ncol(test)])

# Deal with missing values NA. we have checked missing value 
# in Chap 4 with individual attributes and dealt with in Chap 5.
# Here we have a simple function check them all.
# Function to count number of missing values

NuMissing <- function(x) sum(is.na(x))

# Apply the above function to every column in the train dataset
library(plyr)
L <- colwise(NuMissing)(train)
# write to a table
knitr::kable(L, digits = 2, booktabs = TRUE, caption = "Origianl Training Data: NA's")
```
We can see that only attribute *Age* has 177 `NAs`  and None of others has `NAs`. We know that some other variable has empty values. That empty values do not stop the random forest model construction, so we can leave them for the moment. We only need to replace these `NAs` using one of the imputation methods. Let us simple use *Age*'s mean value to replace the `NAs`.
```{r}
# Reaplece NAs in Age with its mean value
ageEverage <- summarise(train, Average = mean(Age, na.rm = TRUE))
train$Age[is.na(train$Age)] <- ageEverage$Average
k1 <- colwise(NuMissing)(train)
knitr::kable(k1, digits = 2, caption = "Training Data: NA's")

```

We can observe from the above summary results (before and after imputation) that changes have worked. We now can use randomForest package to build RF model on train1 data:

```{r}
# Check attributes tyeps 
sapply(train, class)
sapply(test, class)

# convert  variables into factor
train$Survived <- as.factor(train$Survived)
train$Pclass <- as.factor(train$Pclass)

# Convert 
# convert  variables into factor
test$Pclass <- as.factor(test$Pclass)

# Create sample data (model train and valid datasets) for RF to use. we chose a split of 70:30
samp <- sample(nrow(train), 0.7 * nrow(train))
trainData <- train[samp, ]
validData <- train[-samp, ]

# let’s build the random forest model
RFmodel <- randomForest(Survived ~ Pclass + Sex + Age,, data = trainData, importance=TRUE, ntree = 1000)

```
Let us check our *Sex* variable importance:
```{r}
importance(RFmodel)
```
Let us make a prediction, 
```{r}
# Make your prediction using the test set
RF_prediction <- predict(RFmodel, validData)
```

Let us check our random forest model's performance by compare our predicted value with the original value on *Survived* with our `validData` by Confusion Matrix:
```{r}
options('digits'= 3)
conf_matrix <- RF_prediction$confusion
knitr::kable(conf_matrix, digits = 2, caption = "My first RF model's prediciton Errors: ")
```

let us check our model,
```{r}
model
```
The model made 34.4% wrong prediction on death and 15.4 percent wrong prediciton on survived and data OOB estimate of error rate is 22.8%. It is pretty bad. 

let try again, this teim we use all the possible attributes as we did with decision tree.


We need to change attribute types which is a factor and has over 53 levels. They are *Names*, *Ticket* and *Cabin*.
```{r}
modelFull <- randomForest(Survived ~ Pclass + Sex + Age + SibSp + Parch +  Embarked, data = trainData, importance=TRUE, ntree = 2000)
```
Let us make a prediction, 
```{r}
# Make your prediction using the test set
my_RF_prediction2 <- predict(modelFull, testData)
```

Let us check our random forest model's performance by compare our predicted value with the original value on *Survived* iwth our `testData` by Confusion Matrix:
```{r}
options('digits'= 3)
conf_matrix <- modelFull$confusion
knitr::kable(conf_matrix, digits = 2, caption = "My first RF model's prediciton Errors: ")
```



model <- randomforest(taste ~ . - quality, data = train, ntree = 1000, mtry = 5)

model

model$confusion




## Gaussian Naive Bayes

## Regression

One of the best demonstration of machine learning model is Regression. Regression is a statistical relationship between a dependent variable (often called the 'outcome variable') and one or more independent variables (often called 'predictors' or 'attributes') that a change in independent variable is associated with a change in dependent variable.  


```{r regression, out.width = "50%", fig.align ="center", echo =FALSE, fig.cap="Weigh and Age in a linear regression model."}
knitr::include_graphics(here::here("images", "regression.jpg"))
```

It is important to note that not all variables are related to each other. For example, a person’s favorite color may not be related to revenue from a website. But if you look at a chart showing weight and age, see Figure \@ref(fig:regression), the change in one variable `weight` is closely associated with the change in the other variable `age`. This makes intuitive sense, as from birth, as you get older, you get heavier. If you plot that data, you would see those green points on the graph up to some particular age where growth would taper off. The plot in the middle shows the clear linear relationship between age and weight, which is indicated by the solid red line. We sometimes call that line a regression line or a trend line, or the line of best fit. You see that the weight is the dependent variable, and age is the independent variable.

There are various types of regression: **Linear regression**, **Logistic regression** and **Polynomial regression**. 



```{r typeofregression, fig.show ="hold", out.width = "30%", fig.align ="center", echo =FALSE, fig.cap="Example of the three different types of regression models."}
knitr::include_graphics(here::here("images", "Linear_regression.png"))
knitr::include_graphics(here::here("images",  "logistic_curve.jpeg"))
knitr::include_graphics(here::here("images",  "polynormal.png"))

```
Linear regression is the most commonly used type. By definition, when there is a linear relationship between a dependent variable, which takes continuous values, and an independent variable, which is continuous or discrete, linear regression is used to model the relationship between them.

Logistic regression is normally used to model a depended variable that takes value of a categorical such as yes or no, true or false, depends on other independent variables. Notice that the trend line for logistic regression is in a shape of **S**, It is also called **sigmoid Curve**. 

Polynomial regression is when the relationship between the dependent variable  and the independent variable is in the `nth` degree of independent variable. In a plot, you can see that the relationship is not linear; there is a curve to that best-fit trend line.

Figure \@ref(fig:typeofregression) shows the three different types of regression. 
Logistic Regression is a useful model to use. Logistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variables (attributes) by estimating probabilities using a logistic function, which is the cumulative logistic distribution. 

Choice of using linear regress or logistic regression actually depends on the applications. Linear regression answers the question like “How much?”, So it is generally used to predict a continuous variable, like height and weight. Whereas logistic regression predicts if something will happen or not happen. Therefore logistic regression is used when a response variable has only two outcomes: "yes or no", "true or false". Sometimes logistic regression is regarded as a binary classifier, since there are only two outcomes.

In R, Logistic regression model can be built using the `glm` function with the option `family = binomial` (shortcut for family = binomial(link="logit"); the logit being the default link function for the binomial family).

```{r}
# missing value
train$Age[is.na(train$Age)] <- mean(train$Age, na.rm = TRUE) 
# build Logistic Regression model
LR_Model <- glm(Survived ~ Pclass + Sex + Age,
                         family = binomial, data = train)
# Check our model
summary(LR_Model)

exp(coef(LR_Model)[3])
confint(LR_Model)

# calculating the significance of the overall model:
with(LR_Model, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))
```

Deviance Residuals:

Note the confidence score generated by the model based on our training dataset.

I was surprised at the results. The Gaussian Naive algorithm performed poorly and the Random Forest on the other hand was consistently predicting with an accuracy of more than 80%.

Classification and regression trees (CART). CART is a non-parametric decision tree learning technique that produces either classification or regression trees, depending on whether the dependent variable is categorical or numeric, respectively (Breiman et al 1984).

## Evaluation 

https://www.rpubs.com/rezapci/Data_Science_Machine_Learning_HarvardX

models and valiadition

