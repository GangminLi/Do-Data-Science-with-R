---
output:
  pdf_document: default
  html_document: default
---

# Prediction with Decision Trees

A decision tree is the most commonly used classification model, which in a flowchart-like tree structure. In a decision tree, each internal node (non-leaf node) denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (or terminal node) holds a class label. The topmost node in a tree is the root node. A typical decision tree is shown in Figure \@ref(fig:decisiontree). 


```{r decisiontree, fig.cap ="An example of decision tree", out.width = "50%", fig.align ="center", echo =FALSE}
knitr::include_graphics(here::here("images", "decision tree.png"))
```

It represents the concept buys_computer, that is, it predicts whether a customer is likely to buy a computer or not. 'yes' is likely to buy and 'no' is unlikely to buy. Internal nodes are denoted by **rectangles**, they are test conditions, and leaf nodes are denoted by **ovals**, which are the final predictions. Some decision trees produce binary trees where each internal node branches to exactly two other nodes. Others can produce non-binary trees, like `age?` in the above tree has three branches. 

Decision tree is built by a process called tree **induction**, which is the learning or constructing of decision trees from a class-labeled training dataset. Once a decision tree has been constructed, it can be used to classify a test dataset, which is also called **deduction**. 

The deduction process is Starting from the root node of a decision tree, we apply the test condition to a record or data sample and follow the appropriate branch based on the outcome of the test. This will lead us either to another internal node, for which a new test condition is applied, or to a leaf node. The class label associated with the leaf node is then assigned to the record or the data sample. For example, to predict a new data input with `'age=senior'` and `'credit_rating=excellent'`, traverse starting from the root goes to the most right side along the decision tree and reaches to a leaf `yes`, which is indicated by the dot line in the figure \@ref(ref:decisiontree). 

Build a decision tree classifier needs to make two decisions:

1) which attributes to use for test conditions?

2) and in what order? 

Answering these two questions differently forms different decision tree algorithms. Different decision tress can have different prediction accuracy on the test dataset. Some decision trees are more accurate and cheaper to run than others. Finding the optimal tree is computationally expensive and some times is impossible because of the exponential size of the search space. In real practice, it is often to seek for efficient algorithms, that are reasonably accurate and only compute in a reasonable amount of time. Hunt’s, ID3, C4.5 and CART algorithms are all of this kind of algorithms for classification. The common feature of these algorithms is that they all employ a greedy strategy as demonstrated in the Hunt's algorithm.

## Decision tree in Hunt's Algorithm 

Hunt's algorithm builds a decision tree in a recursive fashion by partitioning the training dataset into successively **purer** subsets. Hunt's algorithm takes three input values:

1.  A	training dataset, $D$ with a number of attributes, 
2.  A subset of attributes $Att_{list}$ and its testing criterion together to form a `'test condition'`, such as `'age>=25'` is a test condition, where, `'age'` is the attribute and `'>=25'` is the test criterion.
3.	A `Attribute_selection_method`, it refers a procedure to determine the best splitting.

The general recursive procedure is defined as below [@Tan2005]:

1. Create a node $N$, suppose the training dataset when reach to note $N$ is $D_{N}$. Initially, $D_{N}$ is the entire training set $D$.  Do the following: 
2. If $D_{t}$ contains records that belong the same class $y_{t}$, then $t$ is a leaf node labeled as $y_{t}$;
3. If $D_{t}$ is not empty set but $Att_{list}$ is empty, (there is no more test attributes left untested), then $t$ is a leaf node labeled by the the label of the majority records in the dataset;
4. If $D_{t}$ contains records that belong to more than one class and $Att_{list}$ is not empty, use `Attribute_selection_method` to choose next **best attribute** from the $Att_{list}$ and remove that list from $Att_{list}$. use the attribute and its condition as next test condition. 
5. Repeat steps 2,3 and 4 until all the records in the subset belong to the same class.

There are two fundamental problems needs to be sorted before the Hunt's algorithm can work:
1. How to form a 'test condition'? particularly when non-binary attributes exists?
2. How to define the 'best test conditions', so very loop the best test condition can be used in a decision tree? 


### How to Form a Test Condition? {#test_condition}

Decision tree algorithms must provide a method for expressing an test condition and its corresponding outcomes for different attribute types.

1. Binary Attributes. The test condition for a binary attribute is simple because it only generates two potential outcomes, as shown in figure \@ref(fig:sex).

```{r sex, fig.cap ="Test condition for binary attributes.", out.width = "50%", fig.align ="center", echo =FALSE}
knitr::include_graphics(here::here("images", "sex.png"))
```
2. No-binary attributes. No-binary attributes depends on the types of nominal or ordinal, it can have different ways of split. A nominal attribute can have many values, its test condition can be expressed in two ways, as shown in Figure \@ref(fig:age2). For a multiway split , see Figure \@ref(fig:age2)(a), the number of outcomes depends on the number of distinct values for the corresponding attribute. For example, if an attribute such as age has three distinct values: youth, m_aged, or senior. Its test condition will produce a three-way split or a binary split. Figure \@ref(fig:age2)(b) illustrates three different ways of grouping the attribute values for age into two subsets.



```{r age1,  out.width = "50%",  fig.align ="center", echo =FALSE}
knitr::include_graphics(here::here("images", "age1.png"))
```
```{r age2, fig.cap ="Test condition for no-binary attributes.", out.width = "100%",  fig.align ="center", echo =FALSE}

knitr::include_graphics(here::here("images", "age2.png"))
```
3. Continuous Attributes. For continuous attributes, the test condition can be constructed as a comparison test $(A<v)$ or $(A≥v)$ with binary outcomes, or a range query with outcomes of the form $v_i≤A<v_{i+1}$, For $i=1,…,k$. The difference between these approaches is shown in Figure 11.34. For the binary case, the decision tree algorithm must consider all possible split positions v, and it selects the one that produces the best partition. For the multiway split, the algorithm must consider all possible ranges of continuous values. 

```{r fare, fig.cap ="Test condition for continuous attributes.", out.width = "100%",  fig.align ="center", echo =FALSE}

knitr::include_graphics(here::here("images", "Fare.png"))
```
### How to Determine the Best Split Condition? {#best_split}

The method used to define the best split makes different decision tree algorithms. There are many measures that can be used to determine the best way to split the records. These measures are defined in terms of the class distribution of the records before and after splitting. The best splitting is the one that has more **purity** after the splitting. If we were to split $D$ into smaller partitions according to the outcomes of the splitting criterion, ideally each partition after splitting would be pure (i.e., all the records that fall into a given partition would belong to the same class). Instead of define a split’s purity, the impurity of its child node is used. There are a number of commonly used impurity measurements: **Entropy**, **Gini Index** and **Classification Error**.  

 **Entropy:** measures the degree of uncertainty, impurity, or disorder. The formula for calculate entropy is as shown below:

\begin{equation} 
E(x)= ∑_{i=1}^{n}p_ilog_2(p_i),
  (\#eq:entropy)
\end{equation} 

Where $p$ represents the probability, and $E(x)$ represents the entropy.

**Gini Index:** also called Gini impurity, measures the degree of probability of a particular variable being incorrectly classified when it is chosen randomly. The degree of the Gini index varies between zero and one, where zero denotes that all elements belong to a certain class or only one class exists, and one denotes that the elements are randomly distributed across various classes. A Gini index of 0.5 denotes equally distributed elements into some classes.

The formula used to calculate Gini index is shown below:

\begin{equation} 
GINI(x) = 1- ∑_{i=1}^{n}p_i^2,
  (\#eq:Gini)
\end{equation} 

Where $p_i$ is the probability of an object being classified to a particular class.

**Classification Error** measures the misclassified class labels. It is calculated with the formula shows below:
\begin{equation} 
Classification error(x)= 1 - max_{i}p_i.
  (\#eq:clerror)
\end{equation}

Among these three impurity measurements, Gini is Used by the CART (classification and regression tree) algorithm for classification trees, and Entropy is Used by the ID3, C4.5 and C5.0 tree-generation algorithms. 

With above explanation we can now say that the aims of a decision tree algorithm is to reduce Entropy level from the root to the leaves and the best tree is the one that takes order from the most to the least in reducing Entropy level. 

The good news is that we do not need to calculate impurity of each test condition to build a decision tree manually. The most tools have the tree construction built-in already. We will use R package called *rpart* to build decision tress for our Titanic prediction problem. 

## The Simplest Decision Tree for Titanic 

In the Titanic problem, Let’s quickly review the possible attributes. Previously, we have understood that there are a few attributes that have a little prediction power or we say they have a little association with the dependent variable `Survivded`. These attributes include `PassengerID`, `Name` and `Ticket`. That is why we re-engineered some of them like passenger's name has been re-engineered into title,etc. Other attributes can all be used to predict a passenger's death or survival since they all have some power of prediction. So, which one to use and in what an order? We will use Titanic problem to demonstrate how to build decision trees for prediction. 

Let us consider a simple decision tree firstly. 

The simplest decision tree perhaps is the one only has one test condition and two possible outcomes. In terms of a tree, we called it one internal node and two branches. There are only one attribute meet with the requirements. That is *Sex*, so our decision tree will be build only base on passenger's gender. 

We need a number of libraries to make our code works.


```{r}
library(rpart)
# build our first model. we only use Sex attribute, check help on rpart, 
# this model only takes Sex as predictor and Survived as the consequencer
RE_data <- read.csv("RE_data.csv", header = TRUE)
train <- RE_data[1:891, ]
test <- RE_data[892:1309, ]
model1 <- rpart(Survived ~ Sex, data = train,
              method="class")
```

Simple! isn't it? There are only three lines of code. you can see we have the first two lines to build two variables 'train' and 'test' to hold our training dataset and testing dataset. The model is simple a function invocation, the function is called 'rpart'.

R function did the job for us so we do not need go through the model construction phase manually to build our classifier. The decision tree has been already built. Now we can use our model making predictions on the test dataset. 

For Kaggle competition, you can produce predictions on the test dataset provided and can submit to Kaggle. Kaggle will award a score for a model based on the prediction's accuracy on the test dataset. 

In practice, people would not build a prediction model and use it to produce a prediction on the test dataset blandly to finish the job. We want know how our model is performing before use it to do prediction on the test dataset. One way to get to know about the model's performance is to make a prediction on the training dataset or part of it. So we can compare the model's prediction with the original value.     

```{r}
library(caret)
# assess the accuracy with train data
Predict_model1_train <- predict(model1, train, type = "class")
conMat <- confusionMatrix(as.factor(Predict_model1_train), as.factor(train$Survived))
conMat$table
#conMat$overall
predict_train_accuracy <- conMat$overall["Accuracy"]
predict_train_accuracy
```
A brief assessment shows our model1's accuracy is 79%. It is not bad! Let us use this model to make a prediction on test dataset. 

```{r}
# The firs prediction produced by the first decision tree which only used one predictor Sex
Prediction1 <- predict(model1, test, type = "class")
```
Our prediction is produced. Let us submit to Kaggle for an evaluation. We need to convert our prediction into Kaggle required format and save it into a file and name it as "Tree_Model1.CSV". Here, the importance is knowing the procedure.

```{r}
# produce a submit with Kaggle required format that is only two attributes: PassengerId and Survived
submit1 <- data.frame(PassengerId = test$PassengerId, Survived = Prediction1)
# Write it into a file "Tree_Model1.CSV"
#write.csv(submit1, file = "Tree_Model1.CSV", row.names = FALSE)
```
Once we submit this result to Kaggle. Kaggle will evaluate our results and provide a feedback. That is a good way to know how good the model performed for unknown data. The Kaggle feedback tells us that we have scored 0.76555. It means our prediction has **76.555%** accuracy. 

It prediction accuracy is lower than the accuracy we have assessed on the training dataset, which was 78.68%.

Let us look a bit more into on our prediction model. We check our prediction's death and survive ratio to compare with ratio on the train dataset.  

```{r}
# Inspect prediction
summary(submit1$Survived)
prop.table(table(submit1$Survived))
#train survive ratio
#prop.table(table(as.factor(train$Survived)))

```
The result shows that among total of 418 passenger in the test dataset, 266 passenger predicted perished (with survived value 0), which counts as 64% and 152 passenger predicted to be survived (with survived value 1) and which count as 36%. This is not too far from the radio on the training dataset, which was 62% survived and 38% perished see \@ref(survive).   

We know that our model only had one test condition which is *Sex*. From the train dataset we knew that the gender ratio was very similar to this number. 

```{r }
# add Sex back to the submit and form a new data frame called compare
compare <- data.frame(submit1[1], Sex = test$Sex, submit1[2])
# Check train sex and Survived ratios
prop.table(table(train$Sex, train$Survived), 1)
# Check predicted sex radio
prop.table(table(compare$Sex))
#check predicted Survive and Sex radio
prop.table(table(compare$Sex, compare$Survived), 1)
```
It is clear that our model is too simple: it predict any male will be perished and every female will be survived! This is approved by the male and female ratio in the test dataset is identical to the death ratio in our prediction result. 

Further, our predicted results' survival ratio on sex is male 0% and female is 100%. It make sense, isn't it? since our model was trained using training dataset. The gender survive ratio were male only 18.89 and the death rate was 81%. Similarly, Female survival rate was 74.2 percent and death only has 25.79 percent. 

Any prediction model will have to go for majority. But, you are not satisfied with this model, aren't you? You will not happy with the model only looking into sex of given dataset and predict a passenger's fate with gender! 

This is only the starting, we can improve on it. R has provided many useful library for classification, we can make use of them and improve our classifier. The first thing is that we want see our model (tree) and have a sense of the results produced, rather than simple call a function and produce a result. R has a lot of functions to help too.  
Plot is a visual tool we can use to visual our model.

```{r, echo = FALSE, warning=FALSE, message=FALSE}
# load some useful libraries
library(rattle)
library(rpart.plot)
library(RColorBrewer)

#plot our decision tree
fancyRpartPlot(model1)
```

This graph is pretty and informative. The first box top number is the voting (either 0 - dead or 1-survived). The two percentages shows the value of the splitting (also called **voting** or **confidence**). The final number on each node shows the percent of population which resides in this node. Also the color of nodes signify the two classes here. For example, the root node, "0" (death) shows the way root node is voting; ".62" and ".38" represents the proportion of those who die and those who survive; 100% implies that the entire population resides in root node.

## The Decision Tree with Core Predictors 

We have identified the correlation between response variable *Survived* and other attributes in the previous Chapter. Let us try to improve the basic *Sex* model by introduce more predictors. Let us consider the three most correlated attributes.

```{r}
# The full-house classifier apart from name and ticket 
set.seed(1234)
model2 <- rpart(Survived ~ Pclass + Sex + Deck + Fare_pp + Age_group, data = train, method="class")
```
```{r}
# assess the accuracy with train data
Predict_model2_train <- predict(model2, train, type = "class")
conMat <- confusionMatrix(as.factor(Predict_model2_train), as.factor(train$Survived))
conMat$table
#conMat$overall
predict2_train_accuracy <- conMat$overall["Accuracy"]
predict2_train_accuracy
```
Our assessment on model2's accuracy is make a prediction on train data and compare the predicted value with the original value. It shows our model2's accuracy is 79%. This is great! Let us use this model to make a prediction on test dataset. 

``` {r}
Prediction2 <- predict(model2, test, type = "class")
submit2 <- data.frame(PassengerId = test$PassengerId, Survived = Prediction2)
write.csv(submit2, file = "Tree_model2.CSV", row.names = FALSE)
```

We have produced our second prediction. We can also submit our result to Kaggle website for the second evaluation. 

This time, we can see the score has been increased to 0.76555. The accuracy on the test dataset has not been improved. 

Let us examine our classifier again by plot it in a graph.

```{r tree2, out.width='50%', fig.show='hold', fig.cap='Decision trees with multiple original predictors.'}
# plot our full house classifier 
prp(model2, type = 0, extra = 1, under = TRUE)
# plot our full house classifier 
fancyRpartPlot(model2)
```

The above decision tree appeared much complicated than the first one and it goes a lot deeper than what we saw last time. Note that the both trees are binary trees (have two branches). For test conditions that more than two possible answers have been changed to a binary by auto add a split with them. For example, age are numbers and have 10s of possibilities, our model simply split it by a test condition `"Age >= 6.5"`. Conditions have been automatically set for others attributes as well such as `"Pclass >= 2.5"`, `"SibSp>=2.5"`, and `"Fare >= 18"`, etc. 

This conditions are not ideal, they can be changed if you know how to optimize decision tree. For the moment it looks very promising that resonates with the famous naval law that "women and kids first" is visible in our model. 

If you want look into the difference between our two predictions, you can do,

```{r}
# build a comparison data frame to record each prediction results
Tree_compare <- data.frame(test$PassengerId, predict1=Prediction1, predict2=Prediction2)
# Find differences
dif <- Tree_compare[Tree_compare[2]!=Tree_compare[3], ]
#show dif
dif
```
We can see the second classifier have produced 34 different predictions in comparison with the first classifier. That is a great improvement. 

## The Decision Tree with More Predictors

Let us see if we can improve on this with more predictors from the correlation analysis and PCA analyses results. We use five most related attributes: *Sex*, *Pclass*, *HasCabinNum*, *Deck* and *Fare_pp*.

```{r}
model3 <- rpart(Survived ~ Sex + Fare_pp + Pclass + HasCabinNum + Deck + Group_size,
              data=train,
              method="class")

#assess prediction accuracy on train data
Predict_model3_train <- predict(model3, train, type = "class")
conMat <- confusionMatrix(as.factor(Predict_model3_train), as.factor(train$Survived))
conMat$table
#conMat$overall
predict3_train_accuracy <- conMat$overall["Accuracy"]
predict3_train_accuracy
```
Our assessment on model3's accuracy on the train data shows model3's accuracy is 81%. It has increased a bit from the model2, which was 79%. Let us use this model to make another prediction on the test dataset and see if the accuracy on the test dataset is also increased. 

``` {r}
Prediction3 <- predict(model3, test, type = "class")
submit3<- data.frame(PassengerId = test$PassengerId, Survived = Prediction3)
write.csv(submit3, file = "tree_model3.CSV", row.names = FALSE)
```

After submit to Kaggle the feedback was **0.75837**. Despite the accuracy on train dataset has been decreased almost 4.5%, There is a little improvement on the test dataset. Let us look into the difference between the last two predictions, 

```{r multi-trees, out.width='50%', fig.show='hold', fig.cap='Decision trees.'}
# plot our full house classifier 
prp(model3, type = 0, extra = 1, under = TRUE)
# plot our full house classifier 
fancyRpartPlot(model3)
```

We do the same to understand what went wrong.

```{r}
# build a comparison data frame to record each prediction results
compare <- data.frame(test$PassengerId, predict2 = Prediction2 , predict3 = Prediction3)
# Find differences
dif <- compare[compare[2] != compare[3], ]
#show dif
dif
```
There are 37 differences. 

Now let us use our re-engineered train dataset to train our classifier to see how could we improve our prediction results.

## The Decision Tree with Full Predictors

Let us use all the attributes: 

```{r}
# The full-house classifier apart from name and ticket

model4 <- rpart(Survived ~ Pclass+Sex+Embarked+HasCabinNum+Friend_size+Fare_pp+Title+Deck+Ticket_class+Family_size+Age_group,
              data=train,
              method="class")
#assess prediction accuracy on train data
Predict_model4_train <- predict(model4, train, type = "class")
conMat <- confusionMatrix(as.factor(Predict_model4_train), as.factor(train$Survived))
conMat$table
#conMat$overall
predict4_train_accuracy <- conMat$overall["Accuracy"]
predict4_train_accuracy
```
Our assessment on model4's accuracy on the train data shows model4's accuracy is 85%. This is hug improvement! Let us use this model to make a prediction on the test dataset. 

```{r}
# make prediction on test dataset
Prediction4 <- predict(model4, test, type = "class")
submit4 <- data.frame(PassengerId = test$PassengerId, Survived = Prediction4)
write.csv(submit4, file = "Tree_model4.CSV", row.names = FALSE)
```
We have produced a new prediction with new model. You can submit to Kaggle for an evaluation. You may find it has a pretty bed score (**0.75119**)! It even worse than the decision tree with only looking in to sex!!! Let us examine our classifier again by plot it in a graph.

```{r tree3, out.width='50%', fig.show='hold', fig.cap='Decision trees.'}
# plot our full house classifier 
prp(model4, type = 0, extra = 1, under = TRUE)
fancyRpartPlot(model4)
```

We can see from the tree, the first test condition is `"title = Mr"`. We know that there are large number of passengers are male adult and most of them are perished. However, we can see from our decision tree (left branch) that there are two further test conditions are "Ticket_class" and "Deck" numbers. Our tree end with 2 leafs are survived and one is dead. The purity on the leads are not very high, the highest is 39:366 and the lowest is 1:2. However we know that the most powerful predictor Sex and pclass has not been used. That could be an explanation of the poor performance. We will demonstrate the detailed interpretation of the model in the later chapter of report.  

Let us look into the difference between the last two predictions, 

```{r}
# build a comparison data frame to record each prediction results
compare <- data.frame(test$PassengerId, predict2 = Prediction2 , predict4 = Prediction4)
# Find differences
dif2 <- compare[compare[2] != compare[3], ]
#show dif
dif2
```
There are 40 different predictions in comparison with the second prediction model.

## Summary

In this section, we have produced 4 decision tree models. Model1 and model2 used the original attributes as predictors and model3 and model4 used re-engineered datasets. We have produced 4 predictions with these 4 models on the test datasets. The predictions were submitted to Kaggle for evaluation since we have no idea the survival values for the test dataset. The results obtained from the Kaggle in terms of prediction's accuracy were 76.555%, 77.511%, 75.598% and 75.837% respectively. 

We have noticed the accuracy differences between the model's assessment and model's test accuracy are:

+ model1's assessed accuracy was 78.68% but the accuracy on test was 76.555%; 

+ Model2's accessed accuracy was 83.95% and the actual accuracy on test was 77.511%;

Model3 and model4 are models trained on the re-engineered dataset. They were hoped to achieve a higher prediction's accuracy. 

+ Model3's accessed accuracy was 85.97%. and the actual accuracy on test was 75.598%; 

+ Model4's accessed accuracy was 81.48% and the actual accuracy on test was 75.837%. 

let us plot these model's accuracy, so we can have a comparison.
```{r Tree_model_modelcompare, fig.cap = "Decision Tree models' accuracy on Train dataset and Test dataset."}


library(tidyr)
model <- c("M1","M2","M3","M4")

Train <- c(78.68,83.95,85.97,81.48)
Test <- c(76.555,77.511,75.598,75.837)

df <- data.frame(model, Train, Test)

df.long <- gather(df, Dataset, Accuracy, -model, factor_key =TRUE)

ggplot(data = df.long, aes(x = model, y = Accuracy, fill = Dataset)) +
  geom_col(position = position_dodge()) 

```
From the plot we can see that all four models perform better when predicting the survive on the train dataset than they do on the test data. The biggest drop of prediction accuracy is model3, the difference is almost 10% from 85.97% on train data to 75.6% on the test dataset. The model3 has the highest model accuracy on the training dataset. The best accuracy on teh test dataset is the model2.

What surprising is that the accuracy of model3 and model4 are worse than model1 and model2. It indicates that there is a possibility of overfitting in model3 and model4 because the re-engineered dateset has done a lot of cleaning, formatting and transforming. Ideally, models built based on the re-engineered dataset should have a higher prediction accuracy. 

The fact is model3 and model4 performed badly with the unseen data. This is a common problem with a prediction model. There are many possible reasons for this. We will investigate it by a method called "Cross Validation" later in Chapters 9. 


## Excerces 

1. Prove there is overfitting with model3 and model4. The idea is take certain samples from the training dataset form a validate dataset. Remove the value of attribute *survived* from the validate dataset. Using models to a prediction on validate dataset. Compare their results. If model3 performs better than model2 on the validate dataset but not on test dataset (which is given by Kaggle), it shows the model has an overfit problem. Proive this exists on both model3 and model4.

Here are some sample code.

```{r}
set.seed(1234)
train <- RE_data[1:891, ]
samp <- sample(nrow(train), 0.7 * nrow(train))
trainData <- train[samp, ]
validData <- train[-samp, ]

Prediction2 <- predict(model1, validData, type = "class")
Prediction3 <- predict(model4, validData, type = "class")
#check up
table(Prediction2, validData$Survived)
table(validData$Survived)
table(Prediction2)

table(Prediction3, validData$Survived)
table(validData$Survived)
table(Prediction3)


compare1 <- data.frame(validData$PassengerId, validData = validData$Survived , predict = Prediction2)
# Find differences
dif1 <- compare1[compare1[2] != compare1[3], ]
#show dif
dif1

compare2 <- data.frame(validData$PassengerId, validData = validData$Survived , predict = Prediction3)
# Find differences
dif2 <- compare2[compare2[2] != compare2[3], ]
#show dif
dif2
```

