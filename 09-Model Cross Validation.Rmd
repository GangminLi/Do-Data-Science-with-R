# Model Cross Validation


jeff Leek

   
>
>"...with four parameters I can fit an elephant, and with five I can make him wiggle his trunk."
>
>                                        -- John von Neumann
>
>
>You are entitled to your own opinion, but you are not entitled to your own facts.
>
>
>                                           --   Daniel Patrick Moynihan

In the previous two chapters we have build a number of prediction models using both decision tree and random forest, the two very popular prediction models. Our model have produced different prediction accuracy. The big problem with all the models is that they all have a reduced prediction accuracy with the test dataset. The worse thing is that the reduction of the prediction accuracy on each model is quid different. Together they make us don't know which model should be used for real prediction to achieve the best possible result. 

We are luck because we have Kaggle competition that provides us with a test dataset and the feedback of our model's performance on the test dataset. In real applications, as the titanic competition simulated the test dataset has no response variables (survival status) value. We will have no means to compare to evaluate model accuracy. 

Although we may use the methods we have used in Chapter 7, where we use our model to predict on the train dataset and made a comparison with the original value to estimate the model's prediction accuracy. The similar method (OOB) is also used in the random forest models (in Chapter 8) to estimate the model's accuracy. We know that our estimated accuracy is not reliable. 

There is systematic method in data science used to evaluate a prediction model called "Cross Validation (CV)". This chapter we will demonstrate how to use CV to evaluate the models built in the precious two chapters.  
  
## Model's Underfitting and Overfitting

We have experienced that both of our decision tree models and random forest models have a problem that the model has a higher estimated accuracy and a much lower accuracy with the test dataset. This Would only mean two things with our prediction models ether is overfitting or underfitting.

let us quickly look at a very graphic example of underfitting, a good fit, and overfitting. 

```{r modelfit, fig.cap ="Model's fit by train and test data", out.width = "100%", fig.align ="center", echo =FALSE}
knitr::include_graphics(here::here("images", "underfit.png"))

```

We can see that the first model is a straight line (a low variance model: $y$ = $m$ * $x$ + $c$) fails to capture the underlying parabolic curve in the data, this is underfitting. At the other extreme the high degree polynomial (a low bias model) captures too much of the noise at the same time as the underlying parabola, and is overfitting. Although it is following the data points provided (ie. the training dataset), this curve is not transferable to new data (ie. the test dataset).

Among the models we have produced, the decision tree model1 with only attribute *Sex* as its predictor is an example of underfitting model. It has 78.68% estimated accuracy on the training dataset but only has 76.56% accuracy on the test dataset. On the contrary, our random forest Model4 is seriously overfitted. It has  

## General Cross Validation Methods

There are two general Cross validation methods can be used to valid a prediction model:

1. Single model cross-validation
2. Multiple models comparison

### Single model Cross Validation
The goal of single model cross-validation is to test the model's ability to predict new data that was not used in model construction, in order to flag problems like overfitting or selection bias and to give an insight on how the model will generalize to an independent dataset or an unknown dataset.

One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set). To reduce variability, in most methods multiple rounds of cross-validation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model's predictive performance.

There are two major cross validation methods: exhaustive Cross-validation and non-exhaustive Cross-validation. 

+ **Exhaustive cross-validation** learn and test on all possible ways to divide the original sample into a training and a validation set. **Leave-p-out cross-validation (LpO CV)** is an exhaustive cross validation method. It involves using $p$ data samples as the validation dataset and the remaining data samples as the training dataset. This is repeated over and over until all possible ways to divide the original data sample into a training and a validation dataset $p$. 

+ **Non-exhaustive cross validation**, in the contrary, does not compute all the possible ways of splitting the original data sample but still has a certain coverage. **$k$-fold cross-validation** is a typical non-exhaustive cross validation. In $k$-fold cross-validation, the original data sample is randomly partitioned into $k$ equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation dataset for testing the model, and the remaining $k$ − 1 subsamples are used as training data. The cross-validation process is then repeated $k$ times, with each of the $k$ subsamples used exactly once as the validation data. The $k$ results can then be averaged to produce a single estimation. The advantage of this method over repeated random sub-sampling is that all observations are used for both training and validation, and each observation is used for validation exactly once. 10-fold cross-validation is commonly used in practice. 

### General Procedure of Cross-validation

The general procedure of use of Cross validation is as follows:
1. Define cross validation folds $K$ and create training dataset and validation dataset by Shuffle the dataset randomly.
2. Specify parameters for Cross validation. 
3. Create model from training dataset
4. Create list of predicted values on validation dataset
5. Check prediction error and prediction accuracy 

We will use examples to demonstrate this procedure. 

### Cross Validation on Decision Tree Models

We have produced 4 decision tree models in Chapter 7. Let us do Cross validation on model3 and model4 since these two used re-Engineered dataset and perform badly on the test dataset.
```{r}
library(caret)
library(rpart)
library(rpart.plot)

#read Re-engineered dataset
RE_data <- read.csv("RE_data.csv", header = TRUE)

#Factorize response variable
RE_data$Survived <- factor(RE_data$Survived)
#Seperate Train and test data.
train <- RE_data[1:891, ]
test <- RE_data[892:1309, ]

#setup model's train and valid dataset
set.seed(1000)
samp <- sample(nrow(train), 0.8 * nrow(train))
trainData <- train[samp, ]
validData <- train[-samp, ]

```
```{r}

# First, create cross validation folds, specifying parameters
set.seed(3214)
folds = createMultiFolds(trainData$Survived, 
                         k = 10, # number of folds
                         times = 5) # number of partitions
# Second, specify parameters for cross validation
control <- trainControl(method = "repeatedcv", 
                        index = folds,
                        number = 6, #repeat times
                        search = "grid")
#Third, create model from cross validation data
tree_model3_cv <- train(Survived ~ Pclass + Title + Sex + Age_group + Group_size + Ticket_class + Fare_pp + Deck + HasCabinNum + Embarked,  
                       data = trainData, 
                       method = "rpart", 
                       trControl = control)
```
```{r tree_model3_CV, out.width='32.8%', fig.show='hold', fig.cap='Decision Tree model3.'}
#Visualize cross validation tree

rpart.plot(tree_model3_cv$finalModel, extra=4)

print.train(tree_model3_cv)
plot.train(tree_model3_cv)
# accuracy is 0.83.
```


```{r}
### Access accuracy on different datasets
#predict on train
predict_train <-predict(tree_model3_cv, trainData)
conMat <- confusionMatrix(predict_train, trainData$Survived)
conMat$table
#conMat$overall
predict_train_accuracy <- conMat$overall["Accuracy"]
predict_train_accuracy
#predict on valid
predict_valid <-predict(tree_model3_cv, validData)
conMat <- confusionMatrix(predict_valid, validData$Survived)
conMat$table
#conMat$overall
predict_valid_accuracy <- conMat$overall["Accuracy"]
predict_valid_accuracy

#predict on test
predict_test <-predict(tree_model3_cv, test)
submit <- data.frame(PassengerId = test$PassengerId, Survived = as.factor(predict_test))
write.csv(submit, file = "Tree_model3_CV.CSV", row.names = FALSE)

## test accuracy 0.77751
# accumulate model's accuracy
model3_CV_accuracy <- c(0.83, 0.83, 0.8, 0.77751)
```
Let us try on decision tree model4,

```{r}
set.seed(1010)

tree_model4_cv <- train(Survived ~ Sex + Fare_pp + Pclass,
                      data = trainData, 
                      method = "rpart", 
                      trControl = control)
```
```{r tree_model4_CV, out.width='32.8%', fig.show='hold', fig.cap='Decision Tree model3.'}
#Visualize cross validation tree

rpart.plot(tree_model4_cv$finalModel, extra=4)

print.train(tree_model4_cv)
plot.train(tree_model4_cv)
# accuracy is 0.83.
```
```{r}
### Access accuracy on different datasets
#predict on train
predict_train <-predict(tree_model4_cv, trainData)
conMat <- confusionMatrix(predict_train, trainData$Survived)
conMat$table
#conMat$overall
predict_train_accuracy <- conMat$overall["Accuracy"]
predict_train_accuracy
#predict on valid
predict_valid <-predict(tree_model4_cv, validData)
conMat <- confusionMatrix(predict_valid, validData$Survived)
conMat$table
#conMat$overall
predict_valid_accuracy <- conMat$overall["Accuracy"]
predict_valid_accuracy

#predict on test
predict_test <-predict(tree_model4_cv, test)
submit <- data.frame(PassengerId = test$PassengerId, Survived = as.factor(predict_test))
write.csv(submit, file = "Tree_model4_CV_2.CSV", row.names = FALSE)

## test accuracy 75837
# accumulate model's accuracy
Tree_model4_CV_accuracy <- c(0.83, 0.83, 0.78, 0.75837)

```
Let us try the same with the two Random forest models constructed in Chapter 8.

```{r }
# set seed for reproduction
set.seed(2307)
RF_model1_cv <- train(Survived ~ Sex + Pclass + Fare_pp,  
                       data = trainData, 
                       method = "rf", 
                       trControl = control)

print(RF_model1_cv)
print(RF_model1_cv$results)
```
We can see the accuracy is 83%.

Let us verify on vialdate dataset and make prediction on test dataset. 
```{r}
### Access accuracy on different datasets
#predict on train
predict_train <-predict(RF_model1_cv, trainData)
conMat <- confusionMatrix(predict_train, trainData$Survived)
conMat$table
#conMat$overall
predict_train_accuracy <- conMat$overall["Accuracy"]
predict_train_accuracy
#predict on valid
predict_valid <-predict(RF_model1_cv, validData)
conMat <- confusionMatrix(predict_valid, validData$Survived)
conMat$table
#conMat$overall
predict_valid_accuracy <- conMat$overall["Accuracy"]
predict_valid_accuracy

#predict on test
predict_test <-predict(RF_model1_cv, test)
submit <- data.frame(PassengerId = test$PassengerId, Survived = as.factor(predict_test))
write.csv(submit, file = "RF_model1_CV.CSV", row.names = FALSE)

## test accuracy 75837
# accumulate model's accuracy
RF_model1_cv_accuracy <- c(0.83, 0.92, 0.77, 0.75837)
```
Let us try on random forest model4,
```{r}
# set seed for reproduction
set.seed(2300)
RF_model2_cv <- train(Survived ~ Pclass + Title + Sex + Age_group + Group_size + Ticket_class  + Fare_pp + Deck + HasCabinNum + Embarked, 
                       data = trainData, 
                       method = "rf", 
                       trControl = control)

print(RF_model2_cv)
print(RF_model2_cv$results)
```

```{r}
### Access accuracy on different datasets
#predict on train
predict_train <-predict(RF_model2_cv, trainData)
conMat <- confusionMatrix(predict_train, trainData$Survived)
conMat$table
#conMat$overall
predict_train_accuracy <- conMat$overall["Accuracy"]
predict_train_accuracy
#predict on valid
predict_valid <-predict(RF_model2_cv, validData)
conMat <- confusionMatrix(predict_valid, validData$Survived)
conMat$table
#conMat$overall
predict_valid_accuracy <- conMat$overall["Accuracy"]
predict_valid_accuracy

#predict on test
predict_test <-predict(RF_model2_cv, test)
submit <- data.frame(PassengerId = test$PassengerId, Survived = as.factor(predict_test))
write.csv(submit, file = "RF_model2_CV.CSV", row.names = FALSE)

## test accuracy 75837
# accumulate model's accuracy
RF_model2_cv_accuracy <- c(0.84, 0.97, 0.79, 0.75837)
```
We have cross validated the 4 models we have produced with our re-engineered dataset and two from decision tree and two from random forest. We have produced different accuracy with different datasets. Put them into one table and plot them.

```{r}
library(tidyr)
model <- c("Tree_M3","Tree_M4","RF_model1","RF_model2")

Est <- c(83, 83, 83, 84)
Train <- c(83, 83, 92, 97)
Varif <- c(80, 78, 77, 79)
Test <- c(77.751, 75.837, 75.837, 75.837)

df <- data.frame(model, Est, Train, Varif, Test)

df.long <- gather(df, Dataset, Accuracy, -model, factor_key =TRUE)

ggplot(data = df.long, aes(x = model, y = Accuracy, fill = Dataset)) +
  geom_col(position = position_dodge()) 

```

The comparison clearly shows **the overfitting of the random forest models**. 
Despite our re-engineering the dataset and using repeated 10 folds cross training on the model, the accuracy of the models have been increased significantly on the model and train dataset but they decreased dramatically on the verification dataset and the test dataset.The both dataset are unseen data for the prediction model. 

A general rule is that, the more features you have, and the more samples you used in the training, the more likely your model will suffer from overfitting and vice verse. 

On the contrary, although we have also deployed the repeated 10 folds training on the decision tree models' construction, they only increases a small amount of the accuracy on the training dataset but this increase of accuracy has been transformed into the accuracy increase on the verification and test datasets. This can be seen from the decision tree model3,it has the highest accuracy on the verification dataset and the test dataset. 

Therefore to choose a model for real prediction, we should choose the model that has the smallest accuracy decrease from the model's training to verification by the cross validation. 

## Multiple Models Comparison

Multiple model comparison is also called Cross Model Validation. The idea is to use multiple models constructed from the same training dataset and validated using the same verification dataset to find out the performance of the different models.

We somehow already used the technique to compare our decision tree models and random forest models. Cross model verification has a broader meaning that refers to the comparison between different models produced by the different algorithms or completely different approaches such as decision tree against random forest or decision tree against Support Victor Machine(SVM) etc.

To demonstrate cross model validation, let us produce a few more models with complete different algorithms.

### Regression Model for Titanic 
```{r}

LR_Model <- glm(formula = Survived ~ Pclass + Title + Sex + Age_group + Group_size + Ticket_class  + Fare_pp + Deck + HasCabinNum + Embarked, family = binomial, data = trainData)

#summary(LR_Model_CV)
### Validate on trainData
Valid_trainData <- predict(LR_Model, newdata = trainData, type = "response") #prediction threshold
Valid_trainData <- ifelse(Valid_trainData > 0.5, 1, 0)  # set binary 
#produce confusion matrx
confusion_Mat<- confusionMatrix(as.factor(trainData$Survived),as.factor(Valid_trainData))
# output accuracy
paste('Model Train Accuracy =', round(confusion_Mat$overall["Accuracy"],4))

### Validate on validData
validData_Survived_predicted <- predict(LR_Model, newdata = validData, type = "response")  
validData_Survived_predicted  <- ifelse(validData_Survived_predicted  > 0.5, 1, 0)  # set binary prediction threshold
conMat<- confusionMatrix(as.factor(validData$Survived),as.factor(validData_Survived_predicted))

paste('Model Valid Accuracy =', round(conMat$overall["Accuracy"],4)) 

### produce a prediction on test data
library(pROC)
auc(roc(trainData$Survived,Valid_trainData))  # calculate AUROC curve
#predict on test
test$Survived <- predict(LR_Model, newdata = test, type = "response")  
test$Survived <- ifelse(test$Survived > 0.5, 1, 0)  # set binary prediction threshold
submit <- data.frame(PassengerId = test$PassengerId, Survived = as.factor(test$Survived))

write.csv(submit, file = "LG_model1_CV.CSV", row.names = FALSE)
```
0.76555

Acc <-c(0.8684, 0.7985, 0.76555)

### Support Vector Machine Model for Titanic 

Let us also consider a support vector machine (SVM) model (Cortes and Vapnik 1995 Cortes, Corinna, and Vladimir Vapnik. 1995. “Support-Vector Networks.” Machine Learning, 273–97.). We use the C-classification mode. Again, we fit a model with the same set of attributes as in the logistic regression model to fit the model, we use function svm() from the e1071 package (Meyer et al. 2019). The results of the model are collected for comparison.

```{r}
##use grid-search to find the best gamma & cost. this can be done with different range settings
svm_test<-tune.svm(Survived ~ Pclass + Title + Sex + Age_group + Group_size + Ticket_class + Fare_pp + Deck + HasCabinNum + Embarked, data=trainData, gamma = seq(0,1,.05),cost = seq(.5,10,.5))
plot(svm_test)
```
```{r}
##the accuracy with each gamma and cost 
d<-double(nrow(svm_test$performances))
for(i in 1:nrow(svm_test$performances)){
  b_svm<-svm(Survived~ Pclass + Title + Sex + Age_group + Group_size + Ticket_class + Fare_pp + Deck + HasCabinNum + Embarked, trainData, gamma=svm_test$performances[i,1],cost=svm_test$performances[i,2],type="C-classification")
  b<-table(trainData[,1],predict(b_svm,trainData[,-1]))
  d[i]<-sum(diag(b))/sum(b)
}
e<-data.frame(gamma=svm_test$performances[1],cost=svm_test$performances[2],error=svm_test$performances[3],dispersion=svm_test$performances[4],accrancy=d)
e<-e[order(e$error,decreasing = F),]
head(e,10)  
```
```{R}
#gamma=.05,cost=1.30
SVM_model<-svm(Survived~.-Survived, data=train, gamma=.05,cost=30,type="C-classification")

SVM_model<-svm(Survived~.-Survived, data=train, gamma=.031,cost=4,type="C-classification")

SVM_model<-svm(Survived ~ Pclass + Title + Sex + Age_group + Group_size + Ticket_class + Fare_pp + Deck + HasCabinNum + Embarked, data=trainData, gamma=0.05,cost=1.3,type="C-classification")

SVM_model<-svm(Survived ~ Pclass + Title + Sex + Age_group + Group_size + Ticket_class + Fare_pp + Deck + HasCabinNum + Embarked, data=trainData, gamma=0.1,cost=0.3,type="C-classification")

#summary(SVM_model)
### Validate on trainData
Valid_trainData <- predict(SVM_model, trainData) 
#produce confusion matrix
confusion_Mat<- confusionMatrix(as.factor(trainData$Survived), as.factor(Valid_trainData))
# output accuracy
paste('Model Train Accuracy =', round(confusion_Mat$overall["Accuracy"],4))

### Validate on validData
validData_Survived_predicted <- predict(SVM_model, validData)  
conMat<- confusionMatrix(as.factor(validData$Survived),as.factor(validData_Survived_predicted))

paste('Model Valid Accuracy =', round(conMat$overall["Accuracy"],4)) 

### make prediction on test
# SVM failed to produce a prediction on test because test has Survived col and it has value NA. A work around is assign it with a num like 1.
test$Survived <-1

Survived <- predict(SVM_model, test)
solution <- data.frame(PassengerId=test$PassengerId, Survived =Survived)
write.csv(solution, file = 'svm_predicton.csv', row.names = F)
```

0.8652 0.8603 0.78708

### Neural Network Models

Neural networks are a rapidly developing paradigm for information processing based loosely on how neurons in the brain processes information. A neural network consists of multiple layers of nodes, where each node performs a unit of computation and passes the result onto the next node. Multiple nodes can pass inputs to a single node and vice versa.

The neural network also contains a set of weights, which can be refined over time as the network learns from sample data. The weights are used to describe and refine the connection strengths between nodes. 

Neural Network with one hidden layer utilizing all features


```{r}
library(nnet)

xTrain = train[ , c("Survived", "Pclass","Title", "Sex","Age_group","Group_size", "Ticket_class", "Fare_pp", "Deck", "HasCabinNum", "Embarked")]

NN_model1 <- nnet(Survived ~ ., data = xTrain, size=10, maxit=500, trace=FALSE)

#How do we do on the training data?
nn_pred_train_class = predict(NN_model1, xTrain, type="class" )  # yields "0", "1"
nn_train_pred = as.numeric(nn_pred_train_class ) #transform to 0, 1
confusion_Mat<-confusionMatrix(as.factor(nn_train_pred), train$Survived)
# output accuracy
paste('Model Train Accuracy =', round(confusion_Mat$overall["Accuracy"],4))

#How do we do on the valid data?
nn_pred_valid_class = predict(NN_model1, validData, type="class" )  # yields "0", "1"
nn_valid_pred = as.numeric(nn_pred_valid_class ) #transform to 0, 1
confusion_Mat<-confusionMatrix(as.factor(nn_valid_pred), validData$Survived)
# output accuracy
paste('Model valid Accuracy =', round(confusion_Mat$overall["Accuracy"],4))

#make a prediction on test data
nn_pred_test_class = predict(NN_model1, test, type="class" )  # yields "0", "1"
nn_pred_test = as.numeric(nn_pred_test_class ) #transform to 0, 1
solution <- data.frame(PassengerId=test$PassengerId, Survived = nn_pred_test)
write.csv(solution, file = 'NN_predicton.csv', row.names = F)

###
# 0.8934,0.8547, 0.70574

```
```{r}
xTrain = train[ , c("Survived", "Pclass", "Sex", "Title", "Age", "Group_size", "Family_size", "Ticket_class", "Fare_pp", "Deck", "HasCabinNum", "Embarked")]

NN_model2 = nnet( Survived ~ ., data = xTrain, size=11,  maxit=500, trace=FALSE);

#How do we do on the training data?
nn_pred_train_class = predict(NN_model2, xTrain, type="class" )  # yields "0", "1"
nn_train_pred = as.numeric(nn_pred_train_class ) #transform to 0, 1
confusion_Mat<-confusionMatrix(as.factor(nn_train_pred), train$Survived)
# output accuracy
paste('Model Train Accuracy =', round(confusion_Mat$overall["Accuracy"],4))

#How do we do on the valid data?
nn_pred_valid_class = predict(NN_model2, validData, type="class" )  # yields "0", "1"
nn_valid_pred = as.numeric(nn_pred_valid_class ) #transform to 0, 1
confusion_Mat<-confusionMatrix(as.factor(nn_valid_pred), validData$Survived)
# output accuracy
paste('Model valid Accuracy =', round(confusion_Mat$overall["Accuracy"],4))

#make a prediciton on test data
nn_pred_test_class = predict(NN_model2, test, type="class" )  # yields "0", "1"
nn_pred_test = as.numeric(nn_pred_test_class ) #transform to 0, 1
solution <- data.frame(PassengerId=test$PassengerId, Survived = nn_pred_test)
write.csv(solution, file = 'NN_predicton2.csv', row.names = F)

##0.8878, 0.8827, 0.74880
```




## summary

For the Titanic problem we are luck to have a feedback score. It is called leader board score (LB). We can easily use LB score to check if the model is overfitting or not. that is,

if $LB(score) < CV(score)$ , the model is overfitting;
if $LB(score) > CV(score)$ , the model is underfitting;

What we want is $CV(score) ≈ LB(score)$. 

That said, it is more useful to find a model that from $CV(score)< LB(score)$ (underffitting) up to the point of $CV(score) > LB(score)$ (overfitting). The ideal situation is to fidn a model or to adjust the parameters of a model so as to find the maximum leaderboard score right up to the point where the two values start to diverge. 

In real practice there is not leader board score to compare. That is where the validation dataset and the score of on the validation comes to play. We can use the prediction score on the specifically prepared validation dataset then compare it with the CV score to identify the model performance. It is difficult to continuously adjust the models parameters to find the best parameter and eliminate the overfilling but the concept is the same. depends on the task and the computation power. The model evaluation is more important than produce a model. 
